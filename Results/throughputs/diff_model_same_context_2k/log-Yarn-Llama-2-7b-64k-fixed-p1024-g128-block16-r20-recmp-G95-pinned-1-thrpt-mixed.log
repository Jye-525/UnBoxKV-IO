Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='fixed', input_len=1024, output_len=128, min_len=10, max_len=50, prefill_to_decode_ratio=6.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=2048, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=2048, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=True)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Selected required requests 20
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Mixed batch is enabled. max_num_batched_tokens=2048, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'finetuned', 'original_max_position_embeddings', 'type'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-02 18:40:40 config.py:654] [SchedulerConfig] max_num_batched_tokens: 2048 chunked_prefill_enabled: False
INFO 10-02 18:40:40 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=True)
/home/jieye/viper2/vllm_v0_4_2_mine/vllm/executor/gpu_executor.py:36: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 10-02 18:40:41 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-02 18:40:46 selector.py:27] Using FlashAttention-2 backend.
INFO 10-02 18:40:49 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-02 18:40:49 model_runner.py:183] Loaded model: 
INFO 10-02 18:40:49 model_runner.py:183]  LlamaForCausalLM(
INFO 10-02 18:40:49 model_runner.py:183]   (model): LlamaModel(
INFO 10-02 18:40:49 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-02 18:40:49 model_runner.py:183]     (layers): ModuleList(
INFO 10-02 18:40:49 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-02 18:40:49 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-02 18:40:49 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:40:49 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:40:49 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-02 18:40:49 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-02 18:40:49 model_runner.py:183]         )
INFO 10-02 18:40:49 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-02 18:40:49 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:40:49 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:40:49 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-02 18:40:49 model_runner.py:183]         )
INFO 10-02 18:40:49 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:40:49 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:40:49 model_runner.py:183]       )
INFO 10-02 18:40:49 model_runner.py:183]     )
INFO 10-02 18:40:49 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:40:49 model_runner.py:183]   )
INFO 10-02 18:40:49 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-02 18:40:49 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-02 18:40:49 model_runner.py:183]   (sampler): Sampler()
INFO 10-02 18:40:49 model_runner.py:183] )
INFO 10-02 18:40:49 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:40:50 worker.py:164] Peak: 13.332 GB, Initial: 38.980 GB, Free: 25.648 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.092 GB
INFO 10-02 18:40:50 gpu_executor.py:117] # GPU blocks: 3083, # CPU blocks: 8192
INFO 10-02 18:41:21 worker.py:189] _init_cache_engine took 24.0859 GB
INFO 10-02 18:41:21 scheduler.py:307] Scheduler initialized with prompt limit: 2048
INFO 10-02 18:41:21 Start warmup...
INFO 10-02 18:41:21 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:21 metrics.py:335] Avg prompt throughput: 771.4 tokens/s, Avg generation throughput: 38.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%, Interval(ms): 25.9
INFO 10-02 18:41:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2861 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:41:21 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:41:21 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 52.6
INFO 10-02 18:41:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 52.1121 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:41:26 Start benchmarking...
INFO 10-02 18:41:26 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:41:26 metrics.py:335] Avg prompt throughput: 11367.4 tokens/s, Avg generation throughput: 11.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%, Interval(ms): 180.2
INFO 10-02 18:41:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 156.4999 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:41:26 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1026]), positions.shape=torch.Size([1026]) hidden_states.shape=torch.Size([1026, 4096]) residual=None
INFO 10-02 18:41:26 metrics.py:335] Avg prompt throughput: 11572.4 tokens/s, Avg generation throughput: 33.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 17 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%, Interval(ms): 88.5
INFO 10-02 18:41:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 88.2235 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:41:26 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1027]), positions.shape=torch.Size([1027]) hidden_states.shape=torch.Size([1027, 4096]) residual=None
INFO 10-02 18:41:26 metrics.py:335] Avg prompt throughput: 13650.8 tokens/s, Avg generation throughput: 53.3 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 8.4%, CPU KV cache usage: 0.0%, Interval(ms): 75.0
INFO 10-02 18:41:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 74.8374 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:41:26 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1028]), positions.shape=torch.Size([1028]) hidden_states.shape=torch.Size([1028, 4096]) residual=None
INFO 10-02 18:41:27 metrics.py:335] Avg prompt throughput: 13570.1 tokens/s, Avg generation throughput: 66.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%, Interval(ms): 75.5
INFO 10-02 18:41:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 75.2511 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:41:27 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1029]), positions.shape=torch.Size([1029]) hidden_states.shape=torch.Size([1029, 4096]) residual=None
INFO 10-02 18:41:27 metrics.py:335] Avg prompt throughput: 13535.1 tokens/s, Avg generation throughput: 79.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 75.7
INFO 10-02 18:41:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 75.4795 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:41:27 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1030]), positions.shape=torch.Size([1030]) hidden_states.shape=torch.Size([1030, 4096]) residual=None
INFO 10-02 18:41:27 metrics.py:335] Avg prompt throughput: 13514.9 tokens/s, Avg generation throughput: 92.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 14.7%, CPU KV cache usage: 0.0%, Interval(ms): 75.8
INFO 10-02 18:41:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 75.5892 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:41:27 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1031]), positions.shape=torch.Size([1031]) hidden_states.shape=torch.Size([1031, 4096]) residual=None
INFO 10-02 18:41:27 metrics.py:335] Avg prompt throughput: 13454.1 tokens/s, Avg generation throughput: 105.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 16.8%, CPU KV cache usage: 0.0%, Interval(ms): 76.1
INFO 10-02 18:41:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 75.9261 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:41:27 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1032]), positions.shape=torch.Size([1032]) hidden_states.shape=torch.Size([1032, 4096]) residual=None
INFO 10-02 18:41:27 metrics.py:335] Avg prompt throughput: 13407.6 tokens/s, Avg generation throughput: 117.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 76.4
INFO 10-02 18:41:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.1974 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:41:27 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1033]), positions.shape=torch.Size([1033]) hidden_states.shape=torch.Size([1033, 4096]) residual=None
INFO 10-02 18:41:27 metrics.py:335] Avg prompt throughput: 13335.1 tokens/s, Avg generation throughput: 130.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 76.8
INFO 10-02 18:41:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.6053 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:41:27 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1034]), positions.shape=torch.Size([1034]) hidden_states.shape=torch.Size([1034, 4096]) residual=None
INFO 10-02 18:41:27 metrics.py:335] Avg prompt throughput: 13241.3 tokens/s, Avg generation throughput: 142.2 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 23.2%, CPU KV cache usage: 0.0%, Interval(ms): 77.3
INFO 10-02 18:41:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.1127 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:41:27 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1035]), positions.shape=torch.Size([1035]) hidden_states.shape=torch.Size([1035, 4096]) residual=None
INFO 10-02 18:41:27 metrics.py:335] Avg prompt throughput: 13170.1 tokens/s, Avg generation throughput: 154.3 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 25.3%, CPU KV cache usage: 0.0%, Interval(ms): 77.8
INFO 10-02 18:41:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.5621 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:41:27 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1036]), positions.shape=torch.Size([1036]) hidden_states.shape=torch.Size([1036, 4096]) residual=None
INFO 10-02 18:41:27 metrics.py:335] Avg prompt throughput: 13108.6 tokens/s, Avg generation throughput: 166.4 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 27.4%, CPU KV cache usage: 0.0%, Interval(ms): 78.1
INFO 10-02 18:41:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.9228 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:41:27 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1037]), positions.shape=torch.Size([1037]) hidden_states.shape=torch.Size([1037, 4096]) residual=None
INFO 10-02 18:41:27 metrics.py:335] Avg prompt throughput: 13033.2 tokens/s, Avg generation throughput: 178.2 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 29.5%, CPU KV cache usage: 0.0%, Interval(ms): 78.6
INFO 10-02 18:41:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 78.3706 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:41:27 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1038]), positions.shape=torch.Size([1038]) hidden_states.shape=torch.Size([1038, 4096]) residual=None
INFO 10-02 18:41:27 metrics.py:335] Avg prompt throughput: 12797.3 tokens/s, Avg generation throughput: 187.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 31.6%, CPU KV cache usage: 0.0%, Interval(ms): 80.0
INFO 10-02 18:41:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 79.8147 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:41:27 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1039]), positions.shape=torch.Size([1039]) hidden_states.shape=torch.Size([1039, 4096]) residual=None
INFO 10-02 18:41:27 metrics.py:335] Avg prompt throughput: 12761.0 tokens/s, Avg generation throughput: 199.4 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 80.2
INFO 10-02 18:41:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 80.0347 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:41:27 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1040]), positions.shape=torch.Size([1040]) hidden_states.shape=torch.Size([1040, 4096]) residual=None
INFO 10-02 18:41:27 metrics.py:335] Avg prompt throughput: 12722.3 tokens/s, Avg generation throughput: 211.2 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 35.8%, CPU KV cache usage: 0.0%, Interval(ms): 80.5
INFO 10-02 18:41:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 80.2393 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:41:27 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1041]), positions.shape=torch.Size([1041]) hidden_states.shape=torch.Size([1041, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 12661.0 tokens/s, Avg generation throughput: 222.6 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 37.9%, CPU KV cache usage: 0.0%, Interval(ms): 80.9
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 80.6561 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1042]), positions.shape=torch.Size([1042]) hidden_states.shape=torch.Size([1042, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 12597.7 tokens/s, Avg generation throughput: 233.7 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 40.1%, CPU KV cache usage: 0.0%, Interval(ms): 81.3
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 81.0728 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1043]), positions.shape=torch.Size([1043]) hidden_states.shape=torch.Size([1043, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 12528.7 tokens/s, Avg generation throughput: 244.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 81.7
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 81.5084 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 884.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3918 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 885.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3639 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 883.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4388 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 885.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3794 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 884.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3913 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 885.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3746 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 882.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4557 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 884.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3970 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 882.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4438 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 883.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4302 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 883.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4204 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 883.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.7%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4304 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 883.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.7%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4345 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.7%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4991 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4907 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 881.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.9%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4676 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 882.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.9%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4569 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 881.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.0%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4824 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.0%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4957 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.0%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4841 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 878.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5635 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4867 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4891 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4986 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4960 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 877.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5632 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 878.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5527 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 875.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5987 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 877.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5644 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 877.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5685 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 877.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5637 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 874.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6340 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 876.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:41:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5978 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:28 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6316 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.7%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6705 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 874.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.7%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6552 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 874.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.7%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6417 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 874.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6564 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 874.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6603 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6552 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.9%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6748 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 871.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.9%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7213 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.9%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6772 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 872.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.0%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6884 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 872.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.0%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6991 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 871.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7003 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6729 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 871.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7275 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 869.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7904 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 871.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7273 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 872.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7156 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 869.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7804 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 871.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7189 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 870.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7671 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 869.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7895 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8300 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 868.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8138 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 866.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8741 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 868.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7976 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8317 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8472 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 866.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8512 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8302 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 864.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9099 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 865.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8825 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 865.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8980 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 863.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9378 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 864.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9235 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 864.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9113 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 863.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9435 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 863.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9292 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 863.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9449 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 863.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9223 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 864.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9158 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9566 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9664 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9831 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:29 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0289 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 861.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0005 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 861.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9814 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 861.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9900 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9745 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 859.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0253 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 861.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9976 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 861.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9959 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0186 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0143 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0181 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 850.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.5
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.2558 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0815 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 859.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0393 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0730 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 859.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0577 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 859.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0613 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 855.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1171 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0725 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 856.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1261 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 856.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1259 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 855.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1485 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 853.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.2162 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1552 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 855.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1652 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1678 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1860 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 855.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1669 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1814 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 853.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.2170 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 853.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.2165 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 852.1 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 41.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.5
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.2511 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([18]), positions.shape=torch.Size([18]) hidden_states.shape=torch.Size([18, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 791.6 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4164 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 764.4 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.2
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.9867 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 745.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2259 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 707.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9696 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 669.7 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6661 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 670.0 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.4
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.1712 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 632.3 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.7%, CPU KV cache usage: 0.0%, Interval(ms): 19.0
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7397 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 592.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3532 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 556.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7321 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 513.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3030 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 470.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7968 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.5
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.2761 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-02 18:41:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 374.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.0
INFO 10-02 18:41:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8012 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:41:30 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-02 18:41:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 318.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-02 18:41:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4607 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:41:31 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-02 18:41:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 260.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.3
INFO 10-02 18:41:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.1191 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:41:31 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-02 18:41:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%, Interval(ms): 14.8
INFO 10-02 18:41:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.6129 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:41:31 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-02 18:41:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 136.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-02 18:41:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4267 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:41:31 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:41:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:41:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.9 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-02 18:41:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5345 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6429574, last_token_time=1727894490.7375247, first_scheduled_time=1727894486.6567566, first_token_time=1727894486.812855, time_in_queue=0.013799190521240234, finished_time=1727894490.7374444, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6456785, last_token_time=1727894490.7375247, first_scheduled_time=1727894486.6567566, first_token_time=1727894486.812855, time_in_queue=0.011078119277954102, finished_time=1727894490.7374508, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6463137, last_token_time=1727894490.7602625, first_scheduled_time=1727894486.8134007, first_token_time=1727894486.9013608, time_in_queue=0.16708707809448242, finished_time=1727894490.7601888, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6468725, last_token_time=1727894490.7825017, first_scheduled_time=1727894486.9018078, first_token_time=1727894486.9763594, time_in_queue=0.25493526458740234, finished_time=1727894490.7824335, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6474226, last_token_time=1727894490.8039722, first_scheduled_time=1727894486.9768596, first_token_time=1727894487.0518131, time_in_queue=0.3294370174407959, finished_time=1727894490.8039086, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6480312, last_token_time=1727894490.8251858, first_scheduled_time=1727894487.052275, first_token_time=1727894487.1274514, time_in_queue=0.40424370765686035, finished_time=1727894490.825124, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6486046, last_token_time=1727894490.84609, first_scheduled_time=1727894487.127949, first_token_time=1727894487.2031968, time_in_queue=0.47934436798095703, finished_time=1727894490.8460333, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6491532, last_token_time=1727894490.8654933, first_scheduled_time=1727894487.203704, first_token_time=1727894487.2792902, time_in_queue=0.5545508861541748, finished_time=1727894490.8654406, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6496964, last_token_time=1727894490.8844721, first_scheduled_time=1727894487.2798092, first_token_time=1727894487.3556464, time_in_queue=0.630112886428833, finished_time=1727894490.8844242, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6502392, last_token_time=1727894490.9030545, first_scheduled_time=1727894487.356195, first_token_time=1727894487.4324129, time_in_queue=0.7059557437896729, finished_time=1727894490.903009, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6508217, last_token_time=1727894490.9210122, first_scheduled_time=1727894487.433034, first_token_time=1727894487.5097258, time_in_queue=0.7822122573852539, finished_time=1727894490.9209673, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6513605, last_token_time=1727894490.9385371, first_scheduled_time=1727894487.5103292, first_token_time=1727894487.5874684, time_in_queue=0.8589687347412109, finished_time=1727894490.9385, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6519172, last_token_time=1727894490.9555526, first_scheduled_time=1727894487.588088, first_token_time=1727894487.6655476, time_in_queue=0.9361708164215088, finished_time=1727894490.9555194, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6524897, last_token_time=1727894490.9720483, first_scheduled_time=1727894487.666213, first_token_time=1727894487.7441154, time_in_queue=1.013723373413086, finished_time=1727894490.972019, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6530447, last_token_time=1727894490.9880705, first_scheduled_time=1727894487.7447877, first_token_time=1727894487.8241036, time_in_queue=1.091742992401123, finished_time=1727894490.988044, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.653651, last_token_time=1727894491.0037522, first_scheduled_time=1727894487.8248205, first_token_time=1727894487.904304, time_in_queue=1.1711695194244385, finished_time=1727894491.0037308, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6542206, last_token_time=1727894491.0190818, first_scheduled_time=1727894487.9051123, first_token_time=1727894487.9847999, time_in_queue=1.2508916854858398, finished_time=1727894491.019065, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6547732, last_token_time=1727894491.0338974, first_scheduled_time=1727894487.985571, first_token_time=1727894488.0656507, time_in_queue=1.3307976722717285, finished_time=1727894491.033884, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6553454, last_token_time=1727894491.0485592, first_scheduled_time=1727894488.066443, first_token_time=1727894488.1469243, time_in_queue=1.411097526550293, finished_time=1727894491.0485506, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894486.6559122, last_token_time=1727894491.0632865, first_scheduled_time=1727894488.1477382, first_token_time=1727894488.2286358, time_in_queue=1.491826057434082, finished_time=1727894491.063284, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 23040
End-to-End latency: 4.42 seconds
Throughput: 4.52 requests/s, 5212.02 tokens/s
Per_token_time: 0.192 ms

Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='fixed', input_len=1024, output_len=128, min_len=10, max_len=50, prefill_to_decode_ratio=6.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=2048, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=2048, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Selected required requests 20
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Running vLLM with default batching strategy. max_num_batched_tokens=2048, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'finetuned', 'original_max_position_embeddings', 'type'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-02 18:29:23 config.py:654] [SchedulerConfig] max_num_batched_tokens: 2048 chunked_prefill_enabled: False
INFO 10-02 18:29:23 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
/home/jieye/viper2/vllm_v0_4_2_mine/vllm/executor/gpu_executor.py:36: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 10-02 18:29:24 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-02 18:29:29 selector.py:27] Using FlashAttention-2 backend.
INFO 10-02 18:29:52 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-02 18:29:52 model_runner.py:183] Loaded model: 
INFO 10-02 18:29:52 model_runner.py:183]  LlamaForCausalLM(
INFO 10-02 18:29:52 model_runner.py:183]   (model): LlamaModel(
INFO 10-02 18:29:52 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-02 18:29:52 model_runner.py:183]     (layers): ModuleList(
INFO 10-02 18:29:52 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-02 18:29:52 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-02 18:29:52 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:29:52 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:29:52 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-02 18:29:52 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-02 18:29:52 model_runner.py:183]         )
INFO 10-02 18:29:52 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-02 18:29:52 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:29:52 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:29:52 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-02 18:29:52 model_runner.py:183]         )
INFO 10-02 18:29:52 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:29:52 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:29:52 model_runner.py:183]       )
INFO 10-02 18:29:52 model_runner.py:183]     )
INFO 10-02 18:29:52 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:29:52 model_runner.py:183]   )
INFO 10-02 18:29:52 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-02 18:29:52 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-02 18:29:52 model_runner.py:183]   (sampler): Sampler()
INFO 10-02 18:29:52 model_runner.py:183] )
INFO 10-02 18:29:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:29:52 worker.py:164] Peak: 13.332 GB, Initial: 38.980 GB, Free: 25.648 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.092 GB
INFO 10-02 18:29:52 gpu_executor.py:117] # GPU blocks: 3083, # CPU blocks: 8192
INFO 10-02 18:30:24 worker.py:189] _init_cache_engine took 24.0859 GB
INFO 10-02 18:30:24 scheduler.py:307] Scheduler initialized with prompt limit: 2048
INFO 10-02 18:30:24 Start warmup...
INFO 10-02 18:30:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:24 metrics.py:335] Avg prompt throughput: 565.2 tokens/s, Avg generation throughput: 28.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 18:30:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5457 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:30:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:30:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 68.3
INFO 10-02 18:30:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 67.7912 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:30:29 Start benchmarking...
INFO 10-02 18:30:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:30:29 metrics.py:335] Avg prompt throughput: 11340.1 tokens/s, Avg generation throughput: 11.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%, Interval(ms): 180.6
INFO 10-02 18:30:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 156.4822 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:30:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:30:29 metrics.py:335] Avg prompt throughput: 15944.5 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 8.3%, CPU KV cache usage: 0.0%, Interval(ms): 128.4
INFO 10-02 18:30:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 128.2141 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:30:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:30:29 metrics.py:335] Avg prompt throughput: 16402.0 tokens/s, Avg generation throughput: 16.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 124.9
INFO 10-02 18:30:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 124.6884 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:30:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:30:29 metrics.py:335] Avg prompt throughput: 16457.8 tokens/s, Avg generation throughput: 16.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 16.6%, CPU KV cache usage: 0.0%, Interval(ms): 124.4
INFO 10-02 18:30:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 124.2738 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:30:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:30:29 metrics.py:335] Avg prompt throughput: 16485.6 tokens/s, Avg generation throughput: 16.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 20.8%, CPU KV cache usage: 0.0%, Interval(ms): 124.2
INFO 10-02 18:30:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 124.0237 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:30:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 16496.2 tokens/s, Avg generation throughput: 16.1 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 24.9%, CPU KV cache usage: 0.0%, Interval(ms): 124.1
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 123.9817 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 16480.1 tokens/s, Avg generation throughput: 16.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 124.3
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 124.0993 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 16489.7 tokens/s, Avg generation throughput: 16.1 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 33.2%, CPU KV cache usage: 0.0%, Interval(ms): 124.2
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 124.0346 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 16322.0 tokens/s, Avg generation throughput: 15.9 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 125.5
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 125.3073 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 16205.2 tokens/s, Avg generation throughput: 15.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 41.5%, CPU KV cache usage: 0.0%, Interval(ms): 126.4
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 126.2219 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 884.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4488 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 882.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3837 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 866.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8550 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 884.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3849 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 886.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3382 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 885.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3799 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 885.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3484 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 887.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.5
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3262 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 887.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.5
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3176 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 883.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3854 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 888.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.5
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.2967 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 887.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.5
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3196 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 886.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3455 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 887.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.5
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3215 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 886.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3348 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 885.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3603 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 878.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5034 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 883.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4280 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 882.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4385 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 882.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4531 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 882.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:30:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4316 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 882.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4419 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 883.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4271 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4991 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 884.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4116 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 882.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4390 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 882.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4462 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 882.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4380 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 882.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4466 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 883.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4204 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5015 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 881.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4447 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 878.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5527 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 877.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5685 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 878.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5503 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 878.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5587 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 878.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5582 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 875.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6266 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 876.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5627 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 876.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5973 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 878.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5418 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 877.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5770 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 877.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5754 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 877.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5749 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 876.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5930 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 876.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5890 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 878.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5604 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 877.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5885 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6760 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6784 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6920 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 870.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7518 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6686 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 872.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6719 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6841 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6867 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6834 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6786 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 870.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7501 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 872.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6903 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 870.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7077 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 872.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7120 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6784 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6839 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:30:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8217 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 866.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8584 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8274 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 866.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8698 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 866.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8500 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 868.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7983 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 868.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8152 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 866.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8596 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8388 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8376 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8333 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 866.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8186 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 865.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8851 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8252 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8450 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 866.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8550 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 863.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9506 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 863.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9485 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 861.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9497 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 863.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9526 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9738 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9683 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0026 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 863.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9480 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9640 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 861.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9871 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 863.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9318 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9704 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 863.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9402 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9757 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9611 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 861.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9819 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 851.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.5
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.2778 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 857.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0718 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 859.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0432 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 857.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0980 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0768 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 859.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0451 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0751 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0660 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 856.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0923 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 859.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0584 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0684 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 856.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:30:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1297 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 859.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0594 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0675 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 859.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0570 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 855.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1478 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 852.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.2239 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1948 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 853.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.2148 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1798 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1783 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 852.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.5
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.2432 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1946 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 852.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.5
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.2105 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1957 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 853.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.2110 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 853.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.2000 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 855.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1619 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 855.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1614 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 853.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.2108 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:30:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:30:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 840.9 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.8
INFO 10-02 18:30:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.5269 resumed_reqs=0, running_reqs=20 raw_running=20
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2085693, last_token_time=1727893833.4261804, first_scheduled_time=1727893829.2227337, first_token_time=1727893829.3788984, time_in_queue=0.014164447784423828, finished_time=1727893833.426104, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2115867, last_token_time=1727893833.4261804, first_scheduled_time=1727893829.2227337, first_token_time=1727893829.3788984, time_in_queue=0.011147022247314453, finished_time=1727893833.4261105, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2121975, last_token_time=1727893833.4261804, first_scheduled_time=1727893829.379393, first_token_time=1727893829.507391, time_in_queue=0.16719555854797363, finished_time=1727893833.4261153, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2127757, last_token_time=1727893833.4261804, first_scheduled_time=1727893829.379393, first_token_time=1727893829.507391, time_in_queue=0.16661739349365234, finished_time=1727893833.4261193, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2133424, last_token_time=1727893833.4261804, first_scheduled_time=1727893829.5077927, first_token_time=1727893829.632243, time_in_queue=0.2944502830505371, finished_time=1727893833.4261234, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2140055, last_token_time=1727893833.4261804, first_scheduled_time=1727893829.5077927, first_token_time=1727893829.632243, time_in_queue=0.29378724098205566, finished_time=1727893833.426127, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2145545, last_token_time=1727893833.4261804, first_scheduled_time=1727893829.6326337, first_token_time=1727893829.756705, time_in_queue=0.418079137802124, finished_time=1727893833.4261308, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2151303, last_token_time=1727893833.4261804, first_scheduled_time=1727893829.6326337, first_token_time=1727893829.756705, time_in_queue=0.41750335693359375, finished_time=1727893833.4261343, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2156785, last_token_time=1727893833.4261804, first_scheduled_time=1727893829.7571154, first_token_time=1727893829.8809314, time_in_queue=0.5414369106292725, finished_time=1727893833.426138, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.216225, last_token_time=1727893833.4261804, first_scheduled_time=1727893829.7571154, first_token_time=1727893829.8809314, time_in_queue=0.5408904552459717, finished_time=1727893833.4261417, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2168167, last_token_time=1727893833.4261804, first_scheduled_time=1727893829.8813145, first_token_time=1727893830.0050836, time_in_queue=0.6644978523254395, finished_time=1727893833.4261453, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2173576, last_token_time=1727893833.4261804, first_scheduled_time=1727893829.8813145, first_token_time=1727893830.0050836, time_in_queue=0.663956880569458, finished_time=1727893833.426149, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2179444, last_token_time=1727893833.4261804, first_scheduled_time=1727893830.0054586, first_token_time=1727893830.1293511, time_in_queue=0.7875142097473145, finished_time=1727893833.4261525, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2185216, last_token_time=1727893833.4261804, first_scheduled_time=1727893830.0054586, first_token_time=1727893830.1293511, time_in_queue=0.7869369983673096, finished_time=1727893833.4261563, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2191164, last_token_time=1727893833.4261804, first_scheduled_time=1727893830.1297297, first_token_time=1727893830.2535522, time_in_queue=0.9106132984161377, finished_time=1727893833.4261599, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2196772, last_token_time=1727893833.4261804, first_scheduled_time=1727893830.1297297, first_token_time=1727893830.2535522, time_in_queue=0.9100525379180908, finished_time=1727893833.4261637, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2202446, last_token_time=1727893833.4261804, first_scheduled_time=1727893830.2539277, first_token_time=1727893830.3790236, time_in_queue=1.0336830615997314, finished_time=1727893833.4261675, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.220821, last_token_time=1727893833.4261804, first_scheduled_time=1727893830.2539277, first_token_time=1727893830.3790236, time_in_queue=1.033106803894043, finished_time=1727893833.4261713, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.2213948, last_token_time=1727893833.4261804, first_scheduled_time=1727893830.3793945, first_token_time=1727893830.505404, time_in_queue=1.1579997539520264, finished_time=1727893833.4261749, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893829.222003, last_token_time=1727893833.4261804, first_scheduled_time=1727893830.3793945, first_token_time=1727893830.505404, time_in_queue=1.1573915481567383, finished_time=1727893833.4261785, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 23040
End-to-End latency: 4.22 seconds
Throughput: 4.74 requests/s, 5461.73 tokens/s
Per_token_time: 0.183 ms

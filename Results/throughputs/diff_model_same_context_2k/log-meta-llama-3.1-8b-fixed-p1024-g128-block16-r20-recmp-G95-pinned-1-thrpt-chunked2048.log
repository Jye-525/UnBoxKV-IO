Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='fixed', input_len=1024, output_len=128, min_len=10, max_len=50, prefill_to_decode_ratio=6.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=2048, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=2048, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Selected required requests 20
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Chunked prefill is enabled. max_num_batched_tokens=2048, max_num_batched_seqs=256
rope_scaling = {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}
INFO 10-02 18:50:24 config.py:654] [SchedulerConfig] max_num_batched_tokens: 2048 chunked_prefill_enabled: True
INFO 10-02 18:50:24 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
/home/jieye/viper2/vllm_v0_4_2_mine/vllm/executor/gpu_executor.py:36: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 10-02 18:50:24 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-02 18:50:29 selector.py:27] Using FlashAttention-2 backend.
INFO 10-02 18:50:44 model_runner.py:180] Loading model weights took 14.9888 GB
INFO 10-02 18:50:44 model_runner.py:183] Loaded model: 
INFO 10-02 18:50:44 model_runner.py:183]  LlamaForCausalLM(
INFO 10-02 18:50:44 model_runner.py:183]   (model): LlamaModel(
INFO 10-02 18:50:44 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=128256, embedding_dim=4096, org_vocab_size=128256, num_embeddings_padded=128256, tp_size=1)
INFO 10-02 18:50:44 model_runner.py:183]     (layers): ModuleList(
INFO 10-02 18:50:44 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-02 18:50:44 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-02 18:50:44 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:50:44 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:50:44 model_runner.py:183]           (rotary_emb): Llama3RotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=131072, base=500000.0, is_neox_style=True)
INFO 10-02 18:50:44 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845)
INFO 10-02 18:50:44 model_runner.py:183]         )
INFO 10-02 18:50:44 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-02 18:50:44 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=28672, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:50:44 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=14336, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:50:44 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-02 18:50:44 model_runner.py:183]         )
INFO 10-02 18:50:44 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:50:44 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:50:44 model_runner.py:183]       )
INFO 10-02 18:50:44 model_runner.py:183]     )
INFO 10-02 18:50:44 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:50:44 model_runner.py:183]   )
INFO 10-02 18:50:44 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=128256, embedding_dim=4096, org_vocab_size=128256, num_embeddings_padded=128256, tp_size=1)
INFO 10-02 18:50:44 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=128256, forg_vocab_size=128256, scale=1.0, logits_as_input=False)
INFO 10-02 18:50:44 model_runner.py:183]   (sampler): Sampler()
INFO 10-02 18:50:44 model_runner.py:183] )
INFO 10-02 18:50:44 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:50:44 worker.py:164] Peak: 16.531 GB, Initial: 38.980 GB, Free: 22.448 GB, Total: 39.394 GB,               cache_block_size: 2097152 Bytes, available GPU for KV cache: 20.893 GB
INFO 10-02 18:50:44 gpu_executor.py:117] # GPU blocks: 10697, # CPU blocks: 32768
INFO 10-02 18:51:16 worker.py:189] _init_cache_engine took 20.8926 GB
INFO 10-02 18:51:16 scheduler.py:307] Scheduler initialized with prompt limit: 2048
INFO 10-02 18:51:16 Start warmup...
INFO 10-02 18:51:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:18 metrics.py:335] Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 1941.0
INFO 10-02 18:51:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1934.6757 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:51:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:51:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.9 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 52.8
INFO 10-02 18:51:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 52.3665 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:51:23 Start benchmarking...
INFO 10-02 18:51:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:51:25 metrics.py:335] Avg prompt throughput: 1154.2 tokens/s, Avg generation throughput: 1.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%, Interval(ms): 1774.4
INFO 10-02 18:51:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1746.8863 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:51:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:51:25 metrics.py:335] Avg prompt throughput: 13588.0 tokens/s, Avg generation throughput: 19.9 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%, Interval(ms): 150.6
INFO 10-02 18:51:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 150.3012 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:51:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:51:25 metrics.py:335] Avg prompt throughput: 13938.6 tokens/s, Avg generation throughput: 34.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%, Interval(ms): 146.7
INFO 10-02 18:51:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 146.5037 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:51:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:51:25 metrics.py:335] Avg prompt throughput: 14016.8 tokens/s, Avg generation throughput: 48.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%, Interval(ms): 145.8
INFO 10-02 18:51:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 145.5638 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:51:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:51:25 metrics.py:335] Avg prompt throughput: 13854.1 tokens/s, Avg generation throughput: 61.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%, Interval(ms): 147.3
INFO 10-02 18:51:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 147.1226 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:51:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:51:25 metrics.py:335] Avg prompt throughput: 13800.7 tokens/s, Avg generation throughput: 74.5 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 147.7
INFO 10-02 18:51:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 147.5434 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:51:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 13764.3 tokens/s, Avg generation throughput: 87.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 8.5%, CPU KV cache usage: 0.0%, Interval(ms): 148.0
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 147.7859 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 13815.0 tokens/s, Avg generation throughput: 101.8 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 9.7%, CPU KV cache usage: 0.0%, Interval(ms): 147.3
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 147.0952 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 13650.0 tokens/s, Avg generation throughput: 114.1 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 10.9%, CPU KV cache usage: 0.0%, Interval(ms): 148.9
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 148.7181 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 13610.0 tokens/s, Avg generation throughput: 127.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.1%, CPU KV cache usage: 0.0%, Interval(ms): 149.2
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 149.0107 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([101]), positions.shape=torch.Size([101]) hidden_states.shape=torch.Size([101, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 2719.4 tokens/s, Avg generation throughput: 663.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.1%, CPU KV cache usage: 0.0%, Interval(ms): 30.2
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 29.9253 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7713 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7472 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7584 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 956.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7021 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7679 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7298 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7477 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7305 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 956.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7031 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7593 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7653 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7281 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7596 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7555 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7293 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7505 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8547 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7367 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7412 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7887 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7393 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 955.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:51:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7269 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7660 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8545 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8163 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8013 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8039 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8197 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7596 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7989 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 904.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.9092 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8242 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8256 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8263 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8297 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8213 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8292 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8750 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7968 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7930 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7989 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8626 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8211 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8139 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8638 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8397 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8111 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7975 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7853 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7930 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8154 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8592 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8628 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8817 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9055 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8871 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9374 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9136 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9482 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9701 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9119 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9262 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9489 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9692 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9908 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9572 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9389 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9279 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:51:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9339 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9913 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0230 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0214 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0018 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0214 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9830 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9637 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0063 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9775 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9746 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0528 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0371 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0295 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0023 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0285 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0047 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9956 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0152 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0438 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0483 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0509 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 936.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1077 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0500 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0671 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0302 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0204 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0447 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0388 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0204 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0721 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 938.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0781 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 938.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0905 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 936.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1146 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 937.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0805 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0335 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9851 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0311 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0538 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0278 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 938.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0905 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0464 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0769 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 937.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1267 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0414 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9918 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0567 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:51:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0168 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:51:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.0 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:51:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0590 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:51:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([18]), positions.shape=torch.Size([18]) hidden_states.shape=torch.Size([18, 4096]) residual=None
INFO 10-02 18:51:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.1 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:51:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5486 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:51:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-02 18:51:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 822.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.7
INFO 10-02 18:51:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.4155 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:51:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-02 18:51:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 777.0 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.8%, CPU KV cache usage: 0.0%, Interval(ms): 19.3
INFO 10-02 18:51:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.0129 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:51:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-02 18:51:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 724.6 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-02 18:51:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6611 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:51:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-02 18:51:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 622.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.7
INFO 10-02 18:51:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.4034 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:51:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-02 18:51:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 524.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-02 18:51:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9003 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:51:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-02 18:51:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 412.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 10-02 18:51:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7322 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:51:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-02 18:51:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-02 18:51:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.4196 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:51:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-02 18:51:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 184.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-02 18:51:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.0527 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:51:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:51:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:51:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.8 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-02 18:51:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6875 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.404987, last_token_time=1727895089.0044956, first_scheduled_time=1727895083.4227247, first_token_time=1727895085.169139, time_in_queue=0.017737627029418945, finished_time=1727895089.0044155, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.4079614, last_token_time=1727895089.0044956, first_scheduled_time=1727895083.4227247, first_token_time=1727895085.169139, time_in_queue=0.014763355255126953, finished_time=1727895089.0044217, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.4087355, last_token_time=1727895089.0253742, first_scheduled_time=1727895085.1697857, first_token_time=1727895085.3197274, time_in_queue=1.7610502243041992, finished_time=1727895089.0252976, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.4096105, last_token_time=1727895089.0460503, first_scheduled_time=1727895085.1697857, first_token_time=1727895085.4664185, time_in_queue=1.7601752281188965, finished_time=1727895089.04598, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.4103363, last_token_time=1727895089.0460503, first_scheduled_time=1727895085.3202863, first_token_time=1727895085.4664185, time_in_queue=1.9099500179290771, finished_time=1727895089.045986, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.4110415, last_token_time=1727895089.0653558, first_scheduled_time=1727895085.3202863, first_token_time=1727895085.6121392, time_in_queue=1.9092447757720947, finished_time=1727895089.0652943, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.4117486, last_token_time=1727895089.0653558, first_scheduled_time=1727895085.4670181, first_token_time=1727895085.6121392, time_in_queue=2.055269479751587, finished_time=1727895089.0653007, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.412457, last_token_time=1727895089.083296, first_scheduled_time=1727895085.4670181, first_token_time=1727895085.759426, time_in_queue=2.054561138153076, finished_time=1727895089.083243, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.4139175, last_token_time=1727895089.083296, first_scheduled_time=1727895085.612743, first_token_time=1727895085.759426, time_in_queue=2.1988253593444824, finished_time=1727895089.0832493, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.4147022, last_token_time=1727895089.1009703, first_scheduled_time=1727895085.612743, first_token_time=1727895085.907128, time_in_queue=2.198040723800659, finished_time=1727895089.100924, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.4153843, last_token_time=1727895089.1009703, first_scheduled_time=1727895085.760088, first_token_time=1727895085.907128, time_in_queue=2.3447036743164062, finished_time=1727895089.10093, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.4160633, last_token_time=1727895089.118132, first_scheduled_time=1727895085.760088, first_token_time=1727895086.0550907, time_in_queue=2.344024658203125, finished_time=1727895089.118095, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.416738, last_token_time=1727895089.118132, first_scheduled_time=1727895085.9078193, first_token_time=1727895086.0550907, time_in_queue=2.4910812377929688, finished_time=1727895089.118101, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.417411, last_token_time=1727895089.1351225, first_scheduled_time=1727895085.9078193, first_token_time=1727895086.2023485, time_in_queue=2.490408182144165, finished_time=1727895089.1350935, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.4181616, last_token_time=1727895089.1351225, first_scheduled_time=1727895086.0558217, first_token_time=1727895086.2023485, time_in_queue=2.637660026550293, finished_time=1727895089.1350996, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.41888, last_token_time=1727895089.1518261, first_scheduled_time=1727895086.0558217, first_token_time=1727895086.3512535, time_in_queue=2.63694167137146, finished_time=1727895089.1518044, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.4195912, last_token_time=1727895089.1518261, first_scheduled_time=1727895086.203147, first_token_time=1727895086.3512535, time_in_queue=2.783555746078491, finished_time=1727895089.1518106, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.4202828, last_token_time=1727895089.1681216, first_scheduled_time=1727895086.203147, first_token_time=1727895086.5004416, time_in_queue=2.7828640937805176, finished_time=1727895089.1681082, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.4210455, last_token_time=1727895089.1681216, first_scheduled_time=1727895086.3521695, first_token_time=1727895086.5004416, time_in_queue=2.931123971939087, finished_time=1727895089.1681147, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895083.4217446, last_token_time=1727895089.1840444, first_scheduled_time=1727895086.3521695, first_token_time=1727895086.530591, time_in_queue=2.930424928665161, finished_time=1727895089.1840417, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 23040
End-to-End latency: 5.78 seconds
Throughput: 3.46 requests/s, 3986.66 tokens/s
Per_token_time: 0.251 ms

Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='fixed', input_len=1024, output_len=128, min_len=10, max_len=50, prefill_to_decode_ratio=6.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=2048, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=512, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Selected required requests 20
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Chunked prefill is enabled. max_num_batched_tokens=512, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings', 'finetuned', 'type'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-02 18:48:00 config.py:654] [SchedulerConfig] max_num_batched_tokens: 512 chunked_prefill_enabled: True
INFO 10-02 18:48:00 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
/home/jieye/viper2/vllm_v0_4_2_mine/vllm/executor/gpu_executor.py:36: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 10-02 18:48:01 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-02 18:48:06 selector.py:27] Using FlashAttention-2 backend.
INFO 10-02 18:48:09 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-02 18:48:09 model_runner.py:183] Loaded model: 
INFO 10-02 18:48:09 model_runner.py:183]  LlamaForCausalLM(
INFO 10-02 18:48:09 model_runner.py:183]   (model): LlamaModel(
INFO 10-02 18:48:09 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-02 18:48:09 model_runner.py:183]     (layers): ModuleList(
INFO 10-02 18:48:09 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-02 18:48:09 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-02 18:48:09 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:48:09 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:48:09 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-02 18:48:09 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-02 18:48:09 model_runner.py:183]         )
INFO 10-02 18:48:09 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-02 18:48:09 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:48:09 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:48:09 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-02 18:48:09 model_runner.py:183]         )
INFO 10-02 18:48:09 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:48:09 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:48:09 model_runner.py:183]       )
INFO 10-02 18:48:09 model_runner.py:183]     )
INFO 10-02 18:48:09 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:48:09 model_runner.py:183]   )
INFO 10-02 18:48:09 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-02 18:48:09 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-02 18:48:09 model_runner.py:183]   (sampler): Sampler()
INFO 10-02 18:48:09 model_runner.py:183] )
INFO 10-02 18:48:10 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:10 worker.py:164] Peak: 13.264 GB, Initial: 38.980 GB, Free: 25.716 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.160 GB
INFO 10-02 18:48:10 gpu_executor.py:117] # GPU blocks: 3092, # CPU blocks: 8192
INFO 10-02 18:48:41 worker.py:189] _init_cache_engine took 24.1875 GB
INFO 10-02 18:48:41 scheduler.py:307] Scheduler initialized with prompt limit: 2048
INFO 10-02 18:48:41 Start warmup...
INFO 10-02 18:48:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:43 metrics.py:335] Avg prompt throughput: 10.6 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%, Interval(ms): 1893.8
INFO 10-02 18:48:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1888.1199 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:48:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:43 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:48:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 48.8
INFO 10-02 18:48:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.4095 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:48:48 Start benchmarking...
INFO 10-02 18:48:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:48 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:50 metrics.py:335] Avg prompt throughput: 315.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 19 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 1624.0
INFO 10-02 18:48:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1600.5158 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:48:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:50 metrics.py:335] Avg prompt throughput: 6156.4 tokens/s, Avg generation throughput: 12.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 19 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 83.2
INFO 10-02 18:48:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 82.8910 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:48:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:50 metrics.py:335] Avg prompt throughput: 10296.4 tokens/s, Avg generation throughput: 20.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%, Interval(ms): 49.6
INFO 10-02 18:48:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.4480 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:48:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:50 metrics.py:335] Avg prompt throughput: 9117.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%, Interval(ms): 56.0
INFO 10-02 18:48:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 55.8398 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:48:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:50 metrics.py:335] Avg prompt throughput: 8663.4 tokens/s, Avg generation throughput: 33.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 17 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%, Interval(ms): 59.0
INFO 10-02 18:48:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 58.8222 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:48:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:50 metrics.py:335] Avg prompt throughput: 11434.3 tokens/s, Avg generation throughput: 44.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 17 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%, Interval(ms): 44.6
INFO 10-02 18:48:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.4293 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:48:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:50 metrics.py:335] Avg prompt throughput: 10570.7 tokens/s, Avg generation throughput: 62.2 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 8.3%, CPU KV cache usage: 0.0%, Interval(ms): 48.2
INFO 10-02 18:48:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.0793 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:48:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:50 metrics.py:335] Avg prompt throughput: 11330.4 tokens/s, Avg generation throughput: 66.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 8.4%, CPU KV cache usage: 0.0%, Interval(ms): 44.9
INFO 10-02 18:48:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.7633 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:48:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:50 metrics.py:335] Avg prompt throughput: 10502.7 tokens/s, Avg generation throughput: 82.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 10.4%, CPU KV cache usage: 0.0%, Interval(ms): 48.5
INFO 10-02 18:48:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.3027 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:48:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:50 metrics.py:335] Avg prompt throughput: 11205.9 tokens/s, Avg generation throughput: 88.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%, Interval(ms): 45.3
INFO 10-02 18:48:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.1355 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:48:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:50 metrics.py:335] Avg prompt throughput: 10446.2 tokens/s, Avg generation throughput: 102.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 48.6
INFO 10-02 18:48:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.4662 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:48:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:50 metrics.py:335] Avg prompt throughput: 11124.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 45.6
INFO 10-02 18:48:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.4078 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:48:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:50 metrics.py:335] Avg prompt throughput: 10333.6 tokens/s, Avg generation throughput: 122.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 14.7%, CPU KV cache usage: 0.0%, Interval(ms): 49.1
INFO 10-02 18:48:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.8970 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:48:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10980.8 tokens/s, Avg generation throughput: 130.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 14.7%, CPU KV cache usage: 0.0%, Interval(ms): 46.1
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.9070 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10216.9 tokens/s, Avg generation throughput: 141.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 16.8%, CPU KV cache usage: 0.0%, Interval(ms): 49.5
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.3524 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10917.5 tokens/s, Avg generation throughput: 151.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 16.8%, CPU KV cache usage: 0.0%, Interval(ms): 46.3
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.0775 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10171.3 tokens/s, Avg generation throughput: 161.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 49.6
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.4757 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10805.9 tokens/s, Avg generation throughput: 171.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 46.6
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.4602 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10066.4 tokens/s, Avg generation throughput: 179.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 50.1
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.8874 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10739.3 tokens/s, Avg generation throughput: 192.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 46.8
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.6516 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10005.0 tokens/s, Avg generation throughput: 198.9 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 50.3
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.0915 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10641.0 tokens/s, Avg generation throughput: 212.0 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 23.2%, CPU KV cache usage: 0.0%, Interval(ms): 47.2
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.9875 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 9918.3 tokens/s, Avg generation throughput: 217.3 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 25.2%, CPU KV cache usage: 0.0%, Interval(ms): 50.6
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.4308 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10490.8 tokens/s, Avg generation throughput: 230.3 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 25.3%, CPU KV cache usage: 0.0%, Interval(ms): 47.8
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.5612 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10380.7 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 27.4%, CPU KV cache usage: 0.0%, Interval(ms): 48.3
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.0707 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10588.8 tokens/s, Avg generation throughput: 254.1 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 27.4%, CPU KV cache usage: 0.0%, Interval(ms): 47.2
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.0254 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10175.7 tokens/s, Avg generation throughput: 264.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 29.5%, CPU KV cache usage: 0.0%, Interval(ms): 49.1
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.9416 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10471.0 tokens/s, Avg generation throughput: 272.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 29.6%, CPU KV cache usage: 0.0%, Interval(ms): 47.7
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.4555 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10275.7 tokens/s, Avg generation throughput: 288.3 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 31.6%, CPU KV cache usage: 0.0%, Interval(ms): 48.6
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.3611 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10207.4 tokens/s, Avg generation throughput: 287.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 31.7%, CPU KV cache usage: 0.0%, Interval(ms): 48.8
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.5861 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10022.3 tokens/s, Avg generation throughput: 301.9 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 49.7
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.4864 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10142.1 tokens/s, Avg generation throughput: 306.1 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 49.0
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.7940 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 9927.0 tokens/s, Avg generation throughput: 319.6 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 35.9%, CPU KV cache usage: 0.0%, Interval(ms): 50.1
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.8598 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:51 metrics.py:335] Avg prompt throughput: 10102.3 tokens/s, Avg generation throughput: 325.9 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 36.0%, CPU KV cache usage: 0.0%, Interval(ms): 49.1
INFO 10-02 18:48:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.8880 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:48:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 10041.9 tokens/s, Avg generation throughput: 344.2 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 38.1%, CPU KV cache usage: 0.0%, Interval(ms): 49.4
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.1843 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 10179.7 tokens/s, Avg generation throughput: 349.6 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 38.1%, CPU KV cache usage: 0.0%, Interval(ms): 48.6
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.4116 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 9924.6 tokens/s, Avg generation throughput: 360.9 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 40.2%, CPU KV cache usage: 0.0%, Interval(ms): 49.9
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.6635 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 10100.7 tokens/s, Avg generation throughput: 368.0 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 40.3%, CPU KV cache usage: 0.0%, Interval(ms): 48.9
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.6887 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 9873.4 tokens/s, Avg generation throughput: 379.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.4%, CPU KV cache usage: 0.0%, Interval(ms): 50.0
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.8178 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 10038.7 tokens/s, Avg generation throughput: 386.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.5%, CPU KV cache usage: 0.0%, Interval(ms): 49.1
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.8865 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([381]), positions.shape=torch.Size([381]) hidden_states.shape=torch.Size([381, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 8473.5 tokens/s, Avg generation throughput: 468.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.5%, CPU KV cache usage: 0.0%, Interval(ms): 42.7
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 42.5022 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4857 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4981 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 878.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5384 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4955 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 877.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.7%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5670 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 879.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.7%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5275 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 879.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5296 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 879.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5265 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 878.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5389 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 876.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.9%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6114 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 876.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.9%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6073 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 876.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.9%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5770 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 876.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.0%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5964 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 876.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.0%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6047 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 875.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6266 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 875.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6233 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 874.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6610 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6347 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 875.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6278 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 875.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6390 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6791 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 874.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6619 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6722 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 870.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7239 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6750 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 874.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6562 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6920 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6724 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.7%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:48:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6905 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 870.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7168 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 869.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7759 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 870.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7487 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 870.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7695 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 871.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7365 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 870.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7489 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 868.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8057 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 869.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7916 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 868.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8052 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 869.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7797 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 869.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7878 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 868.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8007 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 868.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8159 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 869.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7914 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 869.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7795 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8279 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8243 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 865.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8949 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 866.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8672 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 866.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8515 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 865.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8865 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 864.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9087 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 864.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9251 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 864.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9034 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9754 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 863.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9306 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 863.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9328 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 863.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9380 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9640 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9688 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9747 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0236 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 861.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9957 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9716 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9783 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0205 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0098 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0374 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0088 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 857.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1087 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0198 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 859.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0429 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 859.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:48:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0496 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0496 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0751 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 857.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0901 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0870 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 857.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1018 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 857.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1144 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 855.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1404 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 848.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.6
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.3476 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1709 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 855.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1674 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1738 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 855.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1566 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 853.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1998 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 855.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1652 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1912 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.4 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1836 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 822.2 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8248 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 824.9 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8150 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 823.6 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 41.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8500 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([18]), positions.shape=torch.Size([18]) hidden_states.shape=torch.Size([18, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 794.5 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 41.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3892 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([18]), positions.shape=torch.Size([18]) hidden_states.shape=torch.Size([18, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 794.8 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4333 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 761.6 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.3
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.0606 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 764.7 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.9%, CPU KV cache usage: 0.0%, Interval(ms): 22.2
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.0230 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 743.6 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2648 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 744.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.3006 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 707.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9670 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 709.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9529 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 669.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6635 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 671.2 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6628 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 669.9 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.1%, CPU KV cache usage: 0.0%, Interval(ms): 19.4
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.1662 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 669.6 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.8%, CPU KV cache usage: 0.0%, Interval(ms): 19.4
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.2196 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 632.5 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.8%, CPU KV cache usage: 0.0%, Interval(ms): 19.0
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7352 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 634.1 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7345 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 592.1 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3480 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 592.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3675 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 556.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7321 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 558.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.9%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7362 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 513.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.9%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2663 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 515.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.6%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2732 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 469.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.6%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8149 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 472.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7782 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.5
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.2697 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.5
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.2973 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 373.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8210 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 375.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.6%, CPU KV cache usage: 0.0%, Interval(ms): 16.0
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8248 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 319.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.6%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4622 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 319.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4743 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 260.9 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.3
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.1327 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-02 18:48:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 261.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.3
INFO 10-02 18:48:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.1165 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:48:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:54 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-02 18:48:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.9
INFO 10-02 18:48:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.6632 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:48:55 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:55 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-02 18:48:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%, Interval(ms): 14.8
INFO 10-02 18:48:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.6403 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:48:55 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:55 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-02 18:48:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 136.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-02 18:48:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4651 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:48:55 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:55 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-02 18:48:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 137.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-02 18:48:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4489 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:48:55 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:55 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:48:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-02 18:48:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5545 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:48:55 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:48:55 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:48:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-02 18:48:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5659 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.7384193, last_token_time=1727894934.3622484, first_scheduled_time=1727894928.7520683, first_token_time=1727894930.43536, time_in_queue=0.01364898681640625, finished_time=1727894934.3621666, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.7411468, last_token_time=1727894934.4314587, first_scheduled_time=1727894930.4357815, first_token_time=1727894930.6000261, time_in_queue=1.6946346759796143, finished_time=1727894934.4313786, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.7417579, last_token_time=1727894934.4767613, first_scheduled_time=1727894930.5414376, first_token_time=1727894930.6928668, time_in_queue=1.7996797561645508, finished_time=1727894934.4766884, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.74248, last_token_time=1727894934.5213158, first_scheduled_time=1727894930.6450245, first_token_time=1727894930.7862296, time_in_queue=1.9025444984436035, finished_time=1727894934.521247, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.743045, last_token_time=1727894934.5643382, first_scheduled_time=1727894930.7381935, first_token_time=1727894930.8801718, time_in_queue=1.9951484203338623, finished_time=1727894934.5642717, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.7435946, last_token_time=1727894934.6067102, first_scheduled_time=1727894930.8319952, first_token_time=1727894930.9747937, time_in_queue=2.0884006023406982, finished_time=1727894934.6066496, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.7441287, last_token_time=1727894934.648476, first_scheduled_time=1727894930.9262104, first_token_time=1727894931.070377, time_in_queue=2.182081699371338, finished_time=1727894934.6484196, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.7446694, last_token_time=1727894934.6872954, first_scheduled_time=1727894931.0213656, first_token_time=1727894931.1662614, time_in_queue=2.27669620513916, finished_time=1727894934.6872418, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.7452536, last_token_time=1727894934.725191, first_scheduled_time=1727894931.1171532, first_token_time=1727894931.2629557, time_in_queue=2.3718996047973633, finished_time=1727894934.7251413, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.7457914, last_token_time=1727894934.762323, first_scheduled_time=1727894931.2134554, first_token_time=1727894931.3600457, time_in_queue=2.4676640033721924, finished_time=1727894934.7622778, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.746316, last_token_time=1727894934.7981963, first_scheduled_time=1727894931.310369, first_token_time=1727894931.4578137, time_in_queue=2.5640530586242676, finished_time=1727894934.7981527, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.746835, last_token_time=1727894934.8331716, first_scheduled_time=1727894931.4078252, first_token_time=1727894931.553809, time_in_queue=2.6609902381896973, finished_time=1727894934.8331344, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.747362, last_token_time=1727894934.8671584, first_scheduled_time=1727894931.5062063, first_token_time=1727894931.6501393, time_in_queue=2.7588443756103516, finished_time=1727894934.8671253, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.747939, last_token_time=1727894934.9001076, first_scheduled_time=1727894931.6017022, first_token_time=1727894931.7463408, time_in_queue=2.8537631034851074, finished_time=1727894934.9000776, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.748482, last_token_time=1727894934.9321663, first_scheduled_time=1727894931.6984978, first_token_time=1727894931.8448057, time_in_queue=2.9500157833099365, finished_time=1727894934.9321404, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.749024, last_token_time=1727894934.963472, first_scheduled_time=1727894931.795853, first_token_time=1727894931.9438467, time_in_queue=3.0468289852142334, finished_time=1727894934.9634511, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.7495627, last_token_time=1727894934.9940798, first_scheduled_time=1727894931.8945558, first_token_time=1727894932.0423186, time_in_queue=3.1449930667877197, finished_time=1727894934.9940627, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.7501073, last_token_time=1727894935.02377, first_scheduled_time=1727894931.9937325, first_token_time=1727894932.1408038, time_in_queue=3.2436251640319824, finished_time=1727894935.0237572, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.750712, last_token_time=1727894935.0530283, first_scheduled_time=1727894932.0917532, first_token_time=1727894932.2397213, time_in_queue=3.341041326522827, finished_time=1727894935.0530198, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894928.7512581, last_token_time=1727894935.0824811, first_scheduled_time=1727894932.1905403, first_token_time=1727894932.3315337, time_in_queue=3.439282178878784, finished_time=1727894935.0824783, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 23040
End-to-End latency: 6.34 seconds
Throughput: 3.15 requests/s, 3631.62 tokens/s
Per_token_time: 0.275 ms

/home/jieye/venvs/vllm_env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/opt-13b', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/opt-13b', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='fixed', input_len=1024, output_len=128, min_len=10, max_len=50, prefill_to_decode_ratio=6.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=2048, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=1024, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Selected required requests 20
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Chunked prefill is enabled. max_num_batched_tokens=1024, max_num_batched_seqs=256
rope_scaling = None
INFO 10-02 19:02:34 config.py:654] [SchedulerConfig] max_num_batched_tokens: 1024 chunked_prefill_enabled: True
INFO 10-02 19:02:34 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/opt-13b', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/opt-13b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/opt-13b, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
/home/jieye/venvs/vllm_env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/home/jieye/viper2/vllm_v0_4_2_mine/vllm/executor/gpu_executor.py:36: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 10-02 19:02:34 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-02 19:02:39 selector.py:27] Using FlashAttention-2 backend.
INFO 10-02 19:02:46 model_runner.py:180] Loading model weights took 23.9425 GB
INFO 10-02 19:02:46 model_runner.py:183] Loaded model: 
INFO 10-02 19:02:46 model_runner.py:183]  OPTForCausalLM(
INFO 10-02 19:02:46 model_runner.py:183]   (model): OPTModel(
INFO 10-02 19:02:46 model_runner.py:183]     (decoder): OPTDecoder(
INFO 10-02 19:02:46 model_runner.py:183]       (embed_tokens): VocabParallelEmbedding(num_embeddings=50304, embedding_dim=5120, org_vocab_size=50272, num_embeddings_padded=50304, tp_size=1)
INFO 10-02 19:02:46 model_runner.py:183]       (embed_positions): OPTLearnedPositionalEmbedding(2050, 5120)
INFO 10-02 19:02:46 model_runner.py:183]       (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
INFO 10-02 19:02:46 model_runner.py:183]       (layers): ModuleList(
INFO 10-02 19:02:46 model_runner.py:183]         (0-39): 40 x OPTDecoderLayer(
INFO 10-02 19:02:46 model_runner.py:183]           (self_attn): OPTAttention(
INFO 10-02 19:02:46 model_runner.py:183]             (qkv_proj): QKVParallelLinear(in_features=5120, output_features=15360, bias=True, tp_size=1, gather_output=False)
INFO 10-02 19:02:46 model_runner.py:183]             (out_proj): RowParallelLinear(input_features=5120, output_features=5120, bias=True, tp_size=1, reduce_results=True)
INFO 10-02 19:02:46 model_runner.py:183]             (attn): Attention(head_size=128, num_heads=40, num_kv_heads=40, scale=0.08838834764831845)
INFO 10-02 19:02:46 model_runner.py:183]           )
INFO 10-02 19:02:46 model_runner.py:183]           (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
INFO 10-02 19:02:46 model_runner.py:183]           (fc1): ColumnParallelLinear(in_features=5120, output_features=20480, bias=True, tp_size=1, gather_output=False)
INFO 10-02 19:02:46 model_runner.py:183]           (activation_fn): ReLU()
INFO 10-02 19:02:46 model_runner.py:183]           (fc2): RowParallelLinear(input_features=20480, output_features=5120, bias=True, tp_size=1, reduce_results=True)
INFO 10-02 19:02:46 model_runner.py:183]           (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
INFO 10-02 19:02:46 model_runner.py:183]         )
INFO 10-02 19:02:46 model_runner.py:183]       )
INFO 10-02 19:02:46 model_runner.py:183]     )
INFO 10-02 19:02:46 model_runner.py:183]   )
INFO 10-02 19:02:46 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=50272, forg_vocab_size=50272, scale=1.0, logits_as_input=False)
INFO 10-02 19:02:46 model_runner.py:183]   (sampler): Sampler()
INFO 10-02 19:02:46 model_runner.py:183] )
INFO 10-02 19:02:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:02:46 worker.py:164] Peak: 24.859 GB, Initial: 38.980 GB, Free: 14.120 GB, Total: 39.394 GB,               cache_block_size: 13107200 Bytes, available GPU for KV cache: 12.565 GB
INFO 10-02 19:02:46 gpu_executor.py:117] # GPU blocks: 1029, # CPU blocks: 5242
INFO 10-02 19:03:26 worker.py:189] _init_cache_engine took 12.5781 GB
INFO 10-02 19:03:26 scheduler.py:307] Scheduler initialized with prompt limit: 2048
INFO 10-02 19:03:26 Start warmup...
INFO 10-02 19:03:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:26 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([21]), positions.shape=torch.Size([21])
INFO 10-02 19:03:27 metrics.py:335] Avg prompt throughput: 11.3 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%, Interval(ms): 1852.7
INFO 10-02 19:03:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1846.2875 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 19:03:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:27 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1])
INFO 10-02 19:03:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 58.9
INFO 10-02 19:03:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 58.4788 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 19:03:33 Start benchmarking...
INFO 10-02 19:03:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:33 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:34 metrics.py:335] Avg prompt throughput: 612.9 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 19 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%, Interval(ms): 1670.8
INFO 10-02 19:03:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1643.8441 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 19:03:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:34 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:34 metrics.py:335] Avg prompt throughput: 7017.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 145.8
INFO 10-02 19:03:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 145.5069 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 19:03:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:34 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:35 metrics.py:335] Avg prompt throughput: 6907.7 tokens/s, Avg generation throughput: 13.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 17 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 148.1
INFO 10-02 19:03:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 147.9170 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 19:03:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:35 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:35 metrics.py:335] Avg prompt throughput: 6907.9 tokens/s, Avg generation throughput: 20.3 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 25.1%, CPU KV cache usage: 0.0%, Interval(ms): 147.9
INFO 10-02 19:03:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 147.6645 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 19:03:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:35 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:35 metrics.py:335] Avg prompt throughput: 6941.5 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 31.4%, CPU KV cache usage: 0.0%, Interval(ms): 147.1
INFO 10-02 19:03:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 146.9076 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:35 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:35 metrics.py:335] Avg prompt throughput: 6900.3 tokens/s, Avg generation throughput: 33.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 37.7%, CPU KV cache usage: 0.0%, Interval(ms): 147.8
INFO 10-02 19:03:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 147.6290 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:35 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:35 metrics.py:335] Avg prompt throughput: 6866.8 tokens/s, Avg generation throughput: 40.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 44.0%, CPU KV cache usage: 0.0%, Interval(ms): 148.4
INFO 10-02 19:03:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 148.2031 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 19:03:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:35 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:35 metrics.py:335] Avg prompt throughput: 6832.3 tokens/s, Avg generation throughput: 47.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 50.3%, CPU KV cache usage: 0.0%, Interval(ms): 149.0
INFO 10-02 19:03:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 148.8104 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 19:03:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:35 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:35 metrics.py:335] Avg prompt throughput: 6778.0 tokens/s, Avg generation throughput: 53.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 56.7%, CPU KV cache usage: 0.0%, Interval(ms): 150.0
INFO 10-02 19:03:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 149.8625 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 19:03:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:35 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:36 metrics.py:335] Avg prompt throughput: 6786.8 tokens/s, Avg generation throughput: 60.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 63.0%, CPU KV cache usage: 0.0%, Interval(ms): 149.7
INFO 10-02 19:03:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 149.5161 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 19:03:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:36 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:36 metrics.py:335] Avg prompt throughput: 6754.2 tokens/s, Avg generation throughput: 66.5 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 69.3%, CPU KV cache usage: 0.0%, Interval(ms): 150.3
INFO 10-02 19:03:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 150.0368 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 19:03:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:36 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:36 metrics.py:335] Avg prompt throughput: 6730.2 tokens/s, Avg generation throughput: 73.0 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 75.6%, CPU KV cache usage: 0.0%, Interval(ms): 150.7
INFO 10-02 19:03:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 150.4736 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 19:03:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:36 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:36 metrics.py:335] Avg prompt throughput: 6645.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 81.9%, CPU KV cache usage: 0.0%, Interval(ms): 152.4
INFO 10-02 19:03:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 152.2419 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 19:03:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:36 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:36 metrics.py:335] Avg prompt throughput: 6630.3 tokens/s, Avg generation throughput: 85.2 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 88.2%, CPU KV cache usage: 0.0%, Interval(ms): 152.6
INFO 10-02 19:03:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 152.4293 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:36 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=56, required_blocks=64, threshold=10
INFO 10-02 19:03:36 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:36 metrics.py:335] Avg prompt throughput: 6605.5 tokens/s, Avg generation throughput: 91.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 94.6%, CPU KV cache usage: 0.0%, Interval(ms): 153.1
INFO 10-02 19:03:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 152.8423 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:36 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=55, required_blocks=64, threshold=10
INFO 10-02 19:03:36 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([106]), positions.shape=torch.Size([106])
INFO 10-02 19:03:36 metrics.py:335] Avg prompt throughput: 2009.3 tokens/s, Avg generation throughput: 327.6 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 94.7%, CPU KV cache usage: 0.0%, Interval(ms): 45.8
INFO 10-02 19:03:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.5718 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:36 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=54, required_blocks=64, threshold=10
INFO 10-02 19:03:36 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 427.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 94.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8401 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:36 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=53, required_blocks=64, threshold=10
INFO 10-02 19:03:36 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 428.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 94.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:03:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8294 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:36 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=53, required_blocks=64, threshold=10
INFO 10-02 19:03:36 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 428.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 94.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:03:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8396 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:36 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=52, required_blocks=64, threshold=10
INFO 10-02 19:03:36 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 427.7 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 94.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8687 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:36 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=51, required_blocks=64, threshold=10
INFO 10-02 19:03:36 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 428.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8470 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=50, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 427.3 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8704 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=49, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 427.6 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8814 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=48, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9414 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=47, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.9 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9309 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=46, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 427.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9333 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=45, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.6 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9624 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=44, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 427.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9081 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=43, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9340 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=42, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.9 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9407 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=41, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 427.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9200 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=40, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0053 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=39, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.7 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9548 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=38, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0142 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=38, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9448 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=37, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9972 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=36, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.9 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0237 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=35, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.3 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9877 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=34, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.7 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0394 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=33, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9977 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=32, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0108 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=31, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0962 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=30, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.6 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0432 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=29, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1038 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=28, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.4 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0575 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=27, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.3 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0711 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=26, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.7 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0356 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=25, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0995 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:37 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=24, required_blocks=64, threshold=10
INFO 10-02 19:03:37 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0757 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=23, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0907 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=23, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.7 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1150 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=22, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1341 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=21, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1226 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=20, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1815 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=19, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1555 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=18, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1636 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=17, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1768 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=16, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 423.4 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1937 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=15, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1601 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=14, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 423.9 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1825 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=13, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1677 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=12, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 423.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2142 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=11, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.9 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2674 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=10, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 423.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2566 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=9, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 423.3 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2373 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=8, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.9 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2652 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=8, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2712 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=7, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.4 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.3112 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=6, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 423.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2628 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=5, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.4 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.3067 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=4, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2952 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=3, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.6 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2898 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=2, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.3227 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=1, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 421.9 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.6
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.3482 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:320] BlockSpaceManagerV1: number of sequences > free gpu blocks, num_seqs=1, free_gpu_blocks=0
INFO 10-02 19:03:38 scheduler.py:451] Scheduler preempt the lowest-priority sequence groups. Preempted mode: PreemptionMode.RECOMPUTE
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=68, required_blocks=64, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 398.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 93.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8990 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=67, required_blocks=68, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.3 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 93.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:03:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.7762 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:38 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=66, required_blocks=68, threshold=10
INFO 10-02 19:03:38 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 93.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.7943 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=65, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 93.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8160 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=64, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 93.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8115 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=63, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 93.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8244 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=63, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 93.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8396 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=62, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8308 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=62, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.2 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8742 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=61, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.2 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8704 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=60, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8341 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=59, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8380 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=58, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8513 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=57, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8754 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=56, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 398.5 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8973 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=55, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.3 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8656 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=54, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 398.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8966 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=53, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8864 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=52, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8866 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=51, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 398.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9271 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=50, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8814 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=49, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 398.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9154 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=49, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 398.5 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9338 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=48, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9913 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=48, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9958 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=47, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0332 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=46, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0301 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=45, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.5 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0182 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=44, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0602 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=43, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0995 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=42, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:03:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0349 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:39 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=41, required_blocks=68, threshold=10
INFO 10-02 19:03:39 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0673 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=40, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0764 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=39, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0935 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=38, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1226 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=37, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0749 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=36, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0962 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=35, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1014 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=35, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0912 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=34, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.2 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1374 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=34, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1179 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=33, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0816 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=32, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 393.2 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.6
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.4111 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=31, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 97.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1522 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=30, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.3 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 97.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1262 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=29, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 97.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1629 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=28, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 97.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1696 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=27, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 97.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1489 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=26, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 97.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1636 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=25, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 97.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1856 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=24, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 97.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1987 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=23, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 97.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1903 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=22, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 97.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1923 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=21, required_blocks=68, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.3 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 91.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2211 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=25, required_blocks=64, threshold=10
INFO 10-02 19:03:40 scheduler.py:1146] A RECOMPUTE sequence is resumed. recomputed_tokens = 1082, tokens = 0
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:40 sequence.py:503] First token: Recomputed SeqGroup 15 recomputing 1011 tokens take 0.133958 seconds. parallel_reqs = 1 parallel_ids=[15] seqs_dict = {15: Sequence(seq_id=15, status=RUNNING, num_blocks=68)}
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 7498.2 tokens/s, Avg generation throughput: 96.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.6%, CPU KV cache usage: 0.0%, Interval(ms): 134.8
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 134.5708 resumed_reqs=1, running_reqs=14 raw_running=13
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=25, required_blocks=64, threshold=10
INFO 10-02 19:03:40 scheduler.py:1146] A RECOMPUTE sequence is resumed. recomputed_tokens = 1082, tokens = 1011
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([85]), positions.shape=torch.Size([85])
INFO 10-02 19:03:40 sequence.py:488] First token: Recomputed SeqGroup 15 recomputing 72 tokens take 0.046898 seconds. parallel_reqs = 1 parallel_ids=[15] seqs_dict = {15: Sequence(seq_id=15, status=RUNNING, num_blocks=68)}
INFO 10-02 19:03:40 metrics.py:335] Avg prompt throughput: 1510.6 tokens/s, Avg generation throughput: 293.7 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 90.6%, CPU KV cache usage: 0.0%, Interval(ms): 47.7
INFO 10-02 19:03:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.4691 resumed_reqs=1, running_reqs=14 raw_running=13
INFO 10-02 19:03:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:40 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=33, required_blocks=64, threshold=10
INFO 10-02 19:03:40 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:41 metrics.py:335] Avg prompt throughput: 7494.3 tokens/s, Avg generation throughput: 96.4 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 89.8%, CPU KV cache usage: 0.0%, Interval(ms): 134.9
INFO 10-02 19:03:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 134.6605 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:41 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=41, required_blocks=64, threshold=10
INFO 10-02 19:03:41 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:41 metrics.py:335] Avg prompt throughput: 6516.1 tokens/s, Avg generation throughput: 83.7 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 89.0%, CPU KV cache usage: 0.0%, Interval(ms): 155.3
INFO 10-02 19:03:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 155.0477 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:41 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=48, required_blocks=64, threshold=10
INFO 10-02 19:03:41 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:41 metrics.py:335] Avg prompt throughput: 6564.9 tokens/s, Avg generation throughput: 84.3 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 88.3%, CPU KV cache usage: 0.0%, Interval(ms): 154.2
INFO 10-02 19:03:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 153.8918 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
WARNING 10-02 19:03:41 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=55, required_blocks=64, threshold=10
INFO 10-02 19:03:41 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:41 metrics.py:335] Avg prompt throughput: 6571.4 tokens/s, Avg generation throughput: 84.4 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.7%, CPU KV cache usage: 0.0%, Interval(ms): 154.0
INFO 10-02 19:03:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 153.7437 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:41 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024])
INFO 10-02 19:03:41 metrics.py:335] Avg prompt throughput: 6576.1 tokens/s, Avg generation throughput: 84.5 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 87.0%, CPU KV cache usage: 0.0%, Interval(ms): 153.9
INFO 10-02 19:03:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 153.6369 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:03:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:41 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([73]), positions.shape=torch.Size([73])
INFO 10-02 19:03:41 metrics.py:335] Avg prompt throughput: 1353.6 tokens/s, Avg generation throughput: 288.5 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 80.2%, CPU KV cache usage: 0.0%, Interval(ms): 45.1
INFO 10-02 19:03:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.7974 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 19:03:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:41 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12])
INFO 10-02 19:03:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.3 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 73.3%, CPU KV cache usage: 0.0%, Interval(ms): 34.3
INFO 10-02 19:03:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.0135 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 19:03:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:41 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11])
INFO 10-02 19:03:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 326.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 66.3%, CPU KV cache usage: 0.0%, Interval(ms): 33.7
INFO 10-02 19:03:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 33.4420 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 19:03:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:41 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10])
INFO 10-02 19:03:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 314.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 59.3%, CPU KV cache usage: 0.0%, Interval(ms): 31.8
INFO 10-02 19:03:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 31.5685 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 19:03:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:41 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9])
INFO 10-02 19:03:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 289.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 52.3%, CPU KV cache usage: 0.0%, Interval(ms): 31.1
INFO 10-02 19:03:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 30.8392 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 19:03:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:41 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8])
INFO 10-02 19:03:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 263.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.3%, CPU KV cache usage: 0.0%, Interval(ms): 30.3
INFO 10-02 19:03:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 30.0786 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 19:03:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:41 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7])
INFO 10-02 19:03:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 236.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.3%, CPU KV cache usage: 0.0%, Interval(ms): 29.6
INFO 10-02 19:03:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 29.4180 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 19:03:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:41 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.3%, CPU KV cache usage: 0.0%, Interval(ms): 28.6
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.4328 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.3%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0118 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.3%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0373 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 213.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.3%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0080 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.3%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0552 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 213.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.3%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0023 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 213.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0056 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.5%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0347 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.6%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0457 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.8%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0867 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.9%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0788 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.9%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1363 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.9%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1036 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.9%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0709 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.9%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0581 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.9%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0554 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.9%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0406 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.9%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0418 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.9%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0569 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.9%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0676 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.9%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0871 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.9%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0693 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0778 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.1%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0998 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.2%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1012 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1198 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.5%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1255 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.5%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1143 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.5%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0890 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.5%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1179 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.5%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0900 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.5%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1317 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.5%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1193 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.5%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0800 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.5%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0793 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.5%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1031 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:42 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.5%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1093 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.5%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0888 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.6%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1417 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.7%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1222 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.7%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1966 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.9%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1844 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1417 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1637 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1565 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1899 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1756 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1708 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.2354 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1591 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1477 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1579 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1777 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1436 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.1%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1758 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.2%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.2028 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.3%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4692 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 181.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.4%, CPU KV cache usage: 0.0%, Interval(ms): 27.6
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4546 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 181.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.6
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4565 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4959 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.6
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4851 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.6
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4737 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 181.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.6
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4589 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.6
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4799 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 181.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.6
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4601 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5347 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4928 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 181.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.6
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4584 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 181.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.6
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4618 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 181.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.6
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4661 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.6%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4818 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 27.6
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4827 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5424 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5035 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5183 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5159 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5099 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5056 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5104 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5319 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5278 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5140 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5047 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5190 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4918 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5333 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.1%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5395 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.2%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5505 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.3%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5381 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.4%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5736 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5490 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5495 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5302 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5733 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5493 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5512 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5431 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5424 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5524 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6029 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5652 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5493 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.6%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5772 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.7%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6096 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.8%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5886 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.9%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5896 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6434 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:03:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6046 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:03:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6093 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:03:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5991 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:03:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6079 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:03:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6170 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:03:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6113 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:03:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6194 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:03:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6163 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:03:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6842 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:03:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:03:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6444 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:03:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4])
INFO 10-02 19:03:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 147.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.2
INFO 10-02 19:03:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 26.9883 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 19:03:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3])
INFO 10-02 19:03:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.0%, CPU KV cache usage: 0.0%, Interval(ms): 26.6
INFO 10-02 19:03:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 26.4103 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 19:03:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2])
INFO 10-02 19:03:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.0%, CPU KV cache usage: 0.0%, Interval(ms): 25.9
INFO 10-02 19:03:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 25.7282 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 19:03:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 19:03:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1])
INFO 10-02 19:03:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 38.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 26.0
INFO 10-02 19:03:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 25.7988 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0579135, last_token_time=1727895820.805098, first_scheduled_time=1727895813.0751445, first_token_time=1727895814.7184265, time_in_queue=0.017230987548828125, finished_time=1727895820.8050408, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.060987, last_token_time=1727895820.9875941, first_scheduled_time=1727895814.7190142, first_token_time=1727895815.012282, time_in_queue=1.658027172088623, finished_time=1727895820.98754, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0617564, last_token_time=1727895821.1224966, first_scheduled_time=1727895814.8646705, first_token_time=1727895815.1602592, time_in_queue=1.8029141426086426, finished_time=1727895821.1224456, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0626464, last_token_time=1727895821.2778049, first_scheduled_time=1727895815.0129244, first_token_time=1727895815.3073406, time_in_queue=1.9502780437469482, finished_time=1727895821.2777576, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0633657, last_token_time=1727895821.431957, first_scheduled_time=1727895815.1607282, first_token_time=1727895815.4551356, time_in_queue=2.097362518310547, finished_time=1727895821.4319112, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0640767, last_token_time=1727895821.585957, first_scheduled_time=1727895815.3078387, first_token_time=1727895815.6035097, time_in_queue=2.2437620162963867, finished_time=1727895821.585909, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0647767, last_token_time=1727895821.7398486, first_scheduled_time=1727895815.4556448, first_token_time=1727895815.7525005, time_in_queue=2.3908681869506836, finished_time=1727895821.7398024, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0654886, last_token_time=1727895821.7849147, first_scheduled_time=1727895815.6040387, first_token_time=1727895815.9025054, time_in_queue=2.5385501384735107, finished_time=1727895821.7848675, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0662465, last_token_time=1727895821.8191752, first_scheduled_time=1727895815.753022, first_token_time=1727895816.0522075, time_in_queue=2.6867754459381104, finished_time=1727895821.8191257, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0669448, last_token_time=1727895821.8528533, first_scheduled_time=1727895815.9030843, first_token_time=1727895816.2024543, time_in_queue=2.836139440536499, finished_time=1727895821.852808, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0676312, last_token_time=1727895821.884654, first_scheduled_time=1727895816.0528436, first_token_time=1727895816.3531075, time_in_queue=2.9852123260498047, finished_time=1727895821.8846145, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0683215, last_token_time=1727895821.9157214, first_scheduled_time=1727895816.2030728, first_token_time=1727895816.5055246, time_in_queue=3.134751319885254, finished_time=1727895821.915685, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0690172, last_token_time=1727895821.9460647, first_scheduled_time=1727895816.3537505, first_token_time=1727895816.6581366, time_in_queue=3.284733295440674, finished_time=1727895821.946032, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0697598, last_token_time=1727895821.975702, first_scheduled_time=1727895816.506198, first_token_time=1727895816.8111637, time_in_queue=3.4364380836486816, finished_time=1727895821.9756732, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RECOMPUTE, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0704587, last_token_time=1727895823.5595546, first_scheduled_time=1727895816.6588826, first_token_time=1727895816.8569472, time_in_queue=3.58842396736145, finished_time=1727895823.5595293, rescheduled_time=1727895820.9402876, reget_first_token_time=1727895820.9871857))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.071167, last_token_time=1727895825.2496462, first_scheduled_time=1727895820.988016, first_token_time=1727895821.2774456, time_in_queue=7.91684889793396, finished_time=1727895825.2496247, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0718656, last_token_time=1727895825.2768486, first_scheduled_time=1727895821.1229444, first_token_time=1727895821.4315941, time_in_queue=8.051078796386719, finished_time=1727895825.2768316, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0725684, last_token_time=1727895825.3034651, first_scheduled_time=1727895821.2782502, first_token_time=1727895821.5855827, time_in_queue=8.205681800842285, finished_time=1727895825.3034523, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0733142, last_token_time=1727895825.329392, first_scheduled_time=1727895821.4324462, first_token_time=1727895821.7394836, time_in_queue=8.359132051467896, finished_time=1727895825.3293834, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895813.0740142, last_token_time=1727895825.3554232, first_scheduled_time=1727895821.5863655, first_token_time=1727895821.784569, time_in_queue=8.512351274490356, finished_time=1727895825.3554206, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 23040
End-to-End latency: 12.30 seconds
Throughput: 1.63 requests/s, 1873.52 tokens/s
Per_token_time: 0.534 ms

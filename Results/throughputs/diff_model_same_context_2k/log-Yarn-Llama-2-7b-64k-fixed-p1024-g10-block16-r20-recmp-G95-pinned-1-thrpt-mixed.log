Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='fixed', input_len=1024, output_len=10, min_len=10, max_len=50, prefill_to_decode_ratio=6.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=2048, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=2048, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=True)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
Selected required requests 20
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
Mixed batch is enabled. max_num_batched_tokens=2048, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'type', 'finetuned', 'original_max_position_embeddings'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-02 18:05:23 config.py:654] [SchedulerConfig] max_num_batched_tokens: 2048 chunked_prefill_enabled: False
INFO 10-02 18:05:23 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=True)
/home/jieye/viper2/vllm_v0_4_2_mine/vllm/executor/gpu_executor.py:36: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 10-02 18:05:24 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-02 18:05:28 selector.py:27] Using FlashAttention-2 backend.
INFO 10-02 18:05:32 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-02 18:05:32 model_runner.py:183] Loaded model: 
INFO 10-02 18:05:32 model_runner.py:183]  LlamaForCausalLM(
INFO 10-02 18:05:32 model_runner.py:183]   (model): LlamaModel(
INFO 10-02 18:05:32 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-02 18:05:32 model_runner.py:183]     (layers): ModuleList(
INFO 10-02 18:05:32 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-02 18:05:32 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-02 18:05:32 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:05:32 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:05:32 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-02 18:05:32 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-02 18:05:32 model_runner.py:183]         )
INFO 10-02 18:05:32 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-02 18:05:32 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:05:32 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:05:32 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-02 18:05:32 model_runner.py:183]         )
INFO 10-02 18:05:32 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:05:32 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:05:32 model_runner.py:183]       )
INFO 10-02 18:05:32 model_runner.py:183]     )
INFO 10-02 18:05:32 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:05:32 model_runner.py:183]   )
INFO 10-02 18:05:32 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-02 18:05:32 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-02 18:05:32 model_runner.py:183]   (sampler): Sampler()
INFO 10-02 18:05:32 model_runner.py:183] )
INFO 10-02 18:05:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:05:32 worker.py:164] Peak: 13.332 GB, Initial: 38.980 GB, Free: 25.648 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.092 GB
INFO 10-02 18:05:33 gpu_executor.py:117] # GPU blocks: 3083, # CPU blocks: 8192
INFO 10-02 18:06:03 worker.py:189] _init_cache_engine took 24.0859 GB
INFO 10-02 18:06:03 scheduler.py:307] Scheduler initialized with prompt limit: 2048
INFO 10-02 18:06:03 Start warmup...
INFO 10-02 18:06:03 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:03 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:06:03 metrics.py:335] Avg prompt throughput: 709.8 tokens/s, Avg generation throughput: 35.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 18:06:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.9216 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:06:03 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:03 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:06:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 50.1
INFO 10-02 18:06:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.6237 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:06:08 Start benchmarking...
INFO 10-02 18:06:08 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:08 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:06:08 metrics.py:335] Avg prompt throughput: 11315.1 tokens/s, Avg generation throughput: 11.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%, Interval(ms): 181.0
INFO 10-02 18:06:08 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 156.8565 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:06:08 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:08 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1026]), positions.shape=torch.Size([1026]) hidden_states.shape=torch.Size([1026, 4096]) residual=None
INFO 10-02 18:06:08 metrics.py:335] Avg prompt throughput: 11482.7 tokens/s, Avg generation throughput: 33.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 17 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%, Interval(ms): 89.2
INFO 10-02 18:06:08 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 88.8786 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:06:08 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:08 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1027]), positions.shape=torch.Size([1027]) hidden_states.shape=torch.Size([1027, 4096]) residual=None
INFO 10-02 18:06:09 metrics.py:335] Avg prompt throughput: 13668.8 tokens/s, Avg generation throughput: 53.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 8.4%, CPU KV cache usage: 0.0%, Interval(ms): 74.9
INFO 10-02 18:06:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 74.7354 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:06:09 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:09 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1028]), positions.shape=torch.Size([1028]) hidden_states.shape=torch.Size([1028, 4096]) residual=None
INFO 10-02 18:06:09 metrics.py:335] Avg prompt throughput: 13681.2 tokens/s, Avg generation throughput: 66.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%, Interval(ms): 74.8
INFO 10-02 18:06:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 74.6360 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:06:09 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:09 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1029]), positions.shape=torch.Size([1029]) hidden_states.shape=torch.Size([1029, 4096]) residual=None
INFO 10-02 18:06:09 metrics.py:335] Avg prompt throughput: 13675.0 tokens/s, Avg generation throughput: 80.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 74.9
INFO 10-02 18:06:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 74.7025 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:06:09 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:09 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1030]), positions.shape=torch.Size([1030]) hidden_states.shape=torch.Size([1030, 4096]) residual=None
INFO 10-02 18:06:09 metrics.py:335] Avg prompt throughput: 13565.2 tokens/s, Avg generation throughput: 92.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 14.7%, CPU KV cache usage: 0.0%, Interval(ms): 75.5
INFO 10-02 18:06:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 75.3067 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:06:09 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:09 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1031]), positions.shape=torch.Size([1031]) hidden_states.shape=torch.Size([1031, 4096]) residual=None
INFO 10-02 18:06:09 metrics.py:335] Avg prompt throughput: 13505.1 tokens/s, Avg generation throughput: 105.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 16.8%, CPU KV cache usage: 0.0%, Interval(ms): 75.8
INFO 10-02 18:06:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 75.6390 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:06:09 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:09 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1032]), positions.shape=torch.Size([1032]) hidden_states.shape=torch.Size([1032, 4096]) residual=None
INFO 10-02 18:06:09 metrics.py:335] Avg prompt throughput: 13451.6 tokens/s, Avg generation throughput: 118.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 76.1
INFO 10-02 18:06:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 75.9401 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:06:09 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:09 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1033]), positions.shape=torch.Size([1033]) hidden_states.shape=torch.Size([1033, 4096]) residual=None
INFO 10-02 18:06:09 metrics.py:335] Avg prompt throughput: 13376.8 tokens/s, Avg generation throughput: 130.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 76.6
INFO 10-02 18:06:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.3681 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:06:09 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:09 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1034]), positions.shape=torch.Size([1034]) hidden_states.shape=torch.Size([1034, 4096]) residual=None
INFO 10-02 18:06:09 metrics.py:335] Avg prompt throughput: 13296.8 tokens/s, Avg generation throughput: 142.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 77.0
INFO 10-02 18:06:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.7868 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:06:09 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:09 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1033]), positions.shape=torch.Size([1033]) hidden_states.shape=torch.Size([1033, 4096]) residual=None
INFO 10-02 18:06:09 metrics.py:335] Avg prompt throughput: 13352.2 tokens/s, Avg generation throughput: 130.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 76.7
INFO 10-02 18:06:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.4439 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:06:09 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:09 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1033]), positions.shape=torch.Size([1033]) hidden_states.shape=torch.Size([1033, 4096]) residual=None
INFO 10-02 18:06:09 metrics.py:335] Avg prompt throughput: 13365.6 tokens/s, Avg generation throughput: 130.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 76.6
INFO 10-02 18:06:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.4003 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:06:09 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:09 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1033]), positions.shape=torch.Size([1033]) hidden_states.shape=torch.Size([1033, 4096]) residual=None
INFO 10-02 18:06:09 metrics.py:335] Avg prompt throughput: 13362.6 tokens/s, Avg generation throughput: 130.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 76.6
INFO 10-02 18:06:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.4182 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:06:09 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:09 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1033]), positions.shape=torch.Size([1033]) hidden_states.shape=torch.Size([1033, 4096]) residual=None
INFO 10-02 18:06:09 metrics.py:335] Avg prompt throughput: 13343.5 tokens/s, Avg generation throughput: 130.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 76.7
INFO 10-02 18:06:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.5285 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:06:09 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:09 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1033]), positions.shape=torch.Size([1033]) hidden_states.shape=torch.Size([1033, 4096]) residual=None
INFO 10-02 18:06:09 metrics.py:335] Avg prompt throughput: 13345.0 tokens/s, Avg generation throughput: 130.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 76.7
INFO 10-02 18:06:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.5235 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:06:09 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:09 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1033]), positions.shape=torch.Size([1033]) hidden_states.shape=torch.Size([1033, 4096]) residual=None
INFO 10-02 18:06:10 metrics.py:335] Avg prompt throughput: 13326.1 tokens/s, Avg generation throughput: 130.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 76.8
INFO 10-02 18:06:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.6301 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:06:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:10 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1033]), positions.shape=torch.Size([1033]) hidden_states.shape=torch.Size([1033, 4096]) residual=None
INFO 10-02 18:06:10 metrics.py:335] Avg prompt throughput: 13338.0 tokens/s, Avg generation throughput: 130.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 76.8
INFO 10-02 18:06:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.5617 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:06:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:10 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1033]), positions.shape=torch.Size([1033]) hidden_states.shape=torch.Size([1033, 4096]) residual=None
INFO 10-02 18:06:10 metrics.py:335] Avg prompt throughput: 13338.7 tokens/s, Avg generation throughput: 130.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 76.8
INFO 10-02 18:06:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.5591 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:06:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:10 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1033]), positions.shape=torch.Size([1033]) hidden_states.shape=torch.Size([1033, 4096]) residual=None
INFO 10-02 18:06:10 metrics.py:335] Avg prompt throughput: 13312.2 tokens/s, Avg generation throughput: 130.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 76.9
INFO 10-02 18:06:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.7095 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:06:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:10 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-02 18:06:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 525.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.9%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-02 18:06:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9022 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:06:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:10 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-02 18:06:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 480.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-02 18:06:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.4611 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:06:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:10 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-02 18:06:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 429.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-02 18:06:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1130 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:06:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:10 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-02 18:06:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 379.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-02 18:06:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6355 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:06:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:10 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-02 18:06:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 322.9 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.4%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-02 18:06:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.2960 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:06:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:10 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-02 18:06:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 265.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.1
INFO 10-02 18:06:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.9071 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:06:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:10 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-02 18:06:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-02 18:06:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5135 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:06:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:10 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-02 18:06:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 137.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 10-02 18:06:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3309 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:06:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:06:10 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:06:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-02 18:06:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4782 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.7189298, last_token_time=1727892369.5846324, first_scheduled_time=1727892368.7332006, first_token_time=1727892368.8896415, time_in_queue=0.014270782470703125, finished_time=1727892369.5845938, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.7217124, last_token_time=1727892369.5846324, first_scheduled_time=1727892368.7332006, first_token_time=1727892368.8896415, time_in_queue=0.011488199234008789, finished_time=1727892369.584598, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.722355, last_token_time=1727892369.6613238, first_scheduled_time=1727892368.8902278, first_token_time=1727892368.9788444, time_in_queue=0.16787290573120117, finished_time=1727892369.6612887, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.722956, last_token_time=1727892369.7379386, first_scheduled_time=1727892368.9792888, first_token_time=1727892369.0537355, time_in_queue=0.2563328742980957, finished_time=1727892369.7379045, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.7235243, last_token_time=1727892369.8145704, first_scheduled_time=1727892369.0542464, first_token_time=1727892369.12858, time_in_queue=0.3307220935821533, finished_time=1727892369.8145354, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.72418, last_token_time=1727892369.8913116, first_scheduled_time=1727892369.1290498, first_token_time=1727892369.2034483, time_in_queue=0.40486979484558105, finished_time=1727892369.8912768, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.7247813, last_token_time=1727892369.9680443, first_scheduled_time=1727892369.2039504, first_token_time=1727892369.2789211, time_in_queue=0.4791691303253174, finished_time=1727892369.9680095, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.7253468, last_token_time=1727892370.0448859, first_scheduled_time=1727892369.2794218, first_token_time=1727892369.3547227, time_in_queue=0.5540750026702881, finished_time=1727892370.0448499, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.7259026, last_token_time=1727892370.1216593, first_scheduled_time=1727892369.3552468, first_token_time=1727892369.4308286, time_in_queue=0.6293442249298096, finished_time=1727892370.121624, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.7264693, last_token_time=1727892370.1984282, first_scheduled_time=1727892369.4313726, first_token_time=1727892369.5073557, time_in_queue=0.7049033641815186, finished_time=1727892370.1983922, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.7270725, last_token_time=1727892370.27535, first_scheduled_time=1727892369.5079763, first_token_time=1727892369.5843108, time_in_queue=0.7809038162231445, finished_time=1727892370.2753134, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.7276511, last_token_time=1727892370.2924654, first_scheduled_time=1727892369.585001, first_token_time=1727892369.6610458, time_in_queue=0.8573498725891113, finished_time=1727892370.2924292, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.7282207, last_token_time=1727892370.3091323, first_scheduled_time=1727892369.6616602, first_token_time=1727892369.737659, time_in_queue=0.9334394931793213, finished_time=1727892370.3090994, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.7287877, last_token_time=1727892370.3254454, first_scheduled_time=1727892369.7382736, first_token_time=1727892369.8142793, time_in_queue=1.0094859600067139, finished_time=1727892370.3254156, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.7294123, last_token_time=1727892370.3412747, first_scheduled_time=1727892369.8149047, first_token_time=1727892369.8910306, time_in_queue=1.0854923725128174, finished_time=1727892370.3412497, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.729993, last_token_time=1727892370.3567607, first_scheduled_time=1727892369.8916407, first_token_time=1727892369.9677675, time_in_queue=1.1616475582122803, finished_time=1727892370.3567402, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.7305717, last_token_time=1727892370.3718543, first_scheduled_time=1727892369.968414, first_token_time=1727892370.0446055, time_in_queue=1.237842321395874, finished_time=1727892370.3718362, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.7311723, last_token_time=1727892370.3865545, first_scheduled_time=1727892370.0452187, first_token_time=1727892370.1213775, time_in_queue=1.3140463829040527, finished_time=1727892370.386542, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.731762, last_token_time=1727892370.401065, first_scheduled_time=1727892370.1219897, first_token_time=1727892370.198139, time_in_queue=1.3902277946472168, finished_time=1727892370.401057, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727892368.7323768, last_token_time=1727892370.4157221, first_scheduled_time=1727892370.1987536, first_token_time=1727892370.2750697, time_in_queue=1.466376781463623, finished_time=1727892370.4157197, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 20680
End-to-End latency: 1.70 seconds
Throughput: 11.79 requests/s, 12186.22 tokens/s
Per_token_time: 0.082 ms

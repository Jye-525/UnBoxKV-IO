Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='fixed', input_len=1024, output_len=128, min_len=10, max_len=50, prefill_to_decode_ratio=6.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=2048, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=1024, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Selected required requests 20
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Chunked prefill is enabled. max_num_batched_tokens=1024, max_num_batched_seqs=256
rope_scaling = {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}
INFO 10-02 18:52:44 config.py:654] [SchedulerConfig] max_num_batched_tokens: 1024 chunked_prefill_enabled: True
INFO 10-02 18:52:44 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
/home/jieye/viper2/vllm_v0_4_2_mine/vllm/executor/gpu_executor.py:36: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 10-02 18:52:45 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-02 18:52:49 selector.py:27] Using FlashAttention-2 backend.
INFO 10-02 18:52:58 model_runner.py:180] Loading model weights took 14.9888 GB
INFO 10-02 18:52:58 model_runner.py:183] Loaded model: 
INFO 10-02 18:52:58 model_runner.py:183]  LlamaForCausalLM(
INFO 10-02 18:52:58 model_runner.py:183]   (model): LlamaModel(
INFO 10-02 18:52:58 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=128256, embedding_dim=4096, org_vocab_size=128256, num_embeddings_padded=128256, tp_size=1)
INFO 10-02 18:52:58 model_runner.py:183]     (layers): ModuleList(
INFO 10-02 18:52:58 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-02 18:52:58 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-02 18:52:58 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:52:58 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:52:58 model_runner.py:183]           (rotary_emb): Llama3RotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=131072, base=500000.0, is_neox_style=True)
INFO 10-02 18:52:58 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845)
INFO 10-02 18:52:58 model_runner.py:183]         )
INFO 10-02 18:52:58 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-02 18:52:58 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=28672, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:52:58 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=14336, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:52:58 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-02 18:52:58 model_runner.py:183]         )
INFO 10-02 18:52:58 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:52:58 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:52:58 model_runner.py:183]       )
INFO 10-02 18:52:58 model_runner.py:183]     )
INFO 10-02 18:52:58 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:52:58 model_runner.py:183]   )
INFO 10-02 18:52:58 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=128256, embedding_dim=4096, org_vocab_size=128256, num_embeddings_padded=128256, tp_size=1)
INFO 10-02 18:52:58 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=128256, forg_vocab_size=128256, scale=1.0, logits_as_input=False)
INFO 10-02 18:52:58 model_runner.py:183]   (sampler): Sampler()
INFO 10-02 18:52:58 model_runner.py:183] )
INFO 10-02 18:52:58 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:52:58 worker.py:164] Peak: 16.473 GB, Initial: 38.980 GB, Free: 22.507 GB, Total: 39.394 GB,               cache_block_size: 2097152 Bytes, available GPU for KV cache: 20.952 GB
INFO 10-02 18:52:58 gpu_executor.py:117] # GPU blocks: 10727, # CPU blocks: 32768
INFO 10-02 18:53:30 worker.py:189] _init_cache_engine took 20.9512 GB
INFO 10-02 18:53:30 scheduler.py:307] Scheduler initialized with prompt limit: 2048
INFO 10-02 18:53:30 Start warmup...
INFO 10-02 18:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:31 metrics.py:335] Avg prompt throughput: 10.6 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 1887.3
INFO 10-02 18:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1881.1216 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:31 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 57.0
INFO 10-02 18:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 56.5679 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:53:36 Start benchmarking...
INFO 10-02 18:53:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:38 metrics.py:335] Avg prompt throughput: 605.4 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 19 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%, Interval(ms): 1691.5
INFO 10-02 18:53:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1663.5432 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:53:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:38 metrics.py:335] Avg prompt throughput: 11049.1 tokens/s, Avg generation throughput: 10.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%, Interval(ms): 92.6
INFO 10-02 18:53:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 92.3226 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:53:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:38 metrics.py:335] Avg prompt throughput: 9425.5 tokens/s, Avg generation throughput: 18.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 17 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%, Interval(ms): 108.5
INFO 10-02 18:53:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 108.3395 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:53:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:38 metrics.py:335] Avg prompt throughput: 11003.3 tokens/s, Avg generation throughput: 32.3 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%, Interval(ms): 92.9
INFO 10-02 18:53:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 92.7050 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:53:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:39 metrics.py:335] Avg prompt throughput: 11835.8 tokens/s, Avg generation throughput: 46.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%, Interval(ms): 86.3
INFO 10-02 18:53:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.0741 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:53:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:39 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:39 metrics.py:335] Avg prompt throughput: 11817.3 tokens/s, Avg generation throughput: 57.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%, Interval(ms): 86.3
INFO 10-02 18:53:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.1328 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:53:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:39 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:39 metrics.py:335] Avg prompt throughput: 11830.0 tokens/s, Avg generation throughput: 69.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%, Interval(ms): 86.1
INFO 10-02 18:53:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 85.9549 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:53:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:39 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:39 metrics.py:335] Avg prompt throughput: 11813.4 tokens/s, Avg generation throughput: 81.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%, Interval(ms): 86.2
INFO 10-02 18:53:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 85.9981 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:53:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:39 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:39 metrics.py:335] Avg prompt throughput: 11794.6 tokens/s, Avg generation throughput: 92.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%, Interval(ms): 86.2
INFO 10-02 18:53:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.0503 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:53:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:39 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:39 metrics.py:335] Avg prompt throughput: 11747.1 tokens/s, Avg generation throughput: 104.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%, Interval(ms): 86.5
INFO 10-02 18:53:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.3078 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:53:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:39 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:39 metrics.py:335] Avg prompt throughput: 11737.8 tokens/s, Avg generation throughput: 115.6 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%, Interval(ms): 86.5
INFO 10-02 18:53:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.2875 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:53:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:39 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:39 metrics.py:335] Avg prompt throughput: 11721.5 tokens/s, Avg generation throughput: 127.2 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 86.5
INFO 10-02 18:53:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.3152 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:53:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:39 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:39 metrics.py:335] Avg prompt throughput: 11665.4 tokens/s, Avg generation throughput: 138.2 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 7.9%, CPU KV cache usage: 0.0%, Interval(ms): 86.8
INFO 10-02 18:53:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.6442 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:53:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:39 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:39 metrics.py:335] Avg prompt throughput: 11671.7 tokens/s, Avg generation throughput: 149.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 8.5%, CPU KV cache usage: 0.0%, Interval(ms): 86.7
INFO 10-02 18:53:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.5006 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:53:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:39 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:39 metrics.py:335] Avg prompt throughput: 11645.7 tokens/s, Avg generation throughput: 161.3 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 9.1%, CPU KV cache usage: 0.0%, Interval(ms): 86.8
INFO 10-02 18:53:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.6132 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:53:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:39 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 11486.1 tokens/s, Avg generation throughput: 170.6 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 9.7%, CPU KV cache usage: 0.0%, Interval(ms): 87.9
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 87.7280 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 11474.1 tokens/s, Avg generation throughput: 181.9 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 10.3%, CPU KV cache usage: 0.0%, Interval(ms): 87.9
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 87.7304 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 11684.0 tokens/s, Avg generation throughput: 197.1 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 10.9%, CPU KV cache usage: 0.0%, Interval(ms): 86.3
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.0565 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 11719.8 tokens/s, Avg generation throughput: 209.5 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 11.5%, CPU KV cache usage: 0.0%, Interval(ms): 85.9
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 85.7143 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 11726.1 tokens/s, Avg generation throughput: 221.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.1%, CPU KV cache usage: 0.0%, Interval(ms): 85.8
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 85.5751 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([191]), positions.shape=torch.Size([191]) hidden_states.shape=torch.Size([191, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 5212.5 tokens/s, Avg generation throughput: 606.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.1%, CPU KV cache usage: 0.0%, Interval(ms): 33.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 32.7735 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7939 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7675 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7379 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7496 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7336 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7314 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8182 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7636 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7617 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7515 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7682 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7684 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8075 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7963 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7999 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8073 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8094 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8247 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8063 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8328 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 904.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.1
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.8945 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8204 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8325 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7980 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7596 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8156 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8149 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9005 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7975 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8189 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8223 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8490 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8089 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7725 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8559 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8106 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8035 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8228 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7944 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8352 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8211 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8805 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8218 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8392 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9079 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9241 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9050 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8740 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9389 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9427 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9622 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9134 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9491 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9684 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9470 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9966 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9758 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9081 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9076 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9117 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9296 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9985 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9596 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9649 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9649 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9918 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0080 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0750 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0075 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0018 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0209 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9804 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9866 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9742 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9999 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0068 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0307 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0419 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 937.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1203 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 935.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0991 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 938.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0438 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0354 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0354 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0133 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0218 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0629 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 938.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0595 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0199 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 938.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1003 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 937.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1103 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0207 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0688 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9951 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9823 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0390 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0457 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0302 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0526 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0402 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0304 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 937.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1146 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9541 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9737 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9875 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0605 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0268 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.4 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0714 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 904.1 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7207 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 908.0 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7086 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([18]), positions.shape=torch.Size([18]) hidden_states.shape=torch.Size([18, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 865.2 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5338 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 822.3 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.7
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.4101 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 822.6 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.1%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.1905 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 776.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.0535 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 729.6 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.7%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9323 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 726.0 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6504 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 675.1 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.5304 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 620.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.7
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.4804 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 579.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0374 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 521.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0214 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 466.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9146 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 414.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6891 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 357.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5474 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 301.2 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.6
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.3865 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 242.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.5
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.2451 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 185.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9829 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-02 18:53:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-02 18:53:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9137 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:53:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:53:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-02 18:53:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6658 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895216.9887142, last_token_time=1727895222.6453555, first_scheduled_time=1727895217.0068285, first_token_time=1727895218.6699743, time_in_queue=0.018114328384399414, finished_time=1727895222.6452718, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895216.9917967, last_token_time=1727895222.6872952, first_scheduled_time=1727895218.6705317, first_token_time=1727895218.8711061, time_in_queue=1.6787350177764893, finished_time=1727895222.6872184, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895216.9925668, last_token_time=1727895222.7080996, first_scheduled_time=1727895218.7630377, first_token_time=1727895218.9639647, time_in_queue=1.7704708576202393, finished_time=1727895222.7080264, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895216.993442, last_token_time=1727895222.7287738, first_scheduled_time=1727895218.8715897, first_token_time=1727895219.0502107, time_in_queue=1.8781476020812988, finished_time=1727895222.728705, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895216.994161, last_token_time=1727895222.7482255, first_scheduled_time=1727895218.9644468, first_token_time=1727895219.1365268, time_in_queue=1.9702858924865723, finished_time=1727895222.7481592, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895216.9948635, last_token_time=1727895222.7675369, first_scheduled_time=1727895219.050715, first_token_time=1727895219.2226562, time_in_queue=2.055851459503174, finished_time=1727895222.767472, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895216.9955676, last_token_time=1727895222.7867248, first_scheduled_time=1727895219.1370118, first_token_time=1727895219.3088057, time_in_queue=2.141444206237793, finished_time=1727895222.786665, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895216.996273, last_token_time=1727895222.8046315, first_scheduled_time=1727895219.2231443, first_token_time=1727895219.3950114, time_in_queue=2.2268712520599365, finished_time=1727895222.8045774, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895216.997733, last_token_time=1727895222.8224053, first_scheduled_time=1727895219.3093235, first_token_time=1727895219.4814749, time_in_queue=2.3115904331207275, finished_time=1727895222.8223534, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895216.9985383, last_token_time=1727895222.8401303, first_scheduled_time=1727895219.3956091, first_token_time=1727895219.5679278, time_in_queue=2.39707088470459, finished_time=1727895222.8400855, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895216.9992409, last_token_time=1727895222.8574004, first_scheduled_time=1727895219.4820561, first_token_time=1727895219.654418, time_in_queue=2.4828152656555176, finished_time=1727895222.8573577, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895216.9999583, last_token_time=1727895222.8746474, first_scheduled_time=1727895219.56854, first_token_time=1727895219.741242, time_in_queue=2.5685818195343018, finished_time=1727895222.8746068, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895217.0006483, last_token_time=1727895222.8917966, first_scheduled_time=1727895219.6550548, first_token_time=1727895219.8279247, time_in_queue=2.6544065475463867, finished_time=1727895222.8917642, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895217.0013356, last_token_time=1727895222.908705, first_scheduled_time=1727895219.7419062, first_token_time=1727895219.9147208, time_in_queue=2.740570545196533, finished_time=1727895222.9086745, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895217.0020926, last_token_time=1727895222.9254677, first_scheduled_time=1727895219.8286164, first_token_time=1727895220.002639, time_in_queue=2.826523780822754, finished_time=1727895222.9254417, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895217.0028338, last_token_time=1727895222.9420679, first_scheduled_time=1727895219.915478, first_token_time=1727895220.090549, time_in_queue=2.912644147872925, finished_time=1727895222.9420464, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895217.0035937, last_token_time=1727895222.958557, first_scheduled_time=1727895220.003378, first_token_time=1727895220.1767998, time_in_queue=2.999784231185913, finished_time=1727895222.9585397, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895217.004337, last_token_time=1727895222.9747407, first_scheduled_time=1727895220.0913281, first_token_time=1727895220.2627122, time_in_queue=3.08699107170105, finished_time=1727895222.9747272, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895217.0050967, last_token_time=1727895222.9908528, first_scheduled_time=1727895220.177601, first_token_time=1727895220.3484826, time_in_queue=3.172504425048828, finished_time=1727895222.9908447, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895217.0058184, last_token_time=1727895223.0067108, first_scheduled_time=1727895220.263525, first_token_time=1727895220.3814554, time_in_queue=3.257706642150879, finished_time=1727895223.0067081, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 23040
End-to-End latency: 6.02 seconds
Throughput: 3.32 requests/s, 3828.38 tokens/s
Per_token_time: 0.261 ms

Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='fixed', input_len=1024, output_len=128, min_len=10, max_len=50, prefill_to_decode_ratio=6.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=2048, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=512, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Selected required requests 20
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Chunked prefill is enabled. max_num_batched_tokens=512, max_num_batched_seqs=256
rope_scaling = {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}
INFO 10-02 18:54:22 config.py:654] [SchedulerConfig] max_num_batched_tokens: 512 chunked_prefill_enabled: True
INFO 10-02 18:54:22 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
/home/jieye/viper2/vllm_v0_4_2_mine/vllm/executor/gpu_executor.py:36: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 10-02 18:54:22 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-02 18:54:27 selector.py:27] Using FlashAttention-2 backend.
INFO 10-02 18:54:35 model_runner.py:180] Loading model weights took 14.9888 GB
INFO 10-02 18:54:35 model_runner.py:183] Loaded model: 
INFO 10-02 18:54:35 model_runner.py:183]  LlamaForCausalLM(
INFO 10-02 18:54:35 model_runner.py:183]   (model): LlamaModel(
INFO 10-02 18:54:35 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=128256, embedding_dim=4096, org_vocab_size=128256, num_embeddings_padded=128256, tp_size=1)
INFO 10-02 18:54:35 model_runner.py:183]     (layers): ModuleList(
INFO 10-02 18:54:35 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-02 18:54:35 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-02 18:54:35 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:54:35 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:54:35 model_runner.py:183]           (rotary_emb): Llama3RotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=131072, base=500000.0, is_neox_style=True)
INFO 10-02 18:54:35 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845)
INFO 10-02 18:54:35 model_runner.py:183]         )
INFO 10-02 18:54:35 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-02 18:54:35 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=28672, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:54:35 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=14336, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:54:35 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-02 18:54:35 model_runner.py:183]         )
INFO 10-02 18:54:35 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:54:35 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:54:35 model_runner.py:183]       )
INFO 10-02 18:54:35 model_runner.py:183]     )
INFO 10-02 18:54:35 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:54:35 model_runner.py:183]   )
INFO 10-02 18:54:35 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=128256, embedding_dim=4096, org_vocab_size=128256, num_embeddings_padded=128256, tp_size=1)
INFO 10-02 18:54:35 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=128256, forg_vocab_size=128256, scale=1.0, logits_as_input=False)
INFO 10-02 18:54:35 model_runner.py:183]   (sampler): Sampler()
INFO 10-02 18:54:35 model_runner.py:183] )
INFO 10-02 18:54:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:54:35 worker.py:164] Peak: 16.400 GB, Initial: 38.980 GB, Free: 22.579 GB, Total: 39.394 GB,               cache_block_size: 2097152 Bytes, available GPU for KV cache: 21.024 GB
INFO 10-02 18:54:35 gpu_executor.py:117] # GPU blocks: 10764, # CPU blocks: 32768
INFO 10-02 18:55:07 worker.py:189] _init_cache_engine took 21.0234 GB
INFO 10-02 18:55:07 scheduler.py:307] Scheduler initialized with prompt limit: 2048
INFO 10-02 18:55:07 Start warmup...
INFO 10-02 18:55:07 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:07 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:09 metrics.py:335] Avg prompt throughput: 10.6 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 1894.2
INFO 10-02 18:55:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1887.9271 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:55:09 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:09 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:55:09 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 51.7
INFO 10-02 18:55:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 51.2760 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:55:14 Start benchmarking...
INFO 10-02 18:55:14 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:14 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:15 metrics.py:335] Avg prompt throughput: 317.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 19 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%, Interval(ms): 1615.1
INFO 10-02 18:55:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1587.9025 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:55:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:15 metrics.py:335] Avg prompt throughput: 6048.4 tokens/s, Avg generation throughput: 11.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 19 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%, Interval(ms): 84.7
INFO 10-02 18:55:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 84.3821 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:55:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:15 metrics.py:335] Avg prompt throughput: 10063.8 tokens/s, Avg generation throughput: 19.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%, Interval(ms): 50.8
INFO 10-02 18:55:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.5836 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:55:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:15 metrics.py:335] Avg prompt throughput: 8981.5 tokens/s, Avg generation throughput: 17.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%, Interval(ms): 56.9
INFO 10-02 18:55:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 56.7236 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:55:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:15 metrics.py:335] Avg prompt throughput: 8351.6 tokens/s, Avg generation throughput: 32.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 17 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%, Interval(ms): 61.2
INFO 10-02 18:55:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 61.0211 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:55:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 8945.3 tokens/s, Avg generation throughput: 35.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 17 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%, Interval(ms): 57.0
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 56.8423 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 10341.9 tokens/s, Avg generation throughput: 60.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%, Interval(ms): 49.3
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.1471 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 11212.2 tokens/s, Avg generation throughput: 66.1 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%, Interval(ms): 45.4
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.2344 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 10419.2 tokens/s, Avg generation throughput: 81.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%, Interval(ms): 48.9
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.6906 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 11137.0 tokens/s, Avg generation throughput: 87.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%, Interval(ms): 45.6
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.4473 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 10374.2 tokens/s, Avg generation throughput: 102.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%, Interval(ms): 49.0
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.8026 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 11069.2 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%, Interval(ms): 45.8
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.6312 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 10359.0 tokens/s, Avg generation throughput: 122.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%, Interval(ms): 48.9
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.7752 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 11079.6 tokens/s, Avg generation throughput: 131.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%, Interval(ms): 45.7
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.4946 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 10314.5 tokens/s, Avg generation throughput: 142.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%, Interval(ms): 49.1
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.8868 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 11018.4 tokens/s, Avg generation throughput: 152.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%, Interval(ms): 45.8
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.6529 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 10287.1 tokens/s, Avg generation throughput: 163.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%, Interval(ms): 49.1
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.9118 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 10987.4 tokens/s, Avg generation throughput: 174.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%, Interval(ms): 45.9
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.6877 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 10248.8 tokens/s, Avg generation throughput: 183.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%, Interval(ms): 49.2
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.9991 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 10946.5 tokens/s, Avg generation throughput: 195.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%, Interval(ms): 46.0
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.7654 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 10211.8 tokens/s, Avg generation throughput: 203.0 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%, Interval(ms): 49.3
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.0699 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 10908.9 tokens/s, Avg generation throughput: 217.3 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%, Interval(ms): 46.0
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.8286 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 10155.2 tokens/s, Avg generation throughput: 222.5 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 7.2%, CPU KV cache usage: 0.0%, Interval(ms): 49.4
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.2468 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 10798.3 tokens/s, Avg generation throughput: 237.1 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 46.4
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.2015 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 10687.5 tokens/s, Avg generation throughput: 256.0 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 7.9%, CPU KV cache usage: 0.0%, Interval(ms): 46.9
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.6857 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:16 metrics.py:335] Avg prompt throughput: 10976.2 tokens/s, Avg generation throughput: 263.4 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 7.9%, CPU KV cache usage: 0.0%, Interval(ms): 45.6
INFO 10-02 18:55:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.3553 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:55:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 10728.8 tokens/s, Avg generation throughput: 278.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 8.5%, CPU KV cache usage: 0.0%, Interval(ms): 46.6
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.4079 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 10948.5 tokens/s, Avg generation throughput: 285.2 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 8.5%, CPU KV cache usage: 0.0%, Interval(ms): 45.6
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.3744 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 10732.5 tokens/s, Avg generation throughput: 301.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 9.1%, CPU KV cache usage: 0.0%, Interval(ms): 46.5
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.2899 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 10654.3 tokens/s, Avg generation throughput: 299.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 9.1%, CPU KV cache usage: 0.0%, Interval(ms): 46.7
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.5374 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 10423.1 tokens/s, Avg generation throughput: 313.9 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 9.7%, CPU KV cache usage: 0.0%, Interval(ms): 47.8
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.5776 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 10658.4 tokens/s, Avg generation throughput: 321.7 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 9.7%, CPU KV cache usage: 0.0%, Interval(ms): 46.6
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.4163 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 10413.5 tokens/s, Avg generation throughput: 335.2 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 10.3%, CPU KV cache usage: 0.0%, Interval(ms): 47.7
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.5214 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 10616.5 tokens/s, Avg generation throughput: 342.5 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 10.3%, CPU KV cache usage: 0.0%, Interval(ms): 46.7
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.5109 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 10489.9 tokens/s, Avg generation throughput: 359.5 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 10.9%, CPU KV cache usage: 0.0%, Interval(ms): 47.3
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.0378 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 10717.0 tokens/s, Avg generation throughput: 368.1 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 11.0%, CPU KV cache usage: 0.0%, Interval(ms): 46.2
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.9743 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 10443.8 tokens/s, Avg generation throughput: 379.8 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 11.5%, CPU KV cache usage: 0.0%, Interval(ms): 47.4
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.1811 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 10673.7 tokens/s, Avg generation throughput: 388.9 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 11.6%, CPU KV cache usage: 0.0%, Interval(ms): 46.3
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.0632 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 10356.4 tokens/s, Avg generation throughput: 398.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 47.7
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.4792 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 10629.5 tokens/s, Avg generation throughput: 409.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 46.4
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.1574 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([381]), positions.shape=torch.Size([381]) hidden_states.shape=torch.Size([381, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 9111.5 tokens/s, Avg generation throughput: 503.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 39.7
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 39.4754 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8342 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8330 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 909.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.0
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.7755 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8771 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8285 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8364 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8056 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8356 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8392 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8621 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8247 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8263 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8097 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8228 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8464 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8399 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8857 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8430 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8416 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8712 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8538 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8392 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8442 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8693 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8561 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8950 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9188 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9198 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9217 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9565 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9582 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8998 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9012 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9172 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9124 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9768 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9763 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0199 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9737 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9770 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9515 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0059 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9734 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9873 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9777 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0078 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0562 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0192 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0338 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0373 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9744 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0001 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9813 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9715 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9999 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0273 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0354 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 938.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0874 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0493 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0326 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0359 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0180 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:18 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0521 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0228 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0257 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0485 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0261 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0140 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 938.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0981 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 938.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0876 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 938.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0824 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 937.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1153 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 937.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0900 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0609 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0454 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0359 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0207 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0357 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 935.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1072 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 937.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1177 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0755 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 938.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0817 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 938.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0946 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0655 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0574 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0381 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0378 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 937.2 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1167 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 901.9 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7748 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 905.1 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7727 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 903.4 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8132 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([18]), positions.shape=torch.Size([18]) hidden_states.shape=torch.Size([18, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 864.1 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5572 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([18]), positions.shape=torch.Size([18]) hidden_states.shape=torch.Size([18, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 868.5 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.7
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5145 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 823.3 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3831 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 825.4 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.6%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3869 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 826.8 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.6%, CPU KV cache usage: 0.0%, Interval(ms): 19.4
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.0935 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 827.7 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.1219 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 778.7 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9643 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 781.2 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.3%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.0005 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 733.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.3%, CPU KV cache usage: 0.0%, Interval(ms): 19.1
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.8434 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 735.3 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.6%, CPU KV cache usage: 0.0%, Interval(ms): 19.0
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.8429 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 729.1 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.6%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.5812 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 733.9 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.7
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.5133 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 676.6 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.7
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.4532 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 680.1 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.6
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.4541 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 626.2 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.6
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3287 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 627.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3390 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 579.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0147 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 582.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9916 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 522.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9528 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 525.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9463 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-02 18:55:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 468.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-02 18:55:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8395 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:55:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:19 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-02 18:55:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 469.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-02 18:55:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8836 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:55:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:20 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-02 18:55:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 413.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-02 18:55:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6991 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:55:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:20 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-02 18:55:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 414.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-02 18:55:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7103 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:55:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:20 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-02 18:55:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 357.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-02 18:55:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5269 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:55:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:20 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-02 18:55:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 360.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-02 18:55:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.4883 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:55:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:20 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-02 18:55:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 301.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.6
INFO 10-02 18:55:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.3982 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:55:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:20 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-02 18:55:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 303.1 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.5
INFO 10-02 18:55:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.3341 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:55:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:20 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-02 18:55:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 242.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.5
INFO 10-02 18:55:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.2919 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:55:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:20 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-02 18:55:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 243.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.4
INFO 10-02 18:55:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.2508 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:55:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:20 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-02 18:55:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 184.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-02 18:55:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1045 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:55:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:20 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-02 18:55:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 185.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-02 18:55:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9969 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:55:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:20 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-02 18:55:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 123.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-02 18:55:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9528 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:55:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:20 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-02 18:55:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.0
INFO 10-02 18:55:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8942 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:55:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:20 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:55:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-02 18:55:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6696 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:55:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 18:55:20 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:55:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-02 18:55:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6662 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.1108441, last_token_time=1727895319.5345504, first_scheduled_time=1727895314.128203, first_token_time=1727895315.8003607, time_in_queue=0.017358779907226562, finished_time=1727895319.5344636, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.1133296, last_token_time=1727895319.5976412, first_scheduled_time=1727895315.800802, first_token_time=1727895315.9692276, time_in_queue=1.6874723434448242, finished_time=1727895319.597561, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.1141012, last_token_time=1727895319.639198, first_scheduled_time=1727895315.908438, first_token_time=1727895316.0755453, time_in_queue=1.7943367958068848, finished_time=1727895319.639122, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.114981, last_token_time=1727895319.680444, first_scheduled_time=1727895316.0266337, first_token_time=1727895316.1697688, time_in_queue=1.9116528034210205, finished_time=1727895319.680373, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.1156967, last_token_time=1727895319.7191257, first_scheduled_time=1727895316.1213431, first_token_time=1727895316.2643309, time_in_queue=2.0056464672088623, finished_time=1727895319.7190557, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.1164017, last_token_time=1727895319.757591, first_scheduled_time=1727895316.215815, first_token_time=1727895316.3590598, time_in_queue=2.0994133949279785, finished_time=1727895319.7575262, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.1171079, last_token_time=1727895319.7957308, first_scheduled_time=1727895316.3105948, first_token_time=1727895316.4537585, time_in_queue=2.1934869289398193, finished_time=1727895319.7956724, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.1178212, last_token_time=1727895319.8312736, first_scheduled_time=1727895316.4052134, first_token_time=1727895316.5486636, time_in_queue=2.2873921394348145, finished_time=1727895319.8312156, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.1192877, last_token_time=1727895319.866654, first_scheduled_time=1727895316.5001183, first_token_time=1727895316.6436903, time_in_queue=2.3808305263519287, finished_time=1727895319.8666024, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.1200852, last_token_time=1727895319.9017465, first_scheduled_time=1727895316.5950828, first_token_time=1727895316.738876, time_in_queue=2.4749975204467773, finished_time=1727895319.9016988, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.1207912, last_token_time=1727895319.9361696, first_scheduled_time=1727895316.6902204, first_token_time=1727895316.8343084, time_in_queue=2.5694291591644287, finished_time=1727895319.9361272, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.121484, last_token_time=1727895319.9705155, first_scheduled_time=1727895316.7855017, first_token_time=1727895316.927559, time_in_queue=2.664017677307129, finished_time=1727895319.9704757, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.1221828, last_token_time=1727895320.0046396, first_scheduled_time=1727895316.881341, first_token_time=1727895317.0196974, time_in_queue=2.759158134460449, finished_time=1727895320.0046043, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.122871, last_token_time=1727895320.038444, first_scheduled_time=1727895316.9737864, first_token_time=1727895317.1117477, time_in_queue=2.8509154319763184, finished_time=1727895320.038414, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.1236215, last_token_time=1727895320.071873, first_scheduled_time=1727895317.065974, first_token_time=1727895317.206248, time_in_queue=2.942352533340454, finished_time=1727895320.0718472, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.1243238, last_token_time=1727895320.1049805, first_scheduled_time=1727895317.1592104, first_token_time=1727895317.3005836, time_in_queue=3.034886598587036, finished_time=1727895320.1049592, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.125044, last_token_time=1727895320.137888, first_scheduled_time=1727895317.2536273, first_token_time=1727895317.3945713, time_in_queue=3.1285831928253174, finished_time=1727895320.1378698, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.1257658, last_token_time=1727895320.1703467, first_scheduled_time=1727895317.3481266, first_token_time=1727895317.4881277, time_in_queue=3.222360849380493, finished_time=1727895320.1703336, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.1265213, last_token_time=1727895320.2025423, first_scheduled_time=1727895317.4415731, first_token_time=1727895317.5820928, time_in_queue=3.315051794052124, finished_time=1727895320.202534, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727895314.1272283, last_token_time=1727895320.2342167, first_scheduled_time=1727895317.5352614, first_token_time=1727895317.668195, time_in_queue=3.4080331325531006, finished_time=1727895320.2342143, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 23040
End-to-End latency: 6.12 seconds
Throughput: 3.27 requests/s, 3762.50 tokens/s
Per_token_time: 0.266 ms

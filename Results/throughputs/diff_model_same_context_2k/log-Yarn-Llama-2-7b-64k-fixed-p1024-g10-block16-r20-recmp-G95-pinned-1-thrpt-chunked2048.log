Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='fixed', input_len=1024, output_len=10, min_len=10, max_len=50, prefill_to_decode_ratio=6.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=2048, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=2048, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
Selected required requests 20
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
User Requests: prompt_len=1024, output_len=10, sequence_len=1034...
Chunked prefill is enabled. max_num_batched_tokens=2048, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'finetuned', 'original_max_position_embeddings', 'type'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-02 18:21:15 config.py:654] [SchedulerConfig] max_num_batched_tokens: 2048 chunked_prefill_enabled: True
INFO 10-02 18:21:15 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
/home/jieye/viper2/vllm_v0_4_2_mine/vllm/executor/gpu_executor.py:36: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 10-02 18:21:16 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-02 18:21:21 selector.py:27] Using FlashAttention-2 backend.
INFO 10-02 18:21:44 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-02 18:21:44 model_runner.py:183] Loaded model: 
INFO 10-02 18:21:44 model_runner.py:183]  LlamaForCausalLM(
INFO 10-02 18:21:44 model_runner.py:183]   (model): LlamaModel(
INFO 10-02 18:21:44 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-02 18:21:44 model_runner.py:183]     (layers): ModuleList(
INFO 10-02 18:21:44 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-02 18:21:44 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-02 18:21:44 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:21:44 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:21:44 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-02 18:21:44 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-02 18:21:44 model_runner.py:183]         )
INFO 10-02 18:21:44 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-02 18:21:44 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:21:44 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:21:44 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-02 18:21:44 model_runner.py:183]         )
INFO 10-02 18:21:44 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:21:44 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:21:44 model_runner.py:183]       )
INFO 10-02 18:21:44 model_runner.py:183]     )
INFO 10-02 18:21:44 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:21:44 model_runner.py:183]   )
INFO 10-02 18:21:44 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-02 18:21:44 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-02 18:21:44 model_runner.py:183]   (sampler): Sampler()
INFO 10-02 18:21:44 model_runner.py:183] )
INFO 10-02 18:21:44 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:21:44 worker.py:164] Peak: 13.332 GB, Initial: 38.980 GB, Free: 25.648 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.092 GB
INFO 10-02 18:21:44 gpu_executor.py:117] # GPU blocks: 3083, # CPU blocks: 8192
INFO 10-02 18:22:15 worker.py:189] _init_cache_engine took 24.0859 GB
INFO 10-02 18:22:15 scheduler.py:307] Scheduler initialized with prompt limit: 2048
INFO 10-02 18:22:15 Start warmup...
INFO 10-02 18:22:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:22:17 metrics.py:335] Avg prompt throughput: 10.7 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%, Interval(ms): 1870.0
INFO 10-02 18:22:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1864.3961 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:22:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:17 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:22:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 46.4
INFO 10-02 18:22:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.9535 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:22:22 Start benchmarking...
INFO 10-02 18:22:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:22:24 metrics.py:335] Avg prompt throughput: 1177.8 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%, Interval(ms): 1738.9
INFO 10-02 18:22:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1714.9091 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:22:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:22:24 metrics.py:335] Avg prompt throughput: 15543.8 tokens/s, Avg generation throughput: 22.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 8.4%, CPU KV cache usage: 0.0%, Interval(ms): 131.6
INFO 10-02 18:22:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 131.3553 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:22:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:22:24 metrics.py:335] Avg prompt throughput: 14028.4 tokens/s, Avg generation throughput: 34.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 145.8
INFO 10-02 18:22:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 145.5951 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:22:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:22:24 metrics.py:335] Avg prompt throughput: 14005.0 tokens/s, Avg generation throughput: 48.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 16.8%, CPU KV cache usage: 0.0%, Interval(ms): 145.9
INFO 10-02 18:22:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 145.6492 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:22:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:22:24 metrics.py:335] Avg prompt throughput: 13935.5 tokens/s, Avg generation throughput: 61.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 146.5
INFO 10-02 18:22:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 146.2643 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:22:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:22:24 metrics.py:335] Avg prompt throughput: 13860.0 tokens/s, Avg generation throughput: 74.8 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 25.2%, CPU KV cache usage: 0.0%, Interval(ms): 147.1
INFO 10-02 18:22:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 146.9152 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:22:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:22:25 metrics.py:335] Avg prompt throughput: 13758.9 tokens/s, Avg generation throughput: 87.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 29.4%, CPU KV cache usage: 0.0%, Interval(ms): 148.0
INFO 10-02 18:22:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 147.8419 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:22:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:22:25 metrics.py:335] Avg prompt throughput: 13681.2 tokens/s, Avg generation throughput: 100.8 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 33.6%, CPU KV cache usage: 0.0%, Interval(ms): 148.7
INFO 10-02 18:22:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 148.5386 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:22:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:22:25 metrics.py:335] Avg prompt throughput: 13536.1 tokens/s, Avg generation throughput: 113.2 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 37.9%, CPU KV cache usage: 0.0%, Interval(ms): 150.2
INFO 10-02 18:22:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 149.9786 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:22:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:22:25 metrics.py:335] Avg prompt throughput: 13476.6 tokens/s, Avg generation throughput: 126.1 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.9%, CPU KV cache usage: 0.0%, Interval(ms): 150.7
INFO 10-02 18:22:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 150.4478 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:22:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([99]), positions.shape=torch.Size([99]) hidden_states.shape=torch.Size([99, 4096]) residual=None
INFO 10-02 18:22:25 metrics.py:335] Avg prompt throughput: 2786.1 tokens/s, Avg generation throughput: 611.6 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.8%, CPU KV cache usage: 0.0%, Interval(ms): 29.4
INFO 10-02 18:22:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 29.1417 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:22:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-02 18:22:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 788.4 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.6
INFO 10-02 18:22:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.3189 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:22:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-02 18:22:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 730.4 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-02 18:22:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2749 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:22:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-02 18:22:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 686.9 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-02 18:22:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.6799 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:22:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-02 18:22:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 607.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-02 18:22:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.8790 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:22:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-02 18:22:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 523.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-02 18:22:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9477 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:22:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-02 18:22:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 429.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-02 18:22:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.0873 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:22:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-02 18:22:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 322.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-02 18:22:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.2910 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:22:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-02 18:22:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-02 18:22:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5230 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:22:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:22:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:22:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-02 18:22:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4327 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.4341986, last_token_time=1727893345.4775321, first_scheduled_time=1727893342.448359, first_token_time=1727893344.1627817, time_in_queue=0.014160394668579102, finished_time=1727893345.4774687, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.4374099, last_token_time=1727893345.4775321, first_scheduled_time=1727893342.448359, first_token_time=1727893344.1627817, time_in_queue=0.010949134826660156, finished_time=1727893345.4774733, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.438183, last_token_time=1727893345.5069644, first_scheduled_time=1727893344.1634405, first_token_time=1727893344.294447, time_in_queue=1.725257396697998, finished_time=1727893345.506897, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.4387617, last_token_time=1727893345.5285263, first_scheduled_time=1727893344.1634405, first_token_time=1727893344.4401886, time_in_queue=1.7246787548065186, finished_time=1727893345.5284567, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.4393165, last_token_time=1727893345.5285263, first_scheduled_time=1727893344.2949536, first_token_time=1727893344.4401886, time_in_queue=1.8556370735168457, finished_time=1727893345.5284631, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.439868, last_token_time=1727893345.5490637, first_scheduled_time=1727893344.2949536, first_token_time=1727893344.5860417, time_in_queue=1.8550856113433838, finished_time=1727893345.5490031, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.4404006, last_token_time=1727893345.5490637, first_scheduled_time=1727893344.4407954, first_token_time=1727893344.5860417, time_in_queue=2.000394821166992, finished_time=1727893345.5490088, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.4409945, last_token_time=1727893345.5679905, first_scheduled_time=1727893344.4407954, first_token_time=1727893344.7324636, time_in_queue=1.9998009204864502, finished_time=1727893345.56794, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.4415283, last_token_time=1727893345.5679905, first_scheduled_time=1727893344.5866337, first_token_time=1727893344.7324636, time_in_queue=2.1451053619384766, finished_time=1727893345.567946, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.442052, last_token_time=1727893345.586106, first_scheduled_time=1727893344.5866337, first_token_time=1727893344.8795333, time_in_queue=2.1445817947387695, finished_time=1727893345.5860615, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.442583, last_token_time=1727893345.586106, first_scheduled_time=1727893344.7331133, first_token_time=1727893344.8795333, time_in_queue=2.290530204772949, finished_time=1727893345.5860677, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.4431007, last_token_time=1727893345.603286, first_scheduled_time=1727893344.7331133, first_token_time=1727893345.0275483, time_in_queue=2.2900125980377197, finished_time=1727893345.6032493, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.4436793, last_token_time=1727893345.603286, first_scheduled_time=1727893344.8802218, first_token_time=1727893345.0275483, time_in_queue=2.436542510986328, finished_time=1727893345.6032555, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.4442232, last_token_time=1727893345.6195946, first_scheduled_time=1727893344.8802218, first_token_time=1727893345.1762502, time_in_queue=2.4359986782073975, finished_time=1727893345.6195664, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.4447637, last_token_time=1727893345.6195946, first_scheduled_time=1727893345.0282762, first_token_time=1727893345.1762502, time_in_queue=2.583512544631958, finished_time=1727893345.6195724, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.4453266, last_token_time=1727893345.6350965, first_scheduled_time=1727893345.0282762, first_token_time=1727893345.3264132, time_in_queue=2.582949638366699, finished_time=1727893345.635076, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.4458709, last_token_time=1727893345.6350965, first_scheduled_time=1727893345.1770403, first_token_time=1727893345.3264132, time_in_queue=2.7311694622039795, finished_time=1727893345.6350822, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.4464576, last_token_time=1727893345.6498241, first_scheduled_time=1727893345.1770403, first_token_time=1727893345.4770265, time_in_queue=2.7305827140808105, finished_time=1727893345.649812, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.4469955, last_token_time=1727893345.6498241, first_scheduled_time=1727893345.3272953, first_token_time=1727893345.4770265, time_in_queue=2.8802998065948486, finished_time=1727893345.649818, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893342.4475315, last_token_time=1727893345.664456, first_scheduled_time=1727893345.3272953, first_token_time=1727893345.506519, time_in_queue=2.8797638416290283, finished_time=1727893345.664453, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 20680
End-to-End latency: 3.23 seconds
Throughput: 6.19 requests/s, 6401.56 tokens/s
Per_token_time: 0.156 ms

Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='fixed', input_len=1024, output_len=128, min_len=10, max_len=50, prefill_to_decode_ratio=6.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=2048, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=1024, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Selected required requests 20
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Chunked prefill is enabled. max_num_batched_tokens=1024, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'finetuned', 'original_max_position_embeddings', 'type'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-02 18:44:29 config.py:654] [SchedulerConfig] max_num_batched_tokens: 1024 chunked_prefill_enabled: True
INFO 10-02 18:44:29 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
/home/jieye/viper2/vllm_v0_4_2_mine/vllm/executor/gpu_executor.py:36: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 10-02 18:44:30 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-02 18:44:35 selector.py:27] Using FlashAttention-2 backend.
INFO 10-02 18:44:53 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-02 18:44:53 model_runner.py:183] Loaded model: 
INFO 10-02 18:44:53 model_runner.py:183]  LlamaForCausalLM(
INFO 10-02 18:44:53 model_runner.py:183]   (model): LlamaModel(
INFO 10-02 18:44:53 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-02 18:44:53 model_runner.py:183]     (layers): ModuleList(
INFO 10-02 18:44:53 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-02 18:44:53 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-02 18:44:53 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:44:53 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:44:53 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-02 18:44:53 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-02 18:44:53 model_runner.py:183]         )
INFO 10-02 18:44:53 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-02 18:44:53 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:44:53 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:44:53 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-02 18:44:53 model_runner.py:183]         )
INFO 10-02 18:44:53 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:44:53 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:44:53 model_runner.py:183]       )
INFO 10-02 18:44:53 model_runner.py:183]     )
INFO 10-02 18:44:53 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:44:53 model_runner.py:183]   )
INFO 10-02 18:44:53 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-02 18:44:53 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-02 18:44:53 model_runner.py:183]   (sampler): Sampler()
INFO 10-02 18:44:53 model_runner.py:183] )
INFO 10-02 18:44:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:44:53 worker.py:164] Peak: 13.307 GB, Initial: 38.980 GB, Free: 25.673 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.118 GB
INFO 10-02 18:44:53 gpu_executor.py:117] # GPU blocks: 3087, # CPU blocks: 8192
INFO 10-02 18:45:25 worker.py:189] _init_cache_engine took 24.1250 GB
INFO 10-02 18:45:25 scheduler.py:307] Scheduler initialized with prompt limit: 2048
INFO 10-02 18:45:25 Start warmup...
INFO 10-02 18:45:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:26 metrics.py:335] Avg prompt throughput: 10.5 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%, Interval(ms): 1912.5
INFO 10-02 18:45:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1906.6055 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:45:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:45:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 49.2
INFO 10-02 18:45:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.8141 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:45:31 Start benchmarking...
INFO 10-02 18:45:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:32 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:33 metrics.py:335] Avg prompt throughput: 606.6 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 19 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 1688.1
INFO 10-02 18:45:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1664.1624 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:45:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:33 metrics.py:335] Avg prompt throughput: 11222.1 tokens/s, Avg generation throughput: 11.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%, Interval(ms): 91.2
INFO 10-02 18:45:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 90.8971 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:45:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:33 metrics.py:335] Avg prompt throughput: 11030.4 tokens/s, Avg generation throughput: 21.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 17 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%, Interval(ms): 92.7
INFO 10-02 18:45:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 92.5543 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:45:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:33 metrics.py:335] Avg prompt throughput: 11866.2 tokens/s, Avg generation throughput: 34.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 8.4%, CPU KV cache usage: 0.0%, Interval(ms): 86.1
INFO 10-02 18:45:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 85.9053 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:45:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:34 metrics.py:335] Avg prompt throughput: 11849.8 tokens/s, Avg generation throughput: 46.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%, Interval(ms): 86.2
INFO 10-02 18:45:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 85.9885 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:45:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:34 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:34 metrics.py:335] Avg prompt throughput: 11781.4 tokens/s, Avg generation throughput: 57.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 86.6
INFO 10-02 18:45:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.3979 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:45:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:34 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:34 metrics.py:335] Avg prompt throughput: 11740.8 tokens/s, Avg generation throughput: 69.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 14.7%, CPU KV cache usage: 0.0%, Interval(ms): 86.8
INFO 10-02 18:45:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.6127 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:45:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:34 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:34 metrics.py:335] Avg prompt throughput: 11702.2 tokens/s, Avg generation throughput: 80.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 16.8%, CPU KV cache usage: 0.0%, Interval(ms): 87.0
INFO 10-02 18:45:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.8182 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:45:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:34 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:34 metrics.py:335] Avg prompt throughput: 11658.1 tokens/s, Avg generation throughput: 91.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 87.2
INFO 10-02 18:45:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 87.0554 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:45:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:34 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:34 metrics.py:335] Avg prompt throughput: 11584.4 tokens/s, Avg generation throughput: 102.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 87.7
INFO 10-02 18:45:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 87.4851 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:45:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:34 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:34 metrics.py:335] Avg prompt throughput: 11504.2 tokens/s, Avg generation throughput: 113.3 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 88.2
INFO 10-02 18:45:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 88.0444 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:45:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:34 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:34 metrics.py:335] Avg prompt throughput: 11497.4 tokens/s, Avg generation throughput: 124.7 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 25.2%, CPU KV cache usage: 0.0%, Interval(ms): 88.2
INFO 10-02 18:45:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 88.0048 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:45:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:34 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:34 metrics.py:335] Avg prompt throughput: 11421.5 tokens/s, Avg generation throughput: 135.3 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 27.3%, CPU KV cache usage: 0.0%, Interval(ms): 88.7
INFO 10-02 18:45:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 88.5007 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:45:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:34 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:34 metrics.py:335] Avg prompt throughput: 11365.1 tokens/s, Avg generation throughput: 146.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 29.4%, CPU KV cache usage: 0.0%, Interval(ms): 89.0
INFO 10-02 18:45:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 88.8469 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:45:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:34 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:34 metrics.py:335] Avg prompt throughput: 11309.8 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 31.5%, CPU KV cache usage: 0.0%, Interval(ms): 89.4
INFO 10-02 18:45:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 89.1900 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:45:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:34 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:34 metrics.py:335] Avg prompt throughput: 11110.9 tokens/s, Avg generation throughput: 165.0 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 33.6%, CPU KV cache usage: 0.0%, Interval(ms): 90.9
INFO 10-02 18:45:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 90.6651 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:45:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:34 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 11105.1 tokens/s, Avg generation throughput: 176.1 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 35.7%, CPU KV cache usage: 0.0%, Interval(ms): 90.9
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 90.6508 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 11350.6 tokens/s, Avg generation throughput: 191.4 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 37.9%, CPU KV cache usage: 0.0%, Interval(ms): 88.8
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 88.5890 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 11381.0 tokens/s, Avg generation throughput: 203.4 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 88.5
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 88.2573 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 11302.7 tokens/s, Avg generation throughput: 213.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.1%, CPU KV cache usage: 0.0%, Interval(ms): 89.0
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 88.7814 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([191]), positions.shape=torch.Size([191]) hidden_states.shape=torch.Size([191, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 5050.5 tokens/s, Avg generation throughput: 587.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 34.1
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 33.8330 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 882.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4075 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 883.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4214 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 881.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4769 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4831 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 881.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4597 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 883.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4082 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 876.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5754 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 882.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4376 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 881.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4371 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 882.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4543 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 881.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4779 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 882.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4268 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.7%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5127 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.7%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4922 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4946 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 881.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4683 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 881.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.9%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4874 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 881.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.9%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4829 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 878.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.0%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5599 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 881.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.0%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4810 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 879.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.0%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5258 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5010 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 880.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5132 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 878.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5487 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 877.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5763 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 877.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5503 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 876.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:45:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5992 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 874.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.2%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6245 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 877.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5878 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 876.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6111 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 877.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5773 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6371 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 876.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.5%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6166 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 876.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6138 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 873.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6789 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 874.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6457 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 874.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.7%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6581 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 872.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.7%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6767 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 872.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.7%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6979 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 872.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6996 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 874.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.8%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6617 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 870.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7489 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 871.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.9%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7239 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 871.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6960 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 872.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.0%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7087 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 872.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.0%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7010 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 872.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7127 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 871.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7387 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8462 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 868.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7809 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 871.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.9
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7320 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 870.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7656 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 870.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7609 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 868.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8128 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 869.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.7892 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 866.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8343 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8372 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8407 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8331 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 866.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8670 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8407 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 866.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8789 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 867.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8305 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 866.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8565 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 865.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8827 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 865.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9032 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 864.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9273 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9845 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 863.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9535 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 864.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:45:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9213 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:36 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9604 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9592 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9633 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0119 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 863.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9518 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9704 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 861.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9995 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 861.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9881 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 859.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0629 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 859.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0415 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0296 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 859.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0572 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.5%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0384 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0165 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0269 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.6%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0243 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0436 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0672 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 859.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.7%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0567 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 857.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1187 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 859.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0625 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.8%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0813 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 856.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1063 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 857.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1011 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 856.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1488 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 857.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1197 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 855.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1571 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 855.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1724 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 852.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.5
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.2036 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 855.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1562 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 846.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.6
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.4091 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1843 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.3%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1898 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1855 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 851.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.5
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.2263 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 854.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.4
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1948 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 852.0 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 23.5
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.2568 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 822.1 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 44.2%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8369 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 824.7 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 41.9%, CPU KV cache usage: 0.0%, Interval(ms): 23.0
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.8245 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([18]), positions.shape=torch.Size([18]) hidden_states.shape=torch.Size([18, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 793.7 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.6%, CPU KV cache usage: 0.0%, Interval(ms): 22.7
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4154 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 762.7 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 22.3
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.0373 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 743.9 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2545 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-02 18:45:37 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 707.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:45:37 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9484 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:45:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:37 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-02 18:45:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 669.1 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:45:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6845 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:45:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-02 18:45:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 669.7 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.4
INFO 10-02 18:45:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.1739 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:45:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-02 18:45:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 633.5 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.7%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-02 18:45:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7094 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:45:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-02 18:45:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 591.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-02 18:45:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3666 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:45:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-02 18:45:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 556.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-02 18:45:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7295 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:45:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-02 18:45:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 515.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-02 18:45:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2551 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:45:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-02 18:45:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 470.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 10-02 18:45:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7754 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:45:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-02 18:45:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.5
INFO 10-02 18:45:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.2580 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:45:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-02 18:45:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 375.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.0
INFO 10-02 18:45:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7886 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:45:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-02 18:45:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 319.5 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-02 18:45:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4464 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:45:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-02 18:45:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 261.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.3
INFO 10-02 18:45:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.0721 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:45:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-02 18:45:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%, Interval(ms): 14.8
INFO 10-02 18:45:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.6112 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:45:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-02 18:45:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 137.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-02 18:45:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4081 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:45:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-02 18:45:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:45:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-02 18:45:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4870 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894731.9907756, last_token_time=1727894737.8499718, first_scheduled_time=1727894732.0048385, first_token_time=1727894733.6685991, time_in_queue=0.014062881469726562, finished_time=1727894737.8498902, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894731.9938686, last_token_time=1727894737.8961208, first_scheduled_time=1727894733.6691635, first_token_time=1727894733.8525114, time_in_queue=1.6752948760986328, finished_time=1727894737.896044, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894731.9947155, last_token_time=1727894737.9187984, first_scheduled_time=1727894733.7602348, first_token_time=1727894733.9386344, time_in_queue=1.765519380569458, finished_time=1727894737.9187233, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894731.9952862, last_token_time=1727894737.941089, first_scheduled_time=1727894733.8530157, first_token_time=1727894734.0247903, time_in_queue=1.857729434967041, finished_time=1727894737.94102, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894731.9958339, last_token_time=1727894737.962597, first_scheduled_time=1727894733.9390917, first_token_time=1727894734.1113434, time_in_queue=1.9432578086853027, finished_time=1727894737.9625328, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894731.9963744, last_token_time=1727894737.9837902, first_scheduled_time=1727894734.0252676, first_token_time=1727894734.1981163, time_in_queue=2.028893232345581, finished_time=1727894737.983729, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894731.9969163, last_token_time=1727894738.0047133, first_scheduled_time=1727894734.1118295, first_token_time=1727894734.285066, time_in_queue=2.11491322517395, finished_time=1727894738.0046563, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894731.9975095, last_token_time=1727894738.024124, first_scheduled_time=1727894734.1986165, first_token_time=1727894734.3723047, time_in_queue=2.2011070251464844, finished_time=1727894738.0240698, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894731.998051, last_token_time=1727894738.0430663, first_scheduled_time=1727894734.2856214, first_token_time=1727894734.4599974, time_in_queue=2.2875704765319824, finished_time=1727894738.0430188, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894731.998572, last_token_time=1727894738.0616572, first_scheduled_time=1727894734.3729093, first_token_time=1727894734.548197, time_in_queue=2.3743371963500977, finished_time=1727894738.0616114, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894731.9990969, last_token_time=1727894738.0796173, first_scheduled_time=1727894734.4605744, first_token_time=1727894734.636374, time_in_queue=2.461477518081665, finished_time=1727894738.079576, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894731.9996161, last_token_time=1727894738.0970926, first_scheduled_time=1727894734.5488105, first_token_time=1727894734.7250452, time_in_queue=2.5491943359375, finished_time=1727894738.097056, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894732.0001876, last_token_time=1727894738.114083, first_scheduled_time=1727894734.6370125, first_token_time=1727894734.8140688, time_in_queue=2.6368248462677, finished_time=1727894738.114051, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894732.0007246, last_token_time=1727894738.1305528, first_scheduled_time=1727894734.725719, first_token_time=1727894734.9034393, time_in_queue=2.724994421005249, finished_time=1727894738.1305234, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894732.0012646, last_token_time=1727894738.1465478, first_scheduled_time=1727894734.8147666, first_token_time=1727894734.9943242, time_in_queue=2.813502073287964, finished_time=1727894738.1465232, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894732.0018218, last_token_time=1727894738.162198, first_scheduled_time=1727894734.9042084, first_token_time=1727894735.085159, time_in_queue=2.9023866653442383, finished_time=1727894738.1621776, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894732.002361, last_token_time=1727894738.1774695, first_scheduled_time=1727894734.9950764, first_token_time=1727894735.1739442, time_in_queue=2.992715358734131, finished_time=1727894738.1774518, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894732.0029604, last_token_time=1727894738.192276, first_scheduled_time=1727894735.08595, first_token_time=1727894735.2624, time_in_queue=3.082989454269409, finished_time=1727894738.1922636, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894732.003502, last_token_time=1727894738.2068746, first_scheduled_time=1727894735.1747735, first_token_time=1727894735.3513932, time_in_queue=3.171271562576294, finished_time=1727894738.2068665, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894732.0040436, last_token_time=1727894738.221548, first_scheduled_time=1727894735.2632523, first_token_time=1727894735.3854244, time_in_queue=3.2592086791992188, finished_time=1727894738.2215455, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 23040
End-to-End latency: 6.23 seconds
Throughput: 3.21 requests/s, 3697.65 tokens/s
Per_token_time: 0.270 ms

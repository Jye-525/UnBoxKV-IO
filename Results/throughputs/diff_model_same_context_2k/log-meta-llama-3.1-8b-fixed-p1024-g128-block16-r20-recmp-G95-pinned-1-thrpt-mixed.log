Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='fixed', input_len=1024, output_len=128, min_len=10, max_len=50, prefill_to_decode_ratio=6.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=2048, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=2048, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=True)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Selected required requests 20
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Mixed batch is enabled. max_num_batched_tokens=2048, max_num_batched_seqs=256
rope_scaling = {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}
INFO 10-02 18:39:08 config.py:654] [SchedulerConfig] max_num_batched_tokens: 2048 chunked_prefill_enabled: False
INFO 10-02 18:39:08 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b, log_stats=True, preemption_mode=recompute, enable_mixed_batch=True)
/home/jieye/viper2/vllm_v0_4_2_mine/vllm/executor/gpu_executor.py:36: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 10-02 18:39:08 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-02 18:39:13 selector.py:27] Using FlashAttention-2 backend.
INFO 10-02 18:39:21 model_runner.py:180] Loading model weights took 14.9888 GB
INFO 10-02 18:39:21 model_runner.py:183] Loaded model: 
INFO 10-02 18:39:21 model_runner.py:183]  LlamaForCausalLM(
INFO 10-02 18:39:21 model_runner.py:183]   (model): LlamaModel(
INFO 10-02 18:39:21 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=128256, embedding_dim=4096, org_vocab_size=128256, num_embeddings_padded=128256, tp_size=1)
INFO 10-02 18:39:21 model_runner.py:183]     (layers): ModuleList(
INFO 10-02 18:39:21 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-02 18:39:21 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-02 18:39:21 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:39:21 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:39:21 model_runner.py:183]           (rotary_emb): Llama3RotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=131072, base=500000.0, is_neox_style=True)
INFO 10-02 18:39:21 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845)
INFO 10-02 18:39:21 model_runner.py:183]         )
INFO 10-02 18:39:21 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-02 18:39:21 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=28672, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:39:21 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=14336, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:39:21 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-02 18:39:21 model_runner.py:183]         )
INFO 10-02 18:39:21 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:39:21 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:39:21 model_runner.py:183]       )
INFO 10-02 18:39:21 model_runner.py:183]     )
INFO 10-02 18:39:21 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:39:21 model_runner.py:183]   )
INFO 10-02 18:39:21 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=128256, embedding_dim=4096, org_vocab_size=128256, num_embeddings_padded=128256, tp_size=1)
INFO 10-02 18:39:21 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=128256, forg_vocab_size=128256, scale=1.0, logits_as_input=False)
INFO 10-02 18:39:21 model_runner.py:183]   (sampler): Sampler()
INFO 10-02 18:39:21 model_runner.py:183] )
INFO 10-02 18:39:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:39:22 worker.py:164] Peak: 16.531 GB, Initial: 38.980 GB, Free: 22.448 GB, Total: 39.394 GB,               cache_block_size: 2097152 Bytes, available GPU for KV cache: 20.893 GB
INFO 10-02 18:39:22 gpu_executor.py:117] # GPU blocks: 10697, # CPU blocks: 32768
INFO 10-02 18:39:53 worker.py:189] _init_cache_engine took 20.8926 GB
INFO 10-02 18:39:53 scheduler.py:307] Scheduler initialized with prompt limit: 2048
INFO 10-02 18:39:53 Start warmup...
INFO 10-02 18:39:53 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:39:53 metrics.py:335] Avg prompt throughput: 673.2 tokens/s, Avg generation throughput: 33.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 29.7
INFO 10-02 18:39:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1116 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:39:53 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:53 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:39:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 54.2
INFO 10-02 18:39:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.7868 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:39:58 Start benchmarking...
INFO 10-02 18:39:58 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:58 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:39:58 metrics.py:335] Avg prompt throughput: 11305.0 tokens/s, Avg generation throughput: 11.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%, Interval(ms): 181.2
INFO 10-02 18:39:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 154.0165 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:39:58 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:58 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1026]), positions.shape=torch.Size([1026]) hidden_states.shape=torch.Size([1026, 4096]) residual=None
INFO 10-02 18:39:58 metrics.py:335] Avg prompt throughput: 13271.5 tokens/s, Avg generation throughput: 38.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 17 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%, Interval(ms): 77.2
INFO 10-02 18:39:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.8607 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:39:58 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:58 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1027]), positions.shape=torch.Size([1027]) hidden_states.shape=torch.Size([1027, 4096]) residual=None
INFO 10-02 18:39:59 metrics.py:335] Avg prompt throughput: 13292.3 tokens/s, Avg generation throughput: 51.9 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%, Interval(ms): 77.0
INFO 10-02 18:39:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.8611 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:39:59 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:59 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1028]), positions.shape=torch.Size([1028]) hidden_states.shape=torch.Size([1028, 4096]) residual=None
INFO 10-02 18:39:59 metrics.py:335] Avg prompt throughput: 13306.3 tokens/s, Avg generation throughput: 65.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%, Interval(ms): 77.0
INFO 10-02 18:39:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.7837 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:39:59 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:59 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1029]), positions.shape=torch.Size([1029]) hidden_states.shape=torch.Size([1029, 4096]) residual=None
INFO 10-02 18:39:59 metrics.py:335] Avg prompt throughput: 13325.2 tokens/s, Avg generation throughput: 78.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%, Interval(ms): 76.8
INFO 10-02 18:39:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.6721 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:39:59 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:59 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1030]), positions.shape=torch.Size([1030]) hidden_states.shape=torch.Size([1030, 4096]) residual=None
INFO 10-02 18:39:59 metrics.py:335] Avg prompt throughput: 13316.1 tokens/s, Avg generation throughput: 91.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%, Interval(ms): 76.9
INFO 10-02 18:39:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.7190 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:39:59 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:59 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1031]), positions.shape=torch.Size([1031]) hidden_states.shape=torch.Size([1031, 4096]) residual=None
INFO 10-02 18:39:59 metrics.py:335] Avg prompt throughput: 13238.9 tokens/s, Avg generation throughput: 103.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%, Interval(ms): 77.3
INFO 10-02 18:39:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.1635 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:39:59 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:59 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1032]), positions.shape=torch.Size([1032]) hidden_states.shape=torch.Size([1032, 4096]) residual=None
INFO 10-02 18:39:59 metrics.py:335] Avg prompt throughput: 13226.0 tokens/s, Avg generation throughput: 116.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 5.5%, CPU KV cache usage: 0.0%, Interval(ms): 77.4
INFO 10-02 18:39:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.2417 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:39:59 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:59 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1033]), positions.shape=torch.Size([1033]) hidden_states.shape=torch.Size([1033, 4096]) residual=None
INFO 10-02 18:39:59 metrics.py:335] Avg prompt throughput: 13208.6 tokens/s, Avg generation throughput: 129.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%, Interval(ms): 77.5
INFO 10-02 18:39:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.3418 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:39:59 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:59 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1034]), positions.shape=torch.Size([1034]) hidden_states.shape=torch.Size([1034, 4096]) residual=None
INFO 10-02 18:39:59 metrics.py:335] Avg prompt throughput: 13186.1 tokens/s, Avg generation throughput: 141.6 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%, Interval(ms): 77.7
INFO 10-02 18:39:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.4708 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:39:59 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:59 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1035]), positions.shape=torch.Size([1035]) hidden_states.shape=torch.Size([1035, 4096]) residual=None
INFO 10-02 18:39:59 metrics.py:335] Avg prompt throughput: 13118.4 tokens/s, Avg generation throughput: 153.7 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 78.1
INFO 10-02 18:39:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.8642 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:39:59 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:59 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1036]), positions.shape=torch.Size([1036]) hidden_states.shape=torch.Size([1036, 4096]) residual=None
INFO 10-02 18:39:59 metrics.py:335] Avg prompt throughput: 13010.6 tokens/s, Avg generation throughput: 165.2 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 7.9%, CPU KV cache usage: 0.0%, Interval(ms): 78.7
INFO 10-02 18:39:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 78.5155 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:39:59 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:59 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1037]), positions.shape=torch.Size([1037]) hidden_states.shape=torch.Size([1037, 4096]) residual=None
INFO 10-02 18:39:59 metrics.py:335] Avg prompt throughput: 13079.3 tokens/s, Avg generation throughput: 178.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 8.5%, CPU KV cache usage: 0.0%, Interval(ms): 78.3
INFO 10-02 18:39:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 78.0957 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:39:59 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:59 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1038]), positions.shape=torch.Size([1038]) hidden_states.shape=torch.Size([1038, 4096]) residual=None
INFO 10-02 18:39:59 metrics.py:335] Avg prompt throughput: 12912.1 tokens/s, Avg generation throughput: 189.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 9.1%, CPU KV cache usage: 0.0%, Interval(ms): 79.3
INFO 10-02 18:39:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 79.1042 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:39:59 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:59 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1039]), positions.shape=torch.Size([1039]) hidden_states.shape=torch.Size([1039, 4096]) residual=None
INFO 10-02 18:39:59 metrics.py:335] Avg prompt throughput: 12898.5 tokens/s, Avg generation throughput: 201.5 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 9.7%, CPU KV cache usage: 0.0%, Interval(ms): 79.4
INFO 10-02 18:39:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 79.1833 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:39:59 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:39:59 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1040]), positions.shape=torch.Size([1040]) hidden_states.shape=torch.Size([1040, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 12801.3 tokens/s, Avg generation throughput: 212.5 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 10.3%, CPU KV cache usage: 0.0%, Interval(ms): 80.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 79.7842 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1041]), positions.shape=torch.Size([1041]) hidden_states.shape=torch.Size([1041, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 12838.2 tokens/s, Avg generation throughput: 225.7 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 10.9%, CPU KV cache usage: 0.0%, Interval(ms): 79.8
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 79.5486 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1042]), positions.shape=torch.Size([1042]) hidden_states.shape=torch.Size([1042, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 12733.9 tokens/s, Avg generation throughput: 236.3 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 11.6%, CPU KV cache usage: 0.0%, Interval(ms): 80.4
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 80.2021 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1043]), positions.shape=torch.Size([1043]) hidden_states.shape=torch.Size([1043, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 12754.6 tokens/s, Avg generation throughput: 249.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 80.3
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 80.0703 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7548 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7548 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7996 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7467 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 958.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6480 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 956.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6962 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 936.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1399 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 957.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6702 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7593 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 955.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7164 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 955.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7212 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 955.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7162 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 956.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6981 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7598 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7353 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7818 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7524 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7810 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7849 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7846 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8120 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7720 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7798 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7884 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7453 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7508 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7608 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8592 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7660 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7767 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7963 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7961 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7996 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8194 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:00 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:00 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8426 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8180 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7992 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8011 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7963 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7939 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8154 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8447 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7827 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 951.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8020 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7787 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8282 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8523 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8597 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8933 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8910 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9169 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9296 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8974 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8814 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8962 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9141 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9086 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9205 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8898 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8874 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9043 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9048 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9332 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9901 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9565 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9649 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0080 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9560 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9625 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9582 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9627 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9761 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9572 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9608 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9558 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9551 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9634 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0097 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9823 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9894 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9873 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0111 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:01 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:01 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9992 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0238 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0159 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9997 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0083 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0173 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 938.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0609 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9754 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0226 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0288 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0195 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0109 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0218 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0004 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0023 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0361 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0028 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0714 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 938.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1036 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0280 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9985 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9911 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0123 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9975 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0395 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0145 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.6 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0443 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([18]), positions.shape=torch.Size([18]) hidden_states.shape=torch.Size([18, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 864.0 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5100 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 825.2 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3438 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 828.6 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.1%, CPU KV cache usage: 0.0%, Interval(ms): 19.3
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.0594 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 779.3 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.0022 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 735.1 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.8%, CPU KV cache usage: 0.0%, Interval(ms): 19.0
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.8043 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 732.5 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.7
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.5076 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 679.5 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.7
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.4274 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 626.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.6
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3321 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 581.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9685 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 525.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9156 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 468.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8242 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 415.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6516 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 360.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.6
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.4351 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 301.1 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.6
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.3989 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 243.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.4
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.2451 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 185.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.0198 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8467 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:40:02 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-02 18:40:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:40:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-02 18:40:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6786 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.690426, last_token_time=1727894402.571924, first_scheduled_time=1727894398.7077086, first_token_time=1727894398.8613284, time_in_queue=0.017282485961914062, finished_time=1727894402.5718403, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.693623, last_token_time=1727894402.571924, first_scheduled_time=1727894398.7077086, first_token_time=1727894398.8613284, time_in_queue=0.014085531234741211, finished_time=1727894402.5718462, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.694482, last_token_time=1727894402.5927565, first_scheduled_time=1727894398.8619034, first_token_time=1727894398.9385056, time_in_queue=0.1674213409423828, finished_time=1727894402.5926826, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.6952178, last_token_time=1727894402.6133564, first_scheduled_time=1727894398.9389477, first_token_time=1727894399.0155258, time_in_queue=0.243729829788208, finished_time=1727894402.613288, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.6959312, last_token_time=1727894402.6326654, first_scheduled_time=1727894399.0160153, first_token_time=1727894399.09248, time_in_queue=0.3200840950012207, finished_time=1727894402.6326022, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.6966474, last_token_time=1727894402.6519136, first_scheduled_time=1727894399.0929313, first_token_time=1727894399.169305, time_in_queue=0.3962838649749756, finished_time=1727894402.6518557, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.6974154, last_token_time=1727894402.6709578, first_scheduled_time=1727894399.169806, first_token_time=1727894399.2461896, time_in_queue=0.47239065170288086, finished_time=1727894402.670901, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.698141, last_token_time=1727894402.6887047, first_scheduled_time=1727894399.2466838, first_token_time=1727894399.323517, time_in_queue=0.5485427379608154, finished_time=1727894402.6886542, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.6988728, last_token_time=1727894402.7063644, first_scheduled_time=1727894399.3240302, first_token_time=1727894399.4009137, time_in_queue=0.625157356262207, finished_time=1727894402.7063165, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.6995707, last_token_time=1727894402.7239265, first_scheduled_time=1727894399.401461, first_token_time=1727894399.4784245, time_in_queue=0.701890230178833, finished_time=1727894402.7238836, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.700265, last_token_time=1727894402.7411218, first_scheduled_time=1727894399.4790301, first_token_time=1727894399.5560622, time_in_queue=0.7787652015686035, finished_time=1727894402.741082, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.7010176, last_token_time=1727894402.7582595, first_scheduled_time=1727894399.5566595, first_token_time=1727894399.6341023, time_in_queue=0.8556418418884277, finished_time=1727894402.7582211, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.701713, last_token_time=1727894402.775341, first_scheduled_time=1727894399.6347144, first_token_time=1727894399.7127893, time_in_queue=0.9330012798309326, finished_time=1727894402.7753093, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.7024267, last_token_time=1727894402.7922063, first_scheduled_time=1727894399.7134285, first_token_time=1727894399.791061, time_in_queue=1.0110018253326416, finished_time=1727894402.7921784, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.703133, last_token_time=1727894402.808853, first_scheduled_time=1727894399.791728, first_token_time=1727894399.8703516, time_in_queue=1.088594913482666, finished_time=1727894402.808829, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.7038496, last_token_time=1727894402.8254597, first_scheduled_time=1727894399.871044, first_token_time=1727894399.9497116, time_in_queue=1.1671943664550781, finished_time=1727894402.8254392, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.7045977, last_token_time=1727894402.8419068, first_scheduled_time=1727894399.950482, first_token_time=1727894400.029684, time_in_queue=1.2458841800689697, finished_time=1727894402.8418896, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.7053084, last_token_time=1727894402.858124, first_scheduled_time=1727894400.0304356, first_token_time=1727894400.1094203, time_in_queue=1.325127124786377, finished_time=1727894402.8581111, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.706008, last_token_time=1727894402.8742056, first_scheduled_time=1727894400.1102026, first_token_time=1727894400.1898265, time_in_queue=1.4041945934295654, finished_time=1727894402.8741972, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727894398.7067273, last_token_time=1727894402.890073, first_scheduled_time=1727894400.190617, first_token_time=1727894400.2700973, time_in_queue=1.4838898181915283, finished_time=1727894402.8900704, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 23040
End-to-End latency: 4.20 seconds
Throughput: 4.76 requests/s, 5485.89 tokens/s
Per_token_time: 0.182 ms

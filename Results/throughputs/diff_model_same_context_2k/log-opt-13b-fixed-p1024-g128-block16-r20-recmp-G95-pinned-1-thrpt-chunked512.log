/home/jieye/venvs/vllm_env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/opt-13b', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/opt-13b', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='fixed', input_len=1024, output_len=128, min_len=10, max_len=50, prefill_to_decode_ratio=6.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=2048, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=512, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Selected required requests 20
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Chunked prefill is enabled. max_num_batched_tokens=512, max_num_batched_seqs=256
rope_scaling = None
INFO 10-02 19:05:43 config.py:654] [SchedulerConfig] max_num_batched_tokens: 512 chunked_prefill_enabled: True
INFO 10-02 19:05:43 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/opt-13b', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/opt-13b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/opt-13b, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
/home/jieye/venvs/vllm_env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/home/jieye/viper2/vllm_v0_4_2_mine/vllm/executor/gpu_executor.py:36: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 10-02 19:05:43 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-02 19:05:48 selector.py:27] Using FlashAttention-2 backend.
INFO 10-02 19:05:55 model_runner.py:180] Loading model weights took 23.9425 GB
INFO 10-02 19:05:55 model_runner.py:183] Loaded model: 
INFO 10-02 19:05:55 model_runner.py:183]  OPTForCausalLM(
INFO 10-02 19:05:55 model_runner.py:183]   (model): OPTModel(
INFO 10-02 19:05:55 model_runner.py:183]     (decoder): OPTDecoder(
INFO 10-02 19:05:55 model_runner.py:183]       (embed_tokens): VocabParallelEmbedding(num_embeddings=50304, embedding_dim=5120, org_vocab_size=50272, num_embeddings_padded=50304, tp_size=1)
INFO 10-02 19:05:55 model_runner.py:183]       (embed_positions): OPTLearnedPositionalEmbedding(2050, 5120)
INFO 10-02 19:05:55 model_runner.py:183]       (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
INFO 10-02 19:05:55 model_runner.py:183]       (layers): ModuleList(
INFO 10-02 19:05:55 model_runner.py:183]         (0-39): 40 x OPTDecoderLayer(
INFO 10-02 19:05:55 model_runner.py:183]           (self_attn): OPTAttention(
INFO 10-02 19:05:55 model_runner.py:183]             (qkv_proj): QKVParallelLinear(in_features=5120, output_features=15360, bias=True, tp_size=1, gather_output=False)
INFO 10-02 19:05:55 model_runner.py:183]             (out_proj): RowParallelLinear(input_features=5120, output_features=5120, bias=True, tp_size=1, reduce_results=True)
INFO 10-02 19:05:55 model_runner.py:183]             (attn): Attention(head_size=128, num_heads=40, num_kv_heads=40, scale=0.08838834764831845)
INFO 10-02 19:05:55 model_runner.py:183]           )
INFO 10-02 19:05:55 model_runner.py:183]           (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
INFO 10-02 19:05:55 model_runner.py:183]           (fc1): ColumnParallelLinear(in_features=5120, output_features=20480, bias=True, tp_size=1, gather_output=False)
INFO 10-02 19:05:55 model_runner.py:183]           (activation_fn): ReLU()
INFO 10-02 19:05:55 model_runner.py:183]           (fc2): RowParallelLinear(input_features=20480, output_features=5120, bias=True, tp_size=1, reduce_results=True)
INFO 10-02 19:05:55 model_runner.py:183]           (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
INFO 10-02 19:05:55 model_runner.py:183]         )
INFO 10-02 19:05:55 model_runner.py:183]       )
INFO 10-02 19:05:55 model_runner.py:183]     )
INFO 10-02 19:05:55 model_runner.py:183]   )
INFO 10-02 19:05:55 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=50272, forg_vocab_size=50272, scale=1.0, logits_as_input=False)
INFO 10-02 19:05:55 model_runner.py:183]   (sampler): Sampler()
INFO 10-02 19:05:55 model_runner.py:183] )
INFO 10-02 19:05:55 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:05:55 worker.py:164] Peak: 24.844 GB, Initial: 38.980 GB, Free: 14.136 GB, Total: 39.394 GB,               cache_block_size: 13107200 Bytes, available GPU for KV cache: 12.580 GB
INFO 10-02 19:05:55 gpu_executor.py:117] # GPU blocks: 1030, # CPU blocks: 5242
INFO 10-02 19:06:34 worker.py:189] _init_cache_engine took 12.5781 GB
INFO 10-02 19:06:34 scheduler.py:307] Scheduler initialized with prompt limit: 2048
INFO 10-02 19:06:34 Start warmup...
INFO 10-02 19:06:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:34 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([21]), positions.shape=torch.Size([21])
INFO 10-02 19:06:36 metrics.py:335] Avg prompt throughput: 11.3 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%, Interval(ms): 1857.3
INFO 10-02 19:06:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1849.9243 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 19:06:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:36 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1])
INFO 10-02 19:06:36 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 60.3
INFO 10-02 19:06:36 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 59.8652 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 19:06:41 Start benchmarking...
INFO 10-02 19:06:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:41 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:43 metrics.py:335] Avg prompt throughput: 324.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 19 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%, Interval(ms): 1578.1
INFO 10-02 19:06:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1550.5674 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 19:06:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:43 metrics.py:335] Avg prompt throughput: 3740.9 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 19 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%, Interval(ms): 136.9
INFO 10-02 19:06:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 136.5993 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 19:06:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:43 metrics.py:335] Avg prompt throughput: 7560.2 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 67.6
INFO 10-02 19:06:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 67.4124 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 19:06:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:43 metrics.py:335] Avg prompt throughput: 6818.4 tokens/s, Avg generation throughput: 13.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 74.9
INFO 10-02 19:06:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 74.7354 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 19:06:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:43 metrics.py:335] Avg prompt throughput: 6398.9 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 17 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 79.9
INFO 10-02 19:06:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 79.6938 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 19:06:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:43 metrics.py:335] Avg prompt throughput: 6885.0 tokens/s, Avg generation throughput: 27.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 17 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 74.1
INFO 10-02 19:06:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 73.9043 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 19:06:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:43 metrics.py:335] Avg prompt throughput: 6426.2 tokens/s, Avg generation throughput: 37.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%, Interval(ms): 79.4
INFO 10-02 19:06:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 79.1924 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 19:06:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:43 metrics.py:335] Avg prompt throughput: 6811.1 tokens/s, Avg generation throughput: 40.1 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 25.1%, CPU KV cache usage: 0.0%, Interval(ms): 74.7
INFO 10-02 19:06:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 74.5671 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 19:06:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:43 metrics.py:335] Avg prompt throughput: 6366.7 tokens/s, Avg generation throughput: 50.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 31.4%, CPU KV cache usage: 0.0%, Interval(ms): 79.9
INFO 10-02 19:06:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 79.7851 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:43 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:44 metrics.py:335] Avg prompt throughput: 6766.0 tokens/s, Avg generation throughput: 53.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 31.5%, CPU KV cache usage: 0.0%, Interval(ms): 75.1
INFO 10-02 19:06:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 74.9154 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:44 metrics.py:335] Avg prompt throughput: 6316.4 tokens/s, Avg generation throughput: 62.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 37.7%, CPU KV cache usage: 0.0%, Interval(ms): 80.4
INFO 10-02 19:06:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 80.2169 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:44 metrics.py:335] Avg prompt throughput: 6702.7 tokens/s, Avg generation throughput: 66.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 37.8%, CPU KV cache usage: 0.0%, Interval(ms): 75.6
INFO 10-02 19:06:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 75.4693 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:44 metrics.py:335] Avg prompt throughput: 6269.0 tokens/s, Avg generation throughput: 74.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 44.0%, CPU KV cache usage: 0.0%, Interval(ms): 80.9
INFO 10-02 19:06:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 80.7056 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 19:06:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:44 metrics.py:335] Avg prompt throughput: 6646.6 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 44.1%, CPU KV cache usage: 0.0%, Interval(ms): 76.1
INFO 10-02 19:06:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 75.9490 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 19:06:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:44 metrics.py:335] Avg prompt throughput: 6217.6 tokens/s, Avg generation throughput: 86.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 50.3%, CPU KV cache usage: 0.0%, Interval(ms): 81.4
INFO 10-02 19:06:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 81.2085 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 19:06:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:44 metrics.py:335] Avg prompt throughput: 6585.4 tokens/s, Avg generation throughput: 91.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 50.4%, CPU KV cache usage: 0.0%, Interval(ms): 76.7
INFO 10-02 19:06:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.5049 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 19:06:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:44 metrics.py:335] Avg prompt throughput: 6159.3 tokens/s, Avg generation throughput: 97.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 56.6%, CPU KV cache usage: 0.0%, Interval(ms): 82.0
INFO 10-02 19:06:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 81.8105 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 19:06:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:44 metrics.py:335] Avg prompt throughput: 6538.1 tokens/s, Avg generation throughput: 103.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 56.7%, CPU KV cache usage: 0.0%, Interval(ms): 77.1
INFO 10-02 19:06:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.9022 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 19:06:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:44 metrics.py:335] Avg prompt throughput: 6113.2 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 63.0%, CPU KV cache usage: 0.0%, Interval(ms): 82.4
INFO 10-02 19:06:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 82.2654 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 19:06:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:44 metrics.py:335] Avg prompt throughput: 6464.6 tokens/s, Avg generation throughput: 115.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 63.1%, CPU KV cache usage: 0.0%, Interval(ms): 77.8
INFO 10-02 19:06:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.6205 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 19:06:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:44 metrics.py:335] Avg prompt throughput: 6051.6 tokens/s, Avg generation throughput: 120.3 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 69.3%, CPU KV cache usage: 0.0%, Interval(ms): 83.1
INFO 10-02 19:06:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 82.9329 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 19:06:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:44 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 6419.6 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 69.5%, CPU KV cache usage: 0.0%, Interval(ms): 78.2
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 78.0036 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 6003.6 tokens/s, Avg generation throughput: 131.6 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 75.7%, CPU KV cache usage: 0.0%, Interval(ms): 83.6
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 83.4291 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 6256.4 tokens/s, Avg generation throughput: 137.4 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 75.9%, CPU KV cache usage: 0.0%, Interval(ms): 80.1
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 79.8790 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 5944.6 tokens/s, Avg generation throughput: 142.4 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 82.1%, CPU KV cache usage: 0.0%, Interval(ms): 84.3
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 84.0828 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 6301.7 tokens/s, Avg generation throughput: 151.2 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 82.3%, CPU KV cache usage: 0.0%, Interval(ms): 79.3
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 79.1423 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 5966.3 tokens/s, Avg generation throughput: 155.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 88.5%, CPU KV cache usage: 0.0%, Interval(ms): 83.8
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 83.6065 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 6260.7 tokens/s, Avg generation throughput: 163.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 88.7%, CPU KV cache usage: 0.0%, Interval(ms): 79.7
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 79.5004 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:45 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=52, required_blocks=64, threshold=10
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 5928.9 tokens/s, Avg generation throughput: 166.3 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.0%, CPU KV cache usage: 0.0%, Interval(ms): 84.2
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 83.9636 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:45 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=50, required_blocks=64, threshold=10
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 6237.4 tokens/s, Avg generation throughput: 175.3 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.1%, CPU KV cache usage: 0.0%, Interval(ms): 79.8
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 79.6356 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:45 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=50, required_blocks=64, threshold=10
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([211]), positions.shape=torch.Size([211])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 3674.5 tokens/s, Avg generation throughput: 279.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.1%, CPU KV cache usage: 0.0%, Interval(ms): 53.6
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.4053 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:45 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=48, required_blocks=64, threshold=10
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 427.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9033 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:45 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=48, required_blocks=64, threshold=10
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 427.3 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8992 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:45 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=47, required_blocks=64, threshold=10
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 427.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8847 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:45 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=46, required_blocks=64, threshold=10
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.9 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9305 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:45 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=45, required_blocks=64, threshold=10
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 427.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9157 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:45 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=45, required_blocks=64, threshold=10
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 427.6 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8816 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:45 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=43, required_blocks=64, threshold=10
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9913 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:45 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=43, required_blocks=64, threshold=10
INFO 10-02 19:06:45 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.7 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 95.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9464 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=41, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9433 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=41, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9610 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=39, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9438 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=39, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.7 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9491 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=37, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.3 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9832 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=37, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 426.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9922 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=35, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.4 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0227 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=35, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.4 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0633 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=33, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0735 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=33, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0254 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=32, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.6 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0313 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=31, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0742 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=30, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.3 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0688 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=30, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.4 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0556 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=28, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1102 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=28, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0921 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=26, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.4 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1419 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=26, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1071 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=24, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1677 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=24, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1336 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=22, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1648 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=22, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.3 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 97.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1548 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=20, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 423.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1918 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=20, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1636 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=18, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1632 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=18, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 424.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1624 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=17, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 423.3 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2383 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=16, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 423.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2104 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:46 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=15, required_blocks=64, threshold=10
INFO 10-02 19:06:46 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.6 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2888 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=15, required_blocks=64, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 423.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2104 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=13, required_blocks=64, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 423.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2411 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=13, required_blocks=64, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 423.6 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2128 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=11, required_blocks=64, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 423.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2232 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=11, required_blocks=64, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 423.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2194 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=9, required_blocks=64, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.9 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2633 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=9, required_blocks=64, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.9 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2652 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=7, required_blocks=64, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2798 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=7, required_blocks=64, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2941 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=5, required_blocks=64, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.3 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.3189 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=5, required_blocks=64, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.7 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2829 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=3, required_blocks=64, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.4 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.3057 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=3, required_blocks=64, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.4 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.3050 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=2, required_blocks=64, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 421.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.6
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.3274 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=1, required_blocks=64, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 422.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 99.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.3062 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:320] BlockSpaceManagerV1: number of sequences > free gpu blocks, num_seqs=1, free_gpu_blocks=0
INFO 10-02 19:06:47 scheduler.py:451] Scheduler preempt the lowest-priority sequence groups. Preempted mode: PreemptionMode.RECOMPUTE
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=68, required_blocks=64, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 398.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 93.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9100 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=68, required_blocks=68, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.2 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 93.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.7803 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=66, required_blocks=68, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 93.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.7946 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=66, required_blocks=68, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 93.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.7779 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=64, required_blocks=68, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.5 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 93.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8392 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=64, required_blocks=68, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 93.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.7881 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=62, required_blocks=68, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.5 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8465 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=62, required_blocks=68, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8139 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=60, required_blocks=68, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 398.5 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8947 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=60, required_blocks=68, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8539 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=58, required_blocks=68, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8830 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=58, required_blocks=68, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.2 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8718 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:47 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=57, required_blocks=68, threshold=10
INFO 10-02 19:06:47 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8864 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=57, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 398.3 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9481 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=56, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 398.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.6%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9267 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=55, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0053 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=54, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 398.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9727 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=54, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 398.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 94.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9607 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=52, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0056 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=52, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2104 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=50, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 398.2 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9624 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=50, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 398.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9648 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=48, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9882 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=48, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.3 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0418 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=46, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.9879 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=46, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0041 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=44, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0788 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=44, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0568 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=43, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0637 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=43, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.8%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0804 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=42, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 95.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0766 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=41, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.0%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0635 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=40, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 397.3 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.2
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0394 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=40, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0842 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=38, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1238 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=38, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.3%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.0838 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=36, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1686 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=36, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.5%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1202 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=34, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.3
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1417 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=34, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.7%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1498 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:48 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=32, required_blocks=68, threshold=10
INFO 10-02 19:06:48 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1593 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:49 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=32, required_blocks=68, threshold=10
INFO 10-02 19:06:49 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 96.9%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1682 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:49 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=30, required_blocks=68, threshold=10
INFO 10-02 19:06:49 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 97.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1784 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:49 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=30, required_blocks=68, threshold=10
INFO 10-02 19:06:49 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 396.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 97.1%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1582 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:49 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=29, required_blocks=68, threshold=10
INFO 10-02 19:06:49 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 394.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 97.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 10-02 19:06:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.2466 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:49 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=29, required_blocks=68, threshold=10
INFO 10-02 19:06:49 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14])
INFO 10-02 19:06:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.6 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 90.2%, CPU KV cache usage: 0.0%, Interval(ms): 35.4
INFO 10-02 19:06:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.1906 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:49 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=32, required_blocks=64, threshold=10
INFO 10-02 19:06:49 scheduler.py:1146] A RECOMPUTE sequence is resumed. recomputed_tokens = 1076, tokens = 0
INFO 10-02 19:06:49 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:49 sequence.py:503] First token: Recomputed SeqGroup 15 recomputing 499 tokens take 0.075721 seconds. parallel_reqs = 1 parallel_ids=[15] seqs_dict = {15: Sequence(seq_id=15, status=RUNNING, num_blocks=68)}
INFO 10-02 19:06:49 metrics.py:335] Avg prompt throughput: 6517.0 tokens/s, Avg generation throughput: 169.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.9%, CPU KV cache usage: 0.0%, Interval(ms): 76.6
INFO 10-02 19:06:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.3023 resumed_reqs=1, running_reqs=14 raw_running=13
INFO 10-02 19:06:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:49 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=32, required_blocks=64, threshold=10
INFO 10-02 19:06:49 scheduler.py:1146] A RECOMPUTE sequence is resumed. recomputed_tokens = 1076, tokens = 499
INFO 10-02 19:06:49 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:49 sequence.py:503] First token: Recomputed SeqGroup 15 recomputing 499 tokens take 0.081702 seconds. parallel_reqs = 1 parallel_ids=[15] seqs_dict = {15: Sequence(seq_id=15, status=RUNNING, num_blocks=68)}
INFO 10-02 19:06:49 metrics.py:335] Avg prompt throughput: 6051.8 tokens/s, Avg generation throughput: 157.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 96.9%, CPU KV cache usage: 0.0%, Interval(ms): 82.5
INFO 10-02 19:06:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 82.2542 resumed_reqs=1, running_reqs=14 raw_running=13
INFO 10-02 19:06:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:49 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=31, required_blocks=64, threshold=10
INFO 10-02 19:06:49 scheduler.py:1146] A RECOMPUTE sequence is resumed. recomputed_tokens = 1076, tokens = 998
INFO 10-02 19:06:49 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([92]), positions.shape=torch.Size([92])
INFO 10-02 19:06:49 sequence.py:488] First token: Recomputed SeqGroup 15 recomputing 79 tokens take 0.046697 seconds. parallel_reqs = 1 parallel_ids=[15] seqs_dict = {15: Sequence(seq_id=15, status=RUNNING, num_blocks=68)}
INFO 10-02 19:06:49 metrics.py:335] Avg prompt throughput: 1664.2 tokens/s, Avg generation throughput: 294.9 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 90.0%, CPU KV cache usage: 0.0%, Interval(ms): 47.5
INFO 10-02 19:06:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.2825 resumed_reqs=1, running_reqs=14 raw_running=13
INFO 10-02 19:06:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:49 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=39, required_blocks=64, threshold=10
INFO 10-02 19:06:49 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:49 metrics.py:335] Avg prompt throughput: 6549.8 tokens/s, Avg generation throughput: 170.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 96.2%, CPU KV cache usage: 0.0%, Interval(ms): 76.2
INFO 10-02 19:06:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 75.9399 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:49 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=38, required_blocks=64, threshold=10
INFO 10-02 19:06:49 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:49 metrics.py:335] Avg prompt throughput: 6063.0 tokens/s, Avg generation throughput: 158.0 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 89.3%, CPU KV cache usage: 0.0%, Interval(ms): 82.3
INFO 10-02 19:06:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 82.1002 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:49 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=46, required_blocks=64, threshold=10
INFO 10-02 19:06:49 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:49 metrics.py:335] Avg prompt throughput: 5747.9 tokens/s, Avg generation throughput: 149.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 95.5%, CPU KV cache usage: 0.0%, Interval(ms): 87.0
INFO 10-02 19:06:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.7338 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:49 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=44, required_blocks=64, threshold=10
INFO 10-02 19:06:49 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:49 metrics.py:335] Avg prompt throughput: 6073.9 tokens/s, Avg generation throughput: 158.2 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 88.7%, CPU KV cache usage: 0.0%, Interval(ms): 82.2
INFO 10-02 19:06:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 81.9478 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:49 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=52, required_blocks=64, threshold=10
INFO 10-02 19:06:49 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:49 metrics.py:335] Avg prompt throughput: 5754.6 tokens/s, Avg generation throughput: 149.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 95.0%, CPU KV cache usage: 0.0%, Interval(ms): 86.9
INFO 10-02 19:06:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.6387 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:49 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=50, required_blocks=64, threshold=10
INFO 10-02 19:06:49 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:49 metrics.py:335] Avg prompt throughput: 6084.3 tokens/s, Avg generation throughput: 158.5 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 88.2%, CPU KV cache usage: 0.0%, Interval(ms): 82.0
INFO 10-02 19:06:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 81.8093 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:49 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=58, required_blocks=64, threshold=10
INFO 10-02 19:06:49 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:49 metrics.py:335] Avg prompt throughput: 5762.4 tokens/s, Avg generation throughput: 149.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 94.4%, CPU KV cache usage: 0.0%, Interval(ms): 86.8
INFO 10-02 19:06:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.5114 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
WARNING 10-02 19:06:49 block_manager_v1.py:277] BlockSpaceManagerV1: free blocks will less than the threshold if allocated, current_free_gpu_blocks=56, required_blocks=64, threshold=10
INFO 10-02 19:06:49 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 6099.2 tokens/s, Avg generation throughput: 158.9 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.6%, CPU KV cache usage: 0.0%, Interval(ms): 81.8
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 81.6104 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 5770.5 tokens/s, Avg generation throughput: 150.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 93.8%, CPU KV cache usage: 0.0%, Interval(ms): 86.6
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.3941 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 6122.1 tokens/s, Avg generation throughput: 159.5 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 87.0%, CPU KV cache usage: 0.0%, Interval(ms): 81.5
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 81.3043 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([138]), positions.shape=torch.Size([138])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 2521.3 tokens/s, Avg generation throughput: 260.1 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 87.0%, CPU KV cache usage: 0.0%, Interval(ms): 50.0
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.6829 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 375.1 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 80.2%, CPU KV cache usage: 0.0%, Interval(ms): 34.7
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.4603 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.0 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 80.2%, CPU KV cache usage: 0.0%, Interval(ms): 34.3
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.0323 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.2 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 73.2%, CPU KV cache usage: 0.0%, Interval(ms): 34.3
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.0731 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 326.0 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 73.2%, CPU KV cache usage: 0.0%, Interval(ms): 33.7
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 33.4990 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 326.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 66.2%, CPU KV cache usage: 0.0%, Interval(ms): 33.7
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 33.4620 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 314.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 66.2%, CPU KV cache usage: 0.0%, Interval(ms): 31.8
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 31.5659 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 314.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 59.2%, CPU KV cache usage: 0.0%, Interval(ms): 31.8
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 31.6091 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 289.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 59.2%, CPU KV cache usage: 0.0%, Interval(ms): 31.1
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 30.8459 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 289.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 52.3%, CPU KV cache usage: 0.0%, Interval(ms): 31.1
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 30.8752 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 263.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 52.3%, CPU KV cache usage: 0.0%, Interval(ms): 30.4
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 30.1406 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 264.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 30.3
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 30.1046 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 236.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.4%, CPU KV cache usage: 0.0%, Interval(ms): 29.6
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 29.4237 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 236.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.5%, CPU KV cache usage: 0.0%, Interval(ms): 29.6
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 29.4616 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.5%, CPU KV cache usage: 0.0%, Interval(ms): 28.7
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.4858 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.6%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0838 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.6%, CPU KV cache usage: 0.0%, Interval(ms): 28.2
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0662 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.8%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1005 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.8%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0950 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.8%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.0902 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.8%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1141 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.8%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1148 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.8%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1146 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.8%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1096 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.8%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1291 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:50 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.9%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1117 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 38.9%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1131 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1179 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1274 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.1%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1489 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.1%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1234 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.2%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1322 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.2%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1353 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1575 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1706 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1401 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1773 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1496 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1692 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1546 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1894 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.5%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1508 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.5%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1448 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.6%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1343 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 212.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.6%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1315 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.7%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1703 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.7%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1515 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.8%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.2047 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.8%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1577 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1847 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1591 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1880 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1758 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1832 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.2018 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1918 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.0%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1622 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.1%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.2004 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.1%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1746 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.2%, CPU KV cache usage: 0.0%, Interval(ms): 28.3
INFO 10-02 19:06:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1804 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:51 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.2%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.2040 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.3%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.2416 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 210.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.3%, CPU KV cache usage: 0.0%, Interval(ms): 28.5
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.3041 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.2531 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.4%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4720 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5061 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4951 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5176 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4959 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 181.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.6
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4656 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.6
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4744 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.6
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4642 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4880 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.6%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5002 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.6%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5006 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5085 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 27.6
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.4856 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5288 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5178 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5207 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5171 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5607 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5331 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5331 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5385 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5397 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5195 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5126 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5171 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.1%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5230 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.1%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5190 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.2%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5548 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.2%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5309 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.3%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5469 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.3%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5645 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.4%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5731 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:52 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.4%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5481 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5533 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5698 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5607 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5817 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6010 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5624 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5490 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.5%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5717 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.6%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5733 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.6%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5912 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.7%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6201 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.7%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6623 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.8%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6105 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.8%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6299 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.9%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6244 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.9%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6051 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6194 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6117 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6644 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6418 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6253 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 35.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6191 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.9 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6260 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 147.1 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.2
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 26.9721 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 147.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 27.1
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 26.9785 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 26.6
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 26.4018 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.0%, CPU KV cache usage: 0.0%, Interval(ms): 26.6
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 26.4192 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.0%, CPU KV cache usage: 0.0%, Interval(ms): 25.9
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 25.7194 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.0%, CPU KV cache usage: 0.0%, Interval(ms): 25.9
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 25.7053 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 38.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.0%, CPU KV cache usage: 0.0%, Interval(ms): 25.9
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 25.7528 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 19:06:53 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-02 19:06:53 opt.py:248] before OPTModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1])
INFO 10-02 19:06:53 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 38.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 25.9
INFO 10-02 19:06:53 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 25.7723 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.7506726, last_token_time=1727896009.1780002, first_scheduled_time=1727896001.7683904, first_token_time=1727896003.4554365, time_in_queue=0.017717838287353516, finished_time=1727896009.17794, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.753964, last_token_time=1727896009.3844953, first_scheduled_time=1727896003.45585, first_token_time=1727896003.6778345, time_in_queue=1.7018859386444092, finished_time=1727896009.3844392, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.7547717, last_token_time=1727896009.5429838, first_scheduled_time=1727896003.598376, first_token_time=1727896003.8312569, time_in_queue=1.843604326248169, finished_time=1727896009.5429308, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.7556806, last_token_time=1727896009.7121265, first_scheduled_time=1727896003.7523098, first_token_time=1727896003.985916, time_in_queue=1.996629238128662, finished_time=1727896009.7120726, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.756421, last_token_time=1727896009.881029, first_scheduled_time=1727896003.9063995, first_token_time=1727896004.1413994, time_in_queue=2.1499783992767334, finished_time=1727896009.8809743, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.7571487, last_token_time=1727896010.0496128, first_scheduled_time=1727896004.0614738, first_token_time=1727896004.2978895, time_in_queue=2.3043251037597656, finished_time=1727896010.0495608, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.7578669, last_token_time=1727896010.2177684, first_scheduled_time=1727896004.2175057, first_token_time=1727896004.4553826, time_in_queue=2.459638833999634, finished_time=1727896010.2177167, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.758593, last_token_time=1727896010.3024056, first_scheduled_time=1727896004.3745196, first_token_time=1727896004.614035, time_in_queue=2.615926504135132, finished_time=1727896010.3023517, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.7593677, last_token_time=1727896010.370959, first_scheduled_time=1727896004.5325966, first_token_time=1727896004.773552, time_in_queue=2.773228883743286, finished_time=1727896010.3709104, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.760081, last_token_time=1727896010.4383504, first_scheduled_time=1727896004.691678, first_token_time=1727896004.9344532, time_in_queue=2.9315969944000244, finished_time=1727896010.4383054, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.7607841, last_token_time=1727896010.5019886, first_scheduled_time=1727896004.8519447, first_token_time=1727896005.0962517, time_in_queue=3.091160535812378, finished_time=1727896010.5019472, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.7615001, last_token_time=1727896010.5641234, first_scheduled_time=1727896005.0132694, first_token_time=1727896005.260588, time_in_queue=3.2517693042755127, finished_time=1727896010.564077, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.7622173, last_token_time=1727896010.6247962, first_scheduled_time=1727896005.1769717, first_token_time=1727896005.4237168, time_in_queue=3.4147543907165527, finished_time=1727896010.624764, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.7629967, last_token_time=1727896010.6840832, first_scheduled_time=1727896005.3406022, first_token_time=1727896005.58757, time_in_queue=3.5776054859161377, finished_time=1727896010.684054, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RECOMPUTE, finished=True, metrics=RequestMetrics(arrival_time=1727896001.763724, last_token_time=1727896012.1008182, first_scheduled_time=1727896005.504147, first_token_time=1727896005.7209961, time_in_queue=3.7404229640960693, finished_time=1727896012.1007903, rescheduled_time=1727896009.3373744, reget_first_token_time=1727896009.3840716))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.7644467, last_token_time=1727896013.6532245, first_scheduled_time=1727896009.3849227, first_token_time=1727896009.6296175, time_in_queue=7.620476007461548, finished_time=1727896013.6532013, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.7651682, last_token_time=1727896013.7075539, first_scheduled_time=1727896009.54342, first_token_time=1727896009.7986703, time_in_queue=7.778251886367798, finished_time=1727896013.707535, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.7658887, last_token_time=1727896013.7607718, first_scheduled_time=1727896009.712594, first_token_time=1727896009.9674525, time_in_queue=7.946705341339111, finished_time=1727896013.7607589, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.7666588, last_token_time=1727896013.8125477, first_scheduled_time=1727896009.8814769, first_token_time=1727896010.1359134, time_in_queue=8.114818096160889, finished_time=1727896013.8125396, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727896001.7673903, last_token_time=1727896013.8644204, first_scheduled_time=1727896010.0500207, first_token_time=1727896010.267414, time_in_queue=8.282630443572998, finished_time=1727896013.864418, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 23040
End-to-End latency: 12.11 seconds
Throughput: 1.65 requests/s, 1901.94 tokens/s
Per_token_time: 0.526 ms

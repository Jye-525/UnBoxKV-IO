Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='fixed', input_len=1024, output_len=128, min_len=10, max_len=50, prefill_to_decode_ratio=6.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=2048, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=2048, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Selected required requests 20
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
User Requests: prompt_len=1024, output_len=128, sequence_len=1152...
Running vLLM with default batching strategy. max_num_batched_tokens=2048, max_num_batched_seqs=256
rope_scaling = {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}
INFO 10-02 18:31:49 config.py:654] [SchedulerConfig] max_num_batched_tokens: 2048 chunked_prefill_enabled: False
INFO 10-02 18:31:49 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/meta-llama-3.1-8b, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
/home/jieye/viper2/vllm_v0_4_2_mine/vllm/executor/gpu_executor.py:36: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 10-02 18:31:49 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-02 18:31:54 selector.py:27] Using FlashAttention-2 backend.
INFO 10-02 18:32:11 model_runner.py:180] Loading model weights took 14.9888 GB
INFO 10-02 18:32:11 model_runner.py:183] Loaded model: 
INFO 10-02 18:32:11 model_runner.py:183]  LlamaForCausalLM(
INFO 10-02 18:32:11 model_runner.py:183]   (model): LlamaModel(
INFO 10-02 18:32:11 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=128256, embedding_dim=4096, org_vocab_size=128256, num_embeddings_padded=128256, tp_size=1)
INFO 10-02 18:32:11 model_runner.py:183]     (layers): ModuleList(
INFO 10-02 18:32:11 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-02 18:32:11 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-02 18:32:11 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:32:11 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:32:11 model_runner.py:183]           (rotary_emb): Llama3RotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=131072, base=500000.0, is_neox_style=True)
INFO 10-02 18:32:11 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845)
INFO 10-02 18:32:11 model_runner.py:183]         )
INFO 10-02 18:32:11 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-02 18:32:11 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=28672, bias=False, tp_size=1, gather_output=False)
INFO 10-02 18:32:11 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=14336, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-02 18:32:11 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-02 18:32:11 model_runner.py:183]         )
INFO 10-02 18:32:11 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:32:11 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:32:11 model_runner.py:183]       )
INFO 10-02 18:32:11 model_runner.py:183]     )
INFO 10-02 18:32:11 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-02 18:32:11 model_runner.py:183]   )
INFO 10-02 18:32:11 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=128256, embedding_dim=4096, org_vocab_size=128256, num_embeddings_padded=128256, tp_size=1)
INFO 10-02 18:32:11 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=128256, forg_vocab_size=128256, scale=1.0, logits_as_input=False)
INFO 10-02 18:32:11 model_runner.py:183]   (sampler): Sampler()
INFO 10-02 18:32:11 model_runner.py:183] )
INFO 10-02 18:32:12 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:32:12 worker.py:164] Peak: 16.531 GB, Initial: 38.980 GB, Free: 22.448 GB, Total: 39.394 GB,               cache_block_size: 2097152 Bytes, available GPU for KV cache: 20.893 GB
INFO 10-02 18:32:12 gpu_executor.py:117] # GPU blocks: 10697, # CPU blocks: 32768
INFO 10-02 18:32:43 worker.py:189] _init_cache_engine took 20.8926 GB
INFO 10-02 18:32:43 scheduler.py:307] Scheduler initialized with prompt limit: 2048
INFO 10-02 18:32:43 Start warmup...
INFO 10-02 18:32:43 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:43 metrics.py:335] Avg prompt throughput: 644.0 tokens/s, Avg generation throughput: 32.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 31.1
INFO 10-02 18:32:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6421 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:32:43 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-02 18:32:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 54.1
INFO 10-02 18:32:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.6709 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-02 18:32:48 Start benchmarking...
INFO 10-02 18:32:48 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:32:48 metrics.py:335] Avg prompt throughput: 10971.4 tokens/s, Avg generation throughput: 10.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%, Interval(ms): 186.7
INFO 10-02 18:32:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 159.6606 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-02 18:32:48 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:32:49 metrics.py:335] Avg prompt throughput: 13819.7 tokens/s, Avg generation throughput: 13.5 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%, Interval(ms): 148.2
INFO 10-02 18:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 147.9766 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-02 18:32:49 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:32:49 metrics.py:335] Avg prompt throughput: 15889.8 tokens/s, Avg generation throughput: 15.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%, Interval(ms): 128.9
INFO 10-02 18:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 128.7143 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-02 18:32:49 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:32:49 metrics.py:335] Avg prompt throughput: 15841.8 tokens/s, Avg generation throughput: 15.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%, Interval(ms): 129.3
INFO 10-02 18:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 129.1111 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-02 18:32:49 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:32:49 metrics.py:335] Avg prompt throughput: 15866.8 tokens/s, Avg generation throughput: 15.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%, Interval(ms): 129.1
INFO 10-02 18:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 128.8722 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-02 18:32:49 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:32:49 metrics.py:335] Avg prompt throughput: 15768.2 tokens/s, Avg generation throughput: 15.4 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 7.2%, CPU KV cache usage: 0.0%, Interval(ms): 129.9
INFO 10-02 18:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 129.7181 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-02 18:32:49 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:32:49 metrics.py:335] Avg prompt throughput: 15797.8 tokens/s, Avg generation throughput: 15.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 8.4%, CPU KV cache usage: 0.0%, Interval(ms): 129.6
INFO 10-02 18:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 129.4763 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-02 18:32:49 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:32:49 metrics.py:335] Avg prompt throughput: 15881.6 tokens/s, Avg generation throughput: 15.5 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 9.6%, CPU KV cache usage: 0.0%, Interval(ms): 129.0
INFO 10-02 18:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 128.7985 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-02 18:32:49 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:32:49 metrics.py:335] Avg prompt throughput: 15866.8 tokens/s, Avg generation throughput: 15.5 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 10.8%, CPU KV cache usage: 0.0%, Interval(ms): 129.1
INFO 10-02 18:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 128.9194 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-02 18:32:49 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 15749.0 tokens/s, Avg generation throughput: 15.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.0%, CPU KV cache usage: 0.0%, Interval(ms): 130.0
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 129.8890 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0147 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8452 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 955.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7264 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 956.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7040 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 955.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7238 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7303 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 958.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6432 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 960.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6122 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 959.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6265 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 957.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6347 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 958.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6637 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 955.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7298 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 955.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7090 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 959.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6435 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 957.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6671 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 956.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6954 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7283 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 955.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7229 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 935.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1620 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 956.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6821 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 956.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6926 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 957.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6816 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 957.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6594 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7605 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 957.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6828 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 956.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6933 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 957.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6683 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 957.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6637 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 955.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7069 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 955.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7081 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 955.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7219 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7407 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8218 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7498 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7293 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7338 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7362 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7276 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7286 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7937 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7520 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7322 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7653 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7746 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:50 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7632 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7915 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7899 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7582 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7849 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7434 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7448 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7765 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7741 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7798 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7939 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 955.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7319 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7651 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7551 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7443 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 954.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7326 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8046 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7565 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 953.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7627 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 952.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7784 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9250 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9236 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9258 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9043 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8673 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9146 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 949.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8578 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9017 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9098 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9162 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9150 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9630 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9296 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9184 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9024 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9181 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9925 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9427 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0140 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9379 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9472 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9591 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9737 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9599 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9982 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0018 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9401 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:51 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9811 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9403 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9262 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9267 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9644 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0207 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0052 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0109 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0195 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 938.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0524 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0273 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0080 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9951 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9894 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0476 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0154 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0183 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0557 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0254 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9849 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0435 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9646 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9584 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9703 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9758 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0314 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9634 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0080 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 942.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9782 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9799 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9539 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9892 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 934.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1973 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0159 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9794 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-02 18:32:52 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-02 18:32:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 927.9 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 21.6
INFO 10-02 18:32:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.3032 resumed_reqs=0, running_reqs=20 raw_running=20
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7119248, last_token_time=1727893972.7484596, first_scheduled_time=1727893968.7289805, first_token_time=1727893968.888357, time_in_queue=0.017055749893188477, finished_time=1727893972.7483797, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7149115, last_token_time=1727893972.7484596, first_scheduled_time=1727893968.7289805, first_token_time=1727893968.888357, time_in_queue=0.014069080352783203, finished_time=1727893972.7483861, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7157335, last_token_time=1727893972.7484596, first_scheduled_time=1727893968.8888135, first_token_time=1727893969.0365803, time_in_queue=0.1730799674987793, finished_time=1727893972.7483912, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7164686, last_token_time=1727893972.7484596, first_scheduled_time=1727893968.8888135, first_token_time=1727893969.0365803, time_in_queue=0.17234492301940918, finished_time=1727893972.7483957, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7171824, last_token_time=1727893972.7484596, first_scheduled_time=1727893969.036966, first_token_time=1727893969.1654587, time_in_queue=0.31978368759155273, finished_time=1727893972.7484, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7179065, last_token_time=1727893972.7484596, first_scheduled_time=1727893969.036966, first_token_time=1727893969.1654587, time_in_queue=0.3190596103668213, finished_time=1727893972.748404, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7186806, last_token_time=1727893972.7484596, first_scheduled_time=1727893969.1658401, first_token_time=1727893969.2947538, time_in_queue=0.4471595287322998, finished_time=1727893972.748408, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7194042, last_token_time=1727893972.7484596, first_scheduled_time=1727893969.1658401, first_token_time=1727893969.2947538, time_in_queue=0.44643592834472656, finished_time=1727893972.7484121, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7201185, last_token_time=1727893972.7484596, first_scheduled_time=1727893969.2951593, first_token_time=1727893969.4238315, time_in_queue=0.5750408172607422, finished_time=1727893972.748416, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7208278, last_token_time=1727893972.7484596, first_scheduled_time=1727893969.2951593, first_token_time=1727893969.4238315, time_in_queue=0.574331521987915, finished_time=1727893972.7484198, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7215374, last_token_time=1727893972.7484596, first_scheduled_time=1727893969.424199, first_token_time=1727893969.5537133, time_in_queue=0.7026617527008057, finished_time=1727893972.748424, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7223117, last_token_time=1727893972.7484596, first_scheduled_time=1727893969.424199, first_token_time=1727893969.5537133, time_in_queue=0.7018873691558838, finished_time=1727893972.7484276, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7230241, last_token_time=1727893972.7484596, first_scheduled_time=1727893969.5540698, first_token_time=1727893969.68335, time_in_queue=0.8310456275939941, finished_time=1727893972.7484314, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7237313, last_token_time=1727893972.7484596, first_scheduled_time=1727893969.5540698, first_token_time=1727893969.68335, time_in_queue=0.8303384780883789, finished_time=1727893972.748435, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7244306, last_token_time=1727893972.7484596, first_scheduled_time=1727893969.683703, first_token_time=1727893969.8123026, time_in_queue=0.9592723846435547, finished_time=1727893972.7484386, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.725133, last_token_time=1727893972.7484596, first_scheduled_time=1727893969.683703, first_token_time=1727893969.8123026, time_in_queue=0.9585700035095215, finished_time=1727893972.7484424, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7258995, last_token_time=1727893972.7484596, first_scheduled_time=1727893969.81266, first_token_time=1727893969.941373, time_in_queue=1.0867605209350586, finished_time=1727893972.7484462, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.726631, last_token_time=1727893972.7484596, first_scheduled_time=1727893969.81266, first_token_time=1727893969.941373, time_in_queue=1.086029052734375, finished_time=1727893972.74845, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.7273815, last_token_time=1727893972.7484596, first_scheduled_time=1727893969.941734, first_token_time=1727893970.0714161, time_in_queue=1.2143526077270508, finished_time=1727893972.7484539, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727893968.728095, last_token_time=1727893972.7484596, first_scheduled_time=1727893969.941734, first_token_time=1727893970.0714161, time_in_queue=1.2136390209197998, finished_time=1727893972.7484577, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 23040
End-to-End latency: 4.04 seconds
Throughput: 4.95 requests/s, 5706.72 tokens/s
Per_token_time: 0.175 ms

Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=4096, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=4096, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=2048, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=1585, output_len=80, sequence_len=1665...
User Requests: prompt_len=970, output_len=49, sequence_len=1019...
User Requests: prompt_len=2398, output_len=120, sequence_len=2518...
User Requests: prompt_len=2366, output_len=119, sequence_len=2485...
User Requests: prompt_len=652, output_len=33, sequence_len=685...
User Requests: prompt_len=658, output_len=33, sequence_len=691...
User Requests: prompt_len=2495, output_len=125, sequence_len=2620...
User Requests: prompt_len=3566, output_len=179, sequence_len=3745...
User Requests: prompt_len=2969, output_len=149, sequence_len=3118...
Selected required requests 10
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=1585, output_len=80, sequence_len=1665...
User Requests: prompt_len=970, output_len=49, sequence_len=1019...
User Requests: prompt_len=2398, output_len=120, sequence_len=2518...
User Requests: prompt_len=2366, output_len=119, sequence_len=2485...
User Requests: prompt_len=652, output_len=33, sequence_len=685...
User Requests: prompt_len=658, output_len=33, sequence_len=691...
User Requests: prompt_len=2495, output_len=125, sequence_len=2620...
User Requests: prompt_len=3566, output_len=179, sequence_len=3745...
User Requests: prompt_len=2969, output_len=149, sequence_len=3118...
Chunked prefill is enabled. max_num_batched_tokens=2048, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'type', 'finetuned', 'original_max_position_embeddings'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 02:39:57 config.py:654] [SchedulerConfig] max_num_batched_tokens: 2048 chunked_prefill_enabled: True
INFO 10-04 02:39:57 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 02:39:58 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 02:40:02 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 02:40:06 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 02:40:06 model_runner.py:183] Loaded model: 
INFO 10-04 02:40:06 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 02:40:06 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 02:40:06 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 02:40:06 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 02:40:06 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 02:40:06 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 02:40:06 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 02:40:06 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 02:40:06 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 02:40:06 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 02:40:06 model_runner.py:183]         )
INFO 10-04 02:40:06 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 02:40:06 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 02:40:06 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 02:40:06 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 02:40:06 model_runner.py:183]         )
INFO 10-04 02:40:06 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:40:06 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:40:06 model_runner.py:183]       )
INFO 10-04 02:40:06 model_runner.py:183]     )
INFO 10-04 02:40:06 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:40:06 model_runner.py:183]   )
INFO 10-04 02:40:06 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 02:40:06 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 02:40:06 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 02:40:06 model_runner.py:183] )
INFO 10-04 02:40:06 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 02:40:07 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 8.1205, fwd=375.1712 cmp_logits=0.2394, sampling=155.0112, total=538.5439
INFO 10-04 02:40:07 worker.py:164] Peak: 13.332 GB, Initial: 38.980 GB, Free: 25.648 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.092 GB
INFO 10-04 02:40:07 gpu_executor.py:117] # GPU blocks: 3083, # CPU blocks: 8192
INFO 10-04 02:40:38 worker.py:189] _init_cache_engine took 24.0859 GB
INFO 10-04 02:40:38 scheduler.py:307] Scheduler initialized with prompt limit: 4096
INFO 10-04 02:40:38 Start warmup...
INFO 10-04 02:40:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:40:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8731, fwd=338.5439 cmp_logits=71.3372, sampling=0.9646, total=411.7208
INFO 10-04 02:40:38 metrics.py:335] Avg prompt throughput: 2446.6 tokens/s, Avg generation throughput: 2.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 418.5
INFO 10-04 02:40:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 412.2355 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2961, fwd=42.9566 cmp_logits=0.1240, sampling=6.6791, total=50.0576
INFO 10-04 02:40:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 51.0
INFO 10-04 02:40:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.5161 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3047, fwd=7.8154 cmp_logits=0.0701, sampling=7.3025, total=15.4946
INFO 10-04 02:40:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 02:40:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7845 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2701, fwd=7.7982 cmp_logits=0.0679, sampling=7.3018, total=15.4397
INFO 10-04 02:40:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:40:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6648 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2677, fwd=7.7343 cmp_logits=0.0677, sampling=7.3478, total=15.4188
INFO 10-04 02:40:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:40:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6336 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2644, fwd=7.7899 cmp_logits=0.0675, sampling=6.1500, total=14.2732
INFO 10-04 02:40:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 02:40:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4930 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2656, fwd=7.8607 cmp_logits=0.0741, sampling=6.0897, total=14.2913
INFO 10-04 02:40:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 02:40:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5082 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2608, fwd=7.8015 cmp_logits=0.0675, sampling=6.1629, total=14.2934
INFO 10-04 02:40:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 02:40:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5366 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2570, fwd=7.8731 cmp_logits=0.0672, sampling=6.0863, total=14.2846
INFO 10-04 02:40:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 02:40:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4806 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2570, fwd=7.8394 cmp_logits=0.0668, sampling=6.1018, total=14.2663
INFO 10-04 02:40:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 02:40:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4780 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:43 Start benchmarking...
INFO 10-04 02:40:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 02:40:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1516, fwd=46.8998 cmp_logits=0.0920, sampling=145.2065, total=193.3513
INFO 10-04 02:40:44 metrics.py:335] Avg prompt throughput: 8531.7 tokens/s, Avg generation throughput: 4.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 4.4%, CPU KV cache usage: 0.0%, Interval(ms): 240.0
INFO 10-04 02:40:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 193.8534 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 02:40:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.0726, fwd=14.5404 cmp_logits=0.1056, sampling=167.5839, total=183.3036
INFO 10-04 02:40:44 metrics.py:335] Avg prompt throughput: 11128.7 tokens/s, Avg generation throughput: 16.3 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 11.3%, CPU KV cache usage: 0.0%, Interval(ms): 183.9
INFO 10-04 02:40:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 183.7010 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 02:40:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 02:40:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.0517, fwd=14.3802 cmp_logits=0.0834, sampling=138.6032, total=154.1195
INFO 10-04 02:40:44 metrics.py:335] Avg prompt throughput: 13221.2 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 16.1%, CPU KV cache usage: 0.0%, Interval(ms): 154.7
INFO 10-04 02:40:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 154.5136 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 02:40:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.0705, fwd=14.5135 cmp_logits=0.0856, sampling=135.9363, total=151.6068
INFO 10-04 02:40:44 metrics.py:335] Avg prompt throughput: 13432.2 tokens/s, Avg generation throughput: 32.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 152.2
INFO 10-04 02:40:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 152.0133 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 02:40:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1480, fwd=14.4792 cmp_logits=0.0839, sampling=122.0450, total=137.7571
INFO 10-04 02:40:44 metrics.py:335] Avg prompt throughput: 14754.6 tokens/s, Avg generation throughput: 50.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 23.8%, CPU KV cache usage: 0.0%, Interval(ms): 138.5
INFO 10-04 02:40:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 138.2802 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 02:40:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 02:40:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1210, fwd=14.4074 cmp_logits=0.0961, sampling=141.2816, total=156.9071
INFO 10-04 02:40:44 metrics.py:335] Avg prompt throughput: 12946.8 tokens/s, Avg generation throughput: 50.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 31.1%, CPU KV cache usage: 0.0%, Interval(ms): 157.6
INFO 10-04 02:40:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 157.4767 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:40:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1253, fwd=14.5247 cmp_logits=0.0713, sampling=138.6800, total=154.4023
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 13149.9 tokens/s, Avg generation throughput: 51.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 31.1%, CPU KV cache usage: 0.0%, Interval(ms): 155.1
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 154.9115 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1437, fwd=14.4484 cmp_logits=0.0861, sampling=158.3936, total=174.0727
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 11669.1 tokens/s, Avg generation throughput: 51.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.2%, CPU KV cache usage: 0.0%, Interval(ms): 174.8
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 174.6528 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1884]), positions.shape=torch.Size([1884]) hidden_states.shape=torch.Size([1884, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1687, fwd=15.0125 cmp_logits=0.0877, sampling=145.0372, total=161.3066
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 11570.1 tokens/s, Avg generation throughput: 61.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.2%, CPU KV cache usage: 0.0%, Interval(ms): 162.1
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 161.8686 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4683, fwd=8.1391 cmp_logits=0.0710, sampling=12.3591, total=21.0383
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5945 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4718, fwd=7.9911 cmp_logits=0.0718, sampling=12.4338, total=20.9692
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5607 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4618, fwd=8.0245 cmp_logits=0.0708, sampling=12.4061, total=20.9644
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 461.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.4982 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4694, fwd=7.8232 cmp_logits=0.0706, sampling=12.6193, total=20.9835
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 461.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5137 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4759, fwd=7.8089 cmp_logits=0.0679, sampling=12.6562, total=21.0099
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5323 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4621, fwd=7.8831 cmp_logits=0.0672, sampling=12.5561, total=20.9692
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 461.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5058 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4914, fwd=7.8304 cmp_logits=0.0670, sampling=12.6095, total=20.9990
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5580 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4494, fwd=7.7598 cmp_logits=0.0677, sampling=12.7089, total=20.9866
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 461.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5139 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4497, fwd=7.6389 cmp_logits=0.0672, sampling=12.8057, total=20.9625
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 461.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.6
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.4775 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4468, fwd=7.8056 cmp_logits=0.0672, sampling=12.6362, total=20.9565
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 462.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.6
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.4736 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4539, fwd=7.6869 cmp_logits=0.0663, sampling=12.8028, total=21.0109
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5337 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4554, fwd=7.7901 cmp_logits=0.0665, sampling=12.6617, total=20.9746
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5292 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4635, fwd=7.6900 cmp_logits=0.0675, sampling=12.8078, total=21.0297
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5850 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4578, fwd=7.8011 cmp_logits=0.0679, sampling=12.6643, total=20.9920
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 461.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5127 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4513, fwd=7.7653 cmp_logits=0.0677, sampling=12.7122, total=20.9973
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 461.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5201 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4547, fwd=7.7763 cmp_logits=0.0684, sampling=12.6772, total=20.9768
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5268 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4563, fwd=7.6776 cmp_logits=0.0668, sampling=12.8160, total=21.0173
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5402 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4559, fwd=7.7000 cmp_logits=0.0672, sampling=12.8636, total=21.0876
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6141 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4513, fwd=7.7291 cmp_logits=0.0672, sampling=12.8157, total=21.0643
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6160 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4797, fwd=7.7903 cmp_logits=0.0758, sampling=12.7616, total=21.1082
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 458.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6477 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4332, fwd=7.8528 cmp_logits=0.0670, sampling=12.4025, total=20.7567
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2526 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4642, fwd=7.7586 cmp_logits=0.0670, sampling=12.5177, total=20.8085
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 418.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.3175 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4299, fwd=7.6647 cmp_logits=0.0672, sampling=12.5730, total=20.7357
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 420.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2364 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4320, fwd=7.6644 cmp_logits=0.0672, sampling=12.5713, total=20.7357
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 420.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2288 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:40:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4327, fwd=7.7291 cmp_logits=0.0675, sampling=12.5055, total=20.7355
INFO 10-04 02:40:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 420.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-04 02:40:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2381 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:40:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4346, fwd=7.7477 cmp_logits=0.0670, sampling=12.5139, total=20.7641
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2605 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4575, fwd=7.8063 cmp_logits=0.0675, sampling=12.4843, total=20.8166
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 418.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.3141 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4425, fwd=7.7307 cmp_logits=0.0668, sampling=12.5232, total=20.7639
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2891 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4125, fwd=7.8399 cmp_logits=0.0739, sampling=11.8256, total=20.1530
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5908 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3915, fwd=7.6954 cmp_logits=0.0670, sampling=11.9798, total=20.1342
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.7
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5696 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3941, fwd=7.6513 cmp_logits=0.0670, sampling=12.0118, total=20.1252
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.7
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5650 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4218, fwd=7.6752 cmp_logits=0.0665, sampling=12.0275, total=20.1917
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6528 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3986, fwd=7.6351 cmp_logits=0.0658, sampling=12.0468, total=20.1471
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5903 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3929, fwd=7.7620 cmp_logits=0.0663, sampling=11.9326, total=20.1547
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5898 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4222, fwd=7.8144 cmp_logits=0.0658, sampling=11.8871, total=20.1902
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6378 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3941, fwd=7.7231 cmp_logits=0.0660, sampling=11.9793, total=20.1633
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5986 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3939, fwd=7.7541 cmp_logits=0.0670, sampling=11.9863, total=20.2019
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6366 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3979, fwd=7.6795 cmp_logits=0.0670, sampling=12.0208, total=20.1662
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6108 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3951, fwd=7.6871 cmp_logits=0.0660, sampling=12.0258, total=20.1747
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6244 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3917, fwd=7.7035 cmp_logits=0.0670, sampling=11.9879, total=20.1511
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5903 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3986, fwd=7.6952 cmp_logits=0.0665, sampling=12.0053, total=20.1664
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6237 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3791, fwd=7.7999 cmp_logits=0.0799, sampling=11.6425, total=19.9020
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3106 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3774, fwd=7.7369 cmp_logits=0.0663, sampling=11.6518, total=19.8331
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 294.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.4
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2363 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3841, fwd=7.7598 cmp_logits=0.0663, sampling=11.6584, total=19.8693
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.4
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2820 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3729, fwd=7.7279 cmp_logits=0.0668, sampling=11.6935, total=19.8615
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.4
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2723 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4003, fwd=7.6401 cmp_logits=0.0656, sampling=11.7838, total=19.8905
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3037 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3755, fwd=7.6840 cmp_logits=0.0663, sampling=11.7946, total=19.9213
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3424 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3734, fwd=7.6585 cmp_logits=0.0656, sampling=11.7984, total=19.8967
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3032 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3736, fwd=7.7114 cmp_logits=0.0660, sampling=11.7114, total=19.8634
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3040 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3750, fwd=7.6914 cmp_logits=0.0663, sampling=11.7812, total=19.9146
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3266 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3765, fwd=7.7751 cmp_logits=0.0670, sampling=11.6808, total=19.8998
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3047 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3824, fwd=7.8356 cmp_logits=0.0665, sampling=11.7209, total=20.0062
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 291.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.4136 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3765, fwd=7.6842 cmp_logits=0.0665, sampling=11.7645, total=19.8927
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2973 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3746, fwd=7.7307 cmp_logits=0.0663, sampling=11.7109, total=19.8829
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2901 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3989, fwd=7.6764 cmp_logits=0.0668, sampling=11.7512, total=19.8941
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3028 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3767, fwd=7.6544 cmp_logits=0.0665, sampling=11.7970, total=19.8951
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3354 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3772, fwd=7.6818 cmp_logits=0.0675, sampling=11.7824, total=19.9101
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3464 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3772, fwd=7.7505 cmp_logits=0.0668, sampling=11.6842, total=19.8796
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.4
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2842 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3743, fwd=7.7379 cmp_logits=0.0670, sampling=11.6780, total=19.8579
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.4
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2634 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3712, fwd=7.7276 cmp_logits=0.0756, sampling=11.7164, total=19.8913
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2975 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3829, fwd=7.6530 cmp_logits=0.0761, sampling=11.7803, total=19.8929
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3087 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3788, fwd=7.6544 cmp_logits=0.0670, sampling=11.7788, total=19.8798
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3238 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3741, fwd=7.5972 cmp_logits=0.0658, sampling=11.8544, total=19.8925
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3009 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3750, fwd=7.6349 cmp_logits=0.0679, sampling=11.8330, total=19.9118
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3123 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3750, fwd=7.6315 cmp_logits=0.0780, sampling=11.7872, total=19.8724
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.4
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2799 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3786, fwd=7.6854 cmp_logits=0.0663, sampling=11.7769, total=19.9082
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3097 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3798, fwd=7.7510 cmp_logits=0.0675, sampling=11.7137, total=19.9132
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3464 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3741, fwd=7.6597 cmp_logits=0.0663, sampling=11.8253, total=19.9261
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3261 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3860, fwd=7.6313 cmp_logits=0.0663, sampling=11.8349, total=19.9192
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3238 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3729, fwd=7.5827 cmp_logits=0.0653, sampling=11.9038, total=19.9256
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3292 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3760, fwd=7.6783 cmp_logits=0.0665, sampling=11.7788, total=19.9006
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3080 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3748, fwd=7.6044 cmp_logits=0.0789, sampling=11.8723, total=19.9313
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 291.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.4256 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3576, fwd=7.7276 cmp_logits=0.0658, sampling=11.4791, total=19.6309
INFO 10-04 02:40:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0055 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3567, fwd=7.6714 cmp_logits=0.0663, sampling=11.5917, total=19.6869
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0548 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3793, fwd=7.6845 cmp_logits=0.0658, sampling=11.5180, total=19.6486
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0202 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3653, fwd=7.7608 cmp_logits=0.0656, sampling=11.4565, total=19.6488
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0353 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3555, fwd=7.7078 cmp_logits=0.0656, sampling=11.4870, total=19.6168
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0307 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3524, fwd=7.7300 cmp_logits=0.0665, sampling=11.4946, total=19.6443
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0150 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3533, fwd=7.8006 cmp_logits=0.0663, sampling=11.4348, total=19.6557
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0355 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3519, fwd=7.7384 cmp_logits=0.0672, sampling=11.4818, total=19.6397
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0093 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3557, fwd=7.7403 cmp_logits=0.0658, sampling=11.4801, total=19.6426
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0188 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3593, fwd=7.7620 cmp_logits=0.0663, sampling=11.4207, total=19.6090
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0171 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3800, fwd=7.8309 cmp_logits=0.0660, sampling=11.3564, total=19.6340
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0174 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3650, fwd=7.7581 cmp_logits=0.0658, sampling=11.4255, total=19.6154
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9842 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3655, fwd=7.6981 cmp_logits=0.0675, sampling=11.4598, total=19.5916
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9573 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3638, fwd=7.6096 cmp_logits=0.0663, sampling=11.5643, total=19.6047
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9766 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3555, fwd=7.6623 cmp_logits=0.0672, sampling=11.5278, total=19.6135
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0019 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3576, fwd=7.6914 cmp_logits=0.0656, sampling=11.5101, total=19.6257
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0036 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3674, fwd=7.7496 cmp_logits=0.0663, sampling=11.4315, total=19.6159
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9916 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3543, fwd=7.7424 cmp_logits=0.0660, sampling=11.4820, total=19.6455
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0121 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3552, fwd=7.7941 cmp_logits=0.0663, sampling=11.3912, total=19.6075
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9816 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3557, fwd=7.8020 cmp_logits=0.0653, sampling=11.3635, total=19.5875
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9678 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3617, fwd=7.7469 cmp_logits=0.0658, sampling=11.4274, total=19.6028
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0009 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3576, fwd=7.7779 cmp_logits=0.0658, sampling=11.3580, total=19.5603
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9466 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3541, fwd=7.7336 cmp_logits=0.0663, sampling=11.4520, total=19.6066
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9816 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3552, fwd=7.7314 cmp_logits=0.0665, sampling=11.4543, total=19.6080
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9814 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3579, fwd=7.7429 cmp_logits=0.0663, sampling=11.4563, total=19.6240
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0150 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3548, fwd=7.7715 cmp_logits=0.0665, sampling=11.4081, total=19.6016
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9742 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3781, fwd=7.7953 cmp_logits=0.0715, sampling=11.3788, total=19.6247
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9952 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3805, fwd=7.8952 cmp_logits=0.0670, sampling=11.2989, total=19.6428
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0136 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3729, fwd=7.8037 cmp_logits=0.0660, sampling=11.3230, total=19.5663
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9480 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3545, fwd=7.7062 cmp_logits=0.0663, sampling=11.4758, total=19.6035
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9826 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3524, fwd=7.6907 cmp_logits=0.0660, sampling=11.4346, total=19.5446
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9256 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3519, fwd=7.7028 cmp_logits=0.0665, sampling=11.4636, total=19.5861
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9594 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3531, fwd=7.7176 cmp_logits=0.0668, sampling=11.4427, total=19.5811
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9506 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3543, fwd=7.6971 cmp_logits=0.0665, sampling=11.4665, total=19.5851
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9554 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3541, fwd=7.7062 cmp_logits=0.0677, sampling=11.4870, total=19.6154
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9931 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3543, fwd=7.8251 cmp_logits=0.0670, sampling=11.3096, total=19.5570
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 249.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9277 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3541, fwd=7.6771 cmp_logits=0.0660, sampling=11.4918, total=19.5899
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9623 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3533, fwd=7.7589 cmp_logits=0.0663, sampling=11.4186, total=19.5978
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9673 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3767, fwd=7.6795 cmp_logits=0.0663, sampling=11.4584, total=19.5816
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9537 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3543, fwd=7.6709 cmp_logits=0.0663, sampling=11.4911, total=19.5835
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9628 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3607, fwd=7.6439 cmp_logits=0.0675, sampling=11.5223, total=19.5954
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0381 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3231, fwd=7.6952 cmp_logits=0.0722, sampling=10.0079, total=18.0993
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 160.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3949 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3109, fwd=7.6826 cmp_logits=0.0656, sampling=9.9990, total=18.0590
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3499 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3083, fwd=7.6683 cmp_logits=0.0653, sampling=10.0455, total=18.0879
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3797 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3140, fwd=7.7217 cmp_logits=0.0653, sampling=9.9704, total=18.0719
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3716 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3119, fwd=7.6926 cmp_logits=0.0658, sampling=9.9962, total=18.0674
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3711 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3111, fwd=7.7064 cmp_logits=0.0660, sampling=9.9745, total=18.0588
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3549 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3119, fwd=7.7112 cmp_logits=0.0658, sampling=9.9947, total=18.0840
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3785 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3128, fwd=7.7834 cmp_logits=0.0656, sampling=9.9254, total=18.0883
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4181 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2911, fwd=7.7720 cmp_logits=0.0758, sampling=9.6908, total=17.8306
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0974 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2882, fwd=7.6778 cmp_logits=0.0651, sampling=9.7589, total=17.7910
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0428 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2894, fwd=7.6201 cmp_logits=0.0644, sampling=9.8405, total=17.8154
INFO 10-04 02:40:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0659 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2892, fwd=7.7283 cmp_logits=0.0656, sampling=9.7115, total=17.7956
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0478 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2899, fwd=7.6606 cmp_logits=0.0644, sampling=9.7787, total=17.7944
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0435 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2899, fwd=7.6625 cmp_logits=0.0770, sampling=9.7525, total=17.7829
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0478 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2892, fwd=7.7112 cmp_logits=0.0651, sampling=9.7244, total=17.7910
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0461 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2873, fwd=7.7214 cmp_logits=0.0656, sampling=9.7282, total=17.8032
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0588 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2902, fwd=7.7062 cmp_logits=0.0653, sampling=9.7404, total=17.8027
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0535 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2890, fwd=7.6516 cmp_logits=0.0741, sampling=9.7771, total=17.7927
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0509 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2875, fwd=7.6401 cmp_logits=0.0658, sampling=9.8050, total=17.7991
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0559 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2885, fwd=7.6911 cmp_logits=0.0651, sampling=9.7458, total=17.7913
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0535 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2899, fwd=7.5908 cmp_logits=0.0651, sampling=9.8445, total=17.7913
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0418 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2880, fwd=7.6826 cmp_logits=0.0648, sampling=9.7685, total=17.8049
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0583 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2868, fwd=7.7016 cmp_logits=0.0644, sampling=9.7582, total=17.8115
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0638 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3090, fwd=7.7007 cmp_logits=0.0660, sampling=9.7547, total=17.8308
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0843 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2899, fwd=7.5727 cmp_logits=0.0651, sampling=9.8810, total=17.8096
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0612 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2904, fwd=7.6671 cmp_logits=0.0646, sampling=9.7835, total=17.8065
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0783 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2942, fwd=7.6294 cmp_logits=0.0653, sampling=9.8200, total=17.8099
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0619 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2885, fwd=7.6919 cmp_logits=0.0651, sampling=9.7568, total=17.8030
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0593 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2921, fwd=7.6783 cmp_logits=0.0648, sampling=9.7671, total=17.8027
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0552 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2863, fwd=7.7038 cmp_logits=0.0656, sampling=9.7387, total=17.7946
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0466 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2885, fwd=7.6399 cmp_logits=0.0653, sampling=9.8135, total=17.8082
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0619 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2906, fwd=7.6776 cmp_logits=0.0653, sampling=9.7783, total=17.8125
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0681 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2942, fwd=7.5700 cmp_logits=0.0656, sampling=9.9664, total=17.8967
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1513 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2894, fwd=7.7002 cmp_logits=0.0651, sampling=9.8612, total=17.9167
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1732 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2894, fwd=7.6802 cmp_logits=0.0644, sampling=9.8438, total=17.8788
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1718 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2649, fwd=7.8654 cmp_logits=0.0665, sampling=9.7141, total=17.9119
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1174 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2623, fwd=7.8793 cmp_logits=0.0658, sampling=9.7270, total=17.9353
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1346 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2623, fwd=7.7887 cmp_logits=0.0663, sampling=9.8088, total=17.9272
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1262 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2646, fwd=7.8168 cmp_logits=0.0663, sampling=9.7647, total=17.9133
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1248 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2725, fwd=7.8568 cmp_logits=0.0653, sampling=9.7239, total=17.9193
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1227 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2625, fwd=7.7894 cmp_logits=0.0665, sampling=9.8200, total=17.9393
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1382 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2637, fwd=7.7586 cmp_logits=0.0658, sampling=9.8398, total=17.9286
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1279 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2623, fwd=7.7741 cmp_logits=0.0768, sampling=9.8031, total=17.9174
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1236 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2635, fwd=7.7767 cmp_logits=0.0663, sampling=9.8085, total=17.9155
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1196 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2627, fwd=7.8068 cmp_logits=0.0651, sampling=9.8007, total=17.9365
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1375 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2604, fwd=7.7734 cmp_logits=0.0663, sampling=9.8271, total=17.9284
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1298 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2866, fwd=7.8325 cmp_logits=0.0675, sampling=9.7504, total=17.9379
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1363 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2635, fwd=7.8249 cmp_logits=0.0665, sampling=9.7446, total=17.9000
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1220 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2651, fwd=7.8781 cmp_logits=0.0658, sampling=9.6927, total=17.9026
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1065 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2680, fwd=7.7894 cmp_logits=0.0663, sampling=9.8069, total=17.9315
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1308 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2632, fwd=7.7553 cmp_logits=0.0656, sampling=9.8441, total=17.9288
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1298 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2649, fwd=7.7484 cmp_logits=0.0660, sampling=9.8336, total=17.9138
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1139 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2611, fwd=7.8590 cmp_logits=0.0658, sampling=9.7306, total=17.9174
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1165 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2658, fwd=7.8218 cmp_logits=0.0660, sampling=9.7771, total=17.9315
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1351 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2666, fwd=7.7927 cmp_logits=0.0656, sampling=9.7873, total=17.9131
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1141 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2961, fwd=7.8676 cmp_logits=0.0668, sampling=9.7263, total=17.9572
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1580 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2730, fwd=7.8189 cmp_logits=0.0730, sampling=9.7494, total=17.9150
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1162 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2618, fwd=7.8444 cmp_logits=0.0665, sampling=9.7458, total=17.9193
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1208 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2649, fwd=7.8647 cmp_logits=0.0663, sampling=9.7148, total=17.9112
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1155 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2623, fwd=7.8743 cmp_logits=0.0656, sampling=9.7146, total=17.9176
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1186 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2611, fwd=7.8850 cmp_logits=0.0665, sampling=9.6838, total=17.8967
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0979 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2630, fwd=7.7724 cmp_logits=0.0675, sampling=9.8190, total=17.9229
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1258 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2639, fwd=7.8506 cmp_logits=0.0663, sampling=9.7437, total=17.9255
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1267 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:40:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:40:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:40:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2632, fwd=7.9088 cmp_logits=0.0679, sampling=9.6793, total=17.9198
INFO 10-04 02:40:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:40:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1718 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009643.9420905, last_token_time=1728009645.8852522, first_scheduled_time=1728009643.978445, first_token_time=1728009644.1719308, time_in_queue=0.03635454177856445, finished_time=1728009645.8852127, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009643.9439313, last_token_time=1728009646.9615111, first_scheduled_time=1728009643.978445, first_token_time=1728009644.355852, time_in_queue=0.03451371192932129, finished_time=1728009646.9614863, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009643.9472532, last_token_time=1728009646.326859, first_scheduled_time=1728009644.1724322, first_token_time=1728009644.355852, time_in_queue=0.22517895698547363, finished_time=1728009646.326835, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009643.9491465, last_token_time=1728009647.7876468, first_scheduled_time=1728009644.1724322, first_token_time=1728009644.51051, time_in_queue=0.22328567504882812, finished_time=1728009647.7876263, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009643.953763, last_token_time=1728009647.7876468, first_scheduled_time=1728009644.3562722, first_token_time=1728009644.662656, time_in_queue=0.4025092124938965, finished_time=1728009647.7876322, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009643.9581332, last_token_time=1728009646.0568345, first_scheduled_time=1728009644.5109289, first_token_time=1728009644.8010697, time_in_queue=0.5527956485748291, finished_time=1728009646.0568163, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009643.9593477, last_token_time=1728009646.0568345, first_scheduled_time=1728009644.6631806, first_token_time=1728009644.8010697, time_in_queue=0.7038328647613525, finished_time=1728009646.0568206, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009643.960536, last_token_time=1728009647.9359808, first_scheduled_time=1728009644.6631806, first_token_time=1728009644.9586785, time_in_queue=0.7026445865631104, finished_time=1728009647.935968, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009643.9653642, last_token_time=1728009648.9578426, first_scheduled_time=1728009644.8016272, first_token_time=1728009645.2886105, time_in_queue=0.8362629413604736, finished_time=1728009648.9578404, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009643.9725835, last_token_time=1728009648.427953, first_scheduled_time=1728009645.1143937, first_token_time=1728009645.450659, time_in_queue=1.1418101787567139, finished_time=1728009648.4279509, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 19139
End-to-End latency: 5.02 seconds
Throughput: 1.99 requests/s, 3815.61 tokens/s
Per_token_time: 0.262 ms

Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=4096, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=4096, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Selected required requests 10
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Chunked prefill is enabled. max_num_batched_tokens=4096, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'type', 'original_max_position_embeddings', 'finetuned'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 02:03:02 config.py:654] [SchedulerConfig] max_num_batched_tokens: 4096 chunked_prefill_enabled: True
INFO 10-04 02:03:02 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 02:03:02 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 02:03:07 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 02:03:28 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 02:03:28 model_runner.py:183] Loaded model: 
INFO 10-04 02:03:28 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 02:03:28 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 02:03:28 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 02:03:28 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 02:03:28 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 02:03:28 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 02:03:28 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 02:03:28 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 02:03:28 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 02:03:28 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 02:03:28 model_runner.py:183]         )
INFO 10-04 02:03:28 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 02:03:28 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 02:03:28 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 02:03:28 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 02:03:28 model_runner.py:183]         )
INFO 10-04 02:03:28 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:03:28 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:03:28 model_runner.py:183]       )
INFO 10-04 02:03:28 model_runner.py:183]     )
INFO 10-04 02:03:28 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:03:28 model_runner.py:183]   )
INFO 10-04 02:03:28 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 02:03:28 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 02:03:28 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 02:03:28 model_runner.py:183] )
INFO 10-04 02:03:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 02:03:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 7.2899, fwd=440.9115 cmp_logits=0.2372, sampling=257.6113, total=706.0516
INFO 10-04 02:03:29 worker.py:164] Peak: 13.385 GB, Initial: 38.980 GB, Free: 25.595 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.039 GB
INFO 10-04 02:03:29 gpu_executor.py:117] # GPU blocks: 3077, # CPU blocks: 8192
INFO 10-04 02:04:01 worker.py:189] _init_cache_engine took 24.0625 GB
INFO 10-04 02:04:01 scheduler.py:307] Scheduler initialized with prompt limit: 4096
INFO 10-04 02:04:01 Start warmup...
INFO 10-04 02:04:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:04:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9062, fwd=1904.0496 cmp_logits=71.2695, sampling=0.9897, total=1977.2172
INFO 10-04 02:04:03 metrics.py:335] Avg prompt throughput: 516.1 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 1984.1
INFO 10-04 02:04:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1977.7696 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3133, fwd=51.1658 cmp_logits=0.1242, sampling=6.6030, total=58.2092
INFO 10-04 02:04:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 59.1
INFO 10-04 02:04:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 58.6684 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3181, fwd=8.0459 cmp_logits=0.0775, sampling=7.0617, total=15.5053
INFO 10-04 02:04:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 02:04:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7995 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2773, fwd=7.8967 cmp_logits=0.0746, sampling=7.2036, total=15.4536
INFO 10-04 02:04:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:04:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6889 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2818, fwd=7.9422 cmp_logits=0.0737, sampling=7.1797, total=15.4788
INFO 10-04 02:04:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 02:04:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7027 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2725, fwd=7.9329 cmp_logits=0.0780, sampling=7.1480, total=15.4326
INFO 10-04 02:04:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:04:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6491 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2787, fwd=7.9503 cmp_logits=0.0803, sampling=7.0679, total=15.3782
INFO 10-04 02:04:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:04:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6043 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2623, fwd=7.9265 cmp_logits=0.0727, sampling=5.9896, total=14.2517
INFO 10-04 02:04:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 02:04:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4923 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2599, fwd=7.9176 cmp_logits=0.0720, sampling=6.0098, total=14.2598
INFO 10-04 02:04:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 02:04:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4560 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2601, fwd=7.9215 cmp_logits=0.0715, sampling=5.9910, total=14.2448
INFO 10-04 02:04:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 02:04:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4622 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:08 Start benchmarking...
INFO 10-04 02:04:08 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:08 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.6561, fwd=1642.9508 cmp_logits=0.1571, sampling=260.4568, total=1905.2219
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 2114.3 tokens/s, Avg generation throughput: 2.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 9.8%, CPU KV cache usage: 0.0%, Interval(ms): 1937.3
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1905.9160 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.7192, fwd=14.3414 cmp_logits=0.0923, sampling=245.4026, total=261.5564
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 15590.5 tokens/s, Avg generation throughput: 30.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 262.5
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 262.1982 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2478]), positions.shape=torch.Size([2478]) hidden_states.shape=torch.Size([2478, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.2486, fwd=14.4269 cmp_logits=0.0887, sampling=161.2246, total=176.9900
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 13896.7 tokens/s, Avg generation throughput: 56.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.8%, CPU KV cache usage: 0.0%, Interval(ms): 177.7
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 177.5548 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4253, fwd=7.8287 cmp_logits=0.0734, sampling=9.1670, total=17.4959
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0016 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4196, fwd=7.7927 cmp_logits=0.0746, sampling=9.1970, total=17.4849
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9853 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4172, fwd=7.7608 cmp_logits=0.0718, sampling=9.2235, total=17.4744
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9894 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4187, fwd=7.7715 cmp_logits=0.0727, sampling=9.2366, total=17.5002
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0035 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4146, fwd=7.7984 cmp_logits=0.0744, sampling=9.2018, total=17.4899
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9923 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4117, fwd=7.7515 cmp_logits=0.0722, sampling=9.2509, total=17.4873
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9899 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4168, fwd=7.7972 cmp_logits=0.0727, sampling=9.2039, total=17.4916
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9982 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4146, fwd=7.7817 cmp_logits=0.0727, sampling=9.2247, total=17.4944
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9958 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4172, fwd=7.7951 cmp_logits=0.0732, sampling=9.1877, total=17.4742
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9825 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4337, fwd=7.6902 cmp_logits=0.0730, sampling=9.3219, total=17.5195
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0202 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4179, fwd=7.7569 cmp_logits=0.0727, sampling=9.3768, total=17.6251
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 546.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1355 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4172, fwd=7.7846 cmp_logits=0.0722, sampling=9.2263, total=17.5014
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0168 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4191, fwd=7.7670 cmp_logits=0.0727, sampling=9.2509, total=17.5104
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0254 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4170, fwd=7.7357 cmp_logits=0.0718, sampling=9.2981, total=17.5233
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0273 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4122, fwd=7.7569 cmp_logits=0.0730, sampling=9.2850, total=17.5283
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0340 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4125, fwd=7.7944 cmp_logits=0.0734, sampling=9.2487, total=17.5300
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0333 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4132, fwd=7.7424 cmp_logits=0.0727, sampling=9.3119, total=17.5409
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0402 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4156, fwd=7.8082 cmp_logits=0.0725, sampling=9.2342, total=17.5312
INFO 10-04 02:04:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0628 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4270, fwd=7.7736 cmp_logits=0.0818, sampling=9.2809, total=17.5643
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0666 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4191, fwd=7.8049 cmp_logits=0.0732, sampling=9.2411, total=17.5395
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0435 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4163, fwd=7.7393 cmp_logits=0.0720, sampling=9.2955, total=17.5238
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0218 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4151, fwd=7.7758 cmp_logits=0.0732, sampling=9.2754, total=17.5402
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0485 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4172, fwd=7.7615 cmp_logits=0.0725, sampling=9.2833, total=17.5354
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0724 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4139, fwd=7.7703 cmp_logits=0.0722, sampling=9.2869, total=17.5445
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0588 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4013, fwd=7.8483 cmp_logits=0.0856, sampling=8.9154, total=17.2510
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7290 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4025, fwd=7.7846 cmp_logits=0.0722, sampling=8.9939, total=17.2539
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7302 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4017, fwd=7.7746 cmp_logits=0.0730, sampling=8.9836, total=17.2336
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7407 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3703, fwd=7.8442 cmp_logits=0.0837, sampling=8.4784, total=16.7778
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2293 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3693, fwd=7.8366 cmp_logits=0.0725, sampling=8.4741, total=16.7532
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1652 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3641, fwd=7.7989 cmp_logits=0.0718, sampling=8.5142, total=16.7499
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 404.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1638 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3641, fwd=7.7779 cmp_logits=0.0715, sampling=8.5235, total=16.7377
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 404.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1528 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3710, fwd=7.7875 cmp_logits=0.0718, sampling=8.6026, total=16.8340
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2501 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3688, fwd=7.7975 cmp_logits=0.0727, sampling=8.6129, total=16.8529
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3202 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3564, fwd=7.8402 cmp_logits=0.0858, sampling=8.3182, total=16.6013
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9907 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3541, fwd=7.8018 cmp_logits=0.0722, sampling=8.3640, total=16.5927
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9787 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3543, fwd=7.8094 cmp_logits=0.0713, sampling=8.3416, total=16.5775
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9578 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3543, fwd=7.7500 cmp_logits=0.0715, sampling=8.4088, total=16.5856
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9721 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3664, fwd=7.7848 cmp_logits=0.0720, sampling=8.3895, total=16.6142
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0290 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3548, fwd=7.7763 cmp_logits=0.0725, sampling=8.3768, total=16.5808
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9618 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3529, fwd=7.8509 cmp_logits=0.0720, sampling=8.3008, total=16.5775
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9649 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3543, fwd=7.7486 cmp_logits=0.0713, sampling=8.4195, total=16.5949
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9837 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3562, fwd=7.8106 cmp_logits=0.0725, sampling=8.3590, total=16.5992
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9785 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3529, fwd=7.7815 cmp_logits=0.0722, sampling=8.3792, total=16.5865
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0043 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3595, fwd=7.7341 cmp_logits=0.0718, sampling=8.4431, total=16.6092
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9945 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3607, fwd=7.7293 cmp_logits=0.0713, sampling=8.4269, total=16.5889
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9699 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3514, fwd=7.7960 cmp_logits=0.0727, sampling=8.3487, total=16.5696
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9671 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3386, fwd=7.8278 cmp_logits=0.0825, sampling=8.0605, total=16.3097
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6621 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3376, fwd=7.7748 cmp_logits=0.0713, sampling=8.0385, total=16.2232
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5768 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3378, fwd=7.7102 cmp_logits=0.0718, sampling=8.0955, total=16.2163
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5713 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3374, fwd=7.7333 cmp_logits=0.0710, sampling=8.0876, total=16.2301
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5780 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3376, fwd=7.7841 cmp_logits=0.0722, sampling=8.0287, total=16.2234
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5787 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3374, fwd=7.7422 cmp_logits=0.0713, sampling=8.0585, total=16.2103
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5627 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3362, fwd=7.8270 cmp_logits=0.0722, sampling=8.0149, total=16.2511
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5992 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3386, fwd=7.7868 cmp_logits=0.0715, sampling=8.0256, total=16.2234
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5761 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3343, fwd=7.7419 cmp_logits=0.0708, sampling=8.0884, total=16.2368
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5920 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3362, fwd=7.7631 cmp_logits=0.0710, sampling=8.0569, total=16.2282
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5756 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3376, fwd=7.7686 cmp_logits=0.0715, sampling=8.0583, total=16.2370
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5839 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3378, fwd=7.7846 cmp_logits=0.0718, sampling=8.1041, total=16.2988
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6461 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3371, fwd=7.7913 cmp_logits=0.0718, sampling=8.0154, total=16.2168
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5703 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3462, fwd=7.7586 cmp_logits=0.0713, sampling=8.0695, total=16.2468
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.6%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6018 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3386, fwd=7.7887 cmp_logits=0.0720, sampling=8.0202, total=16.2203
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.6%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5665 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3376, fwd=7.7910 cmp_logits=0.0713, sampling=8.0132, total=16.2141
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.6%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5625 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3414, fwd=7.7698 cmp_logits=0.0715, sampling=8.1041, total=16.2876
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.3 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6633 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3216, fwd=7.7868 cmp_logits=0.0706, sampling=7.5254, total=15.7051
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 246.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.0468 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3030, fwd=7.8127 cmp_logits=0.0839, sampling=7.1135, total=15.3139
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5966 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3016, fwd=7.7064 cmp_logits=0.0710, sampling=7.2539, total=15.3337
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6391 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2859, fwd=7.7987 cmp_logits=0.0832, sampling=7.1170, total=15.2857
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5315 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2897, fwd=7.7863 cmp_logits=0.0710, sampling=7.1523, total=15.3003
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5468 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2861, fwd=7.7169 cmp_logits=0.0706, sampling=7.1783, total=15.2526
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4963 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2837, fwd=7.6849 cmp_logits=0.0703, sampling=7.1974, total=15.2371
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4827 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2875, fwd=7.7283 cmp_logits=0.0699, sampling=7.1509, total=15.2378
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4819 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2830, fwd=7.7441 cmp_logits=0.0713, sampling=7.1571, total=15.2562
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5010 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2840, fwd=7.7927 cmp_logits=0.0703, sampling=7.1478, total=15.2957
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5401 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2859, fwd=7.7562 cmp_logits=0.0706, sampling=7.1449, total=15.2583
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5017 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2909, fwd=7.7996 cmp_logits=0.0708, sampling=7.0784, total=15.2404
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4850 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2809, fwd=7.7288 cmp_logits=0.0718, sampling=7.1685, total=15.2507
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4960 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2851, fwd=7.7734 cmp_logits=0.0708, sampling=7.1208, total=15.2512
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5172 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2611, fwd=7.9725 cmp_logits=0.0725, sampling=7.1015, total=15.4083
INFO 10-04 02:04:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:04:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6095 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2613, fwd=7.9203 cmp_logits=0.0718, sampling=7.1471, total=15.4009
INFO 10-04 02:04:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:04:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6002 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2637, fwd=7.9253 cmp_logits=0.0718, sampling=7.1261, total=15.3875
INFO 10-04 02:04:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:04:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5864 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2666, fwd=7.8690 cmp_logits=0.0708, sampling=7.1657, total=15.3728
INFO 10-04 02:04:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:04:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5714 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2608, fwd=7.8800 cmp_logits=0.0720, sampling=7.1795, total=15.3930
INFO 10-04 02:04:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:04:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5885 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2599, fwd=7.9362 cmp_logits=0.0715, sampling=7.1816, total=15.4502
INFO 10-04 02:04:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:04:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6476 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2611, fwd=7.8657 cmp_logits=0.0715, sampling=7.1993, total=15.3987
INFO 10-04 02:04:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:04:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5973 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2620, fwd=7.9160 cmp_logits=0.0727, sampling=7.1340, total=15.3856
INFO 10-04 02:04:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:04:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5838 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2623, fwd=7.9150 cmp_logits=0.0722, sampling=7.1342, total=15.3852
INFO 10-04 02:04:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:04:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5833 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2649, fwd=7.8900 cmp_logits=0.0718, sampling=7.1731, total=15.4006
INFO 10-04 02:04:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:04:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5973 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2632, fwd=7.8967 cmp_logits=0.0710, sampling=7.1952, total=15.4269
INFO 10-04 02:04:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:04:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6269 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2749, fwd=7.8280 cmp_logits=0.0789, sampling=7.2122, total=15.3947
INFO 10-04 02:04:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:04:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5919 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:04:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:04:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:04:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2582, fwd=7.9205 cmp_logits=0.0713, sampling=7.1230, total=15.3739
INFO 10-04 02:04:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:04:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5957 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007448.2907376, last_token_time=1728007451.0951111, first_scheduled_time=1728007448.3122842, first_token_time=1728007450.2176828, time_in_queue=0.021546602249145508, finished_time=1728007451.095071, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007448.292679, last_token_time=1728007451.4760833, first_scheduled_time=1728007448.3122842, first_token_time=1728007450.2176828, time_in_queue=0.019605159759521484, finished_time=1728007451.4760578, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007448.2946222, last_token_time=1728007451.2532058, first_scheduled_time=1728007448.3122842, first_token_time=1728007450.2176828, time_in_queue=0.01766204833984375, finished_time=1728007451.253182, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007448.2958934, last_token_time=1728007451.7608192, first_scheduled_time=1728007448.3122842, first_token_time=1728007450.2176828, time_in_queue=0.01639080047607422, finished_time=1728007451.7607973, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007448.2985253, last_token_time=1728007451.7770615, first_scheduled_time=1728007448.3122842, first_token_time=1728007450.480116, time_in_queue=0.01375889778137207, finished_time=1728007451.7770445, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007448.3009043, last_token_time=1728007451.1488638, first_scheduled_time=1728007450.2184076, first_token_time=1728007450.480116, time_in_queue=1.9175033569335938, finished_time=1728007451.1488445, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007448.3019385, last_token_time=1728007451.1488638, first_scheduled_time=1728007450.2184076, first_token_time=1728007450.480116, time_in_queue=1.9164690971374512, finished_time=1728007451.1488492, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007448.302979, last_token_time=1728007451.8086307, first_scheduled_time=1728007450.2184076, first_token_time=1728007450.480116, time_in_queue=1.915428638458252, finished_time=1728007451.8086169, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007448.3055058, last_token_time=1728007452.1854198, first_scheduled_time=1728007450.2184076, first_token_time=1728007450.657837, time_in_queue=1.9129018783569336, finished_time=1728007452.1854172, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007448.3090405, last_token_time=1728007451.980825, first_scheduled_time=1728007450.4807029, first_token_time=1728007450.657837, time_in_queue=2.1716623306274414, finished_time=1728007451.980823, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 11197
End-to-End latency: 3.89 seconds
Throughput: 2.57 requests/s, 2874.79 tokens/s
Per_token_time: 0.348 ms

Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=128, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=4096, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=2048, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Selected required requests 20
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Chunked prefill is enabled. max_num_batched_tokens=2048, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'finetuned', 'original_max_position_embeddings', 'type'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 01:51:34 config.py:654] [SchedulerConfig] max_num_batched_tokens: 2048 chunked_prefill_enabled: True
INFO 10-04 01:51:34 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 01:51:34 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 01:51:39 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 01:52:00 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 01:52:00 model_runner.py:183] Loaded model: 
INFO 10-04 01:52:00 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 01:52:00 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 01:52:00 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 01:52:00 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 01:52:00 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 01:52:00 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 01:52:00 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 01:52:00 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 01:52:00 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 01:52:00 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 01:52:00 model_runner.py:183]         )
INFO 10-04 01:52:00 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 01:52:00 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 01:52:00 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 01:52:00 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 01:52:00 model_runner.py:183]         )
INFO 10-04 01:52:00 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 01:52:00 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 01:52:00 model_runner.py:183]       )
INFO 10-04 01:52:00 model_runner.py:183]     )
INFO 10-04 01:52:00 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 01:52:00 model_runner.py:183]   )
INFO 10-04 01:52:00 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 01:52:00 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 01:52:00 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 01:52:00 model_runner.py:183] )
INFO 10-04 01:52:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 01:52:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 6.1238, fwd=349.2856 cmp_logits=0.2422, sampling=156.6439, total=512.2972
INFO 10-04 01:52:00 worker.py:164] Peak: 13.332 GB, Initial: 38.980 GB, Free: 25.648 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.092 GB
INFO 10-04 01:52:01 gpu_executor.py:117] # GPU blocks: 3083, # CPU blocks: 8192
INFO 10-04 01:52:33 worker.py:189] _init_cache_engine took 24.0859 GB
INFO 10-04 01:52:33 scheduler.py:307] Scheduler initialized with prompt limit: 4096
INFO 10-04 01:52:33 Start warmup...
INFO 10-04 01:52:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:33 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 01:52:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9148, fwd=1874.4302 cmp_logits=71.6321, sampling=0.9363, total=1947.9153
INFO 10-04 01:52:34 metrics.py:335] Avg prompt throughput: 523.7 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 1955.5
INFO 10-04 01:52:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1948.4477 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2978, fwd=38.5017 cmp_logits=0.1142, sampling=6.8884, total=45.8033
INFO 10-04 01:52:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 46.6
INFO 10-04 01:52:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.2265 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3111, fwd=7.7868 cmp_logits=0.0706, sampling=6.5000, total=14.6699
INFO 10-04 01:52:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.2
INFO 10-04 01:52:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.9555 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2711, fwd=7.7598 cmp_logits=0.0687, sampling=6.2304, total=14.3311
INFO 10-04 01:52:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 01:52:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5750 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2639, fwd=7.8485 cmp_logits=0.0684, sampling=6.2354, total=14.4176
INFO 10-04 01:52:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.8
INFO 10-04 01:52:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.6430 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2632, fwd=7.7438 cmp_logits=0.0687, sampling=6.2602, total=14.3368
INFO 10-04 01:52:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 01:52:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5521 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2646, fwd=7.7648 cmp_logits=0.0718, sampling=6.2268, total=14.3292
INFO 10-04 01:52:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 01:52:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5471 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2570, fwd=7.8425 cmp_logits=0.0670, sampling=6.1500, total=14.3173
INFO 10-04 01:52:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 01:52:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5562 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2637, fwd=7.9021 cmp_logits=0.0672, sampling=6.1104, total=14.3442
INFO 10-04 01:52:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 01:52:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5421 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2556, fwd=7.8192 cmp_logits=0.0665, sampling=6.2096, total=14.3521
INFO 10-04 01:52:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 01:52:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5652 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:40 Start benchmarking...
INFO 10-04 01:52:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 01:52:41 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1530, fwd=1579.2120 cmp_logits=0.1490, sampling=136.8909, total=1717.4065
INFO 10-04 01:52:41 metrics.py:335] Avg prompt throughput: 1164.8 tokens/s, Avg generation throughput: 1.7 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%, Interval(ms): 1758.2
INFO 10-04 01:52:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1718.0319 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 01:52:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:41 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 01:52:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1747, fwd=1569.9298 cmp_logits=0.1395, sampling=122.8554, total=1694.1004
INFO 10-04 01:52:43 metrics.py:335] Avg prompt throughput: 1206.5 tokens/s, Avg generation throughput: 4.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 10.6%, CPU KV cache usage: 0.0%, Interval(ms): 1695.0
INFO 10-04 01:52:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1694.7758 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 01:52:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 01:52:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1768, fwd=14.3619 cmp_logits=0.0858, sampling=118.6604, total=134.2862
INFO 10-04 01:52:43 metrics.py:335] Avg prompt throughput: 15109.9 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 14.3%, CPU KV cache usage: 0.0%, Interval(ms): 135.1
INFO 10-04 01:52:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 134.8369 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 01:52:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 01:52:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1284, fwd=14.2548 cmp_logits=0.0818, sampling=132.9973, total=148.4632
INFO 10-04 01:52:43 metrics.py:335] Avg prompt throughput: 13674.8 tokens/s, Avg generation throughput: 60.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 149.2
INFO 10-04 01:52:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 149.0021 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 01:52:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.2131, fwd=14.3108 cmp_logits=0.0815, sampling=129.7312, total=145.3376
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 13940.9 tokens/s, Avg generation throughput: 88.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 22.6%, CPU KV cache usage: 0.0%, Interval(ms): 146.3
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 146.0826 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.2379, fwd=14.2744 cmp_logits=0.0851, sampling=118.7336, total=134.3319
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 15039.3 tokens/s, Avg generation throughput: 118.2 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 25.5%, CPU KV cache usage: 0.0%, Interval(ms): 135.3
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 135.1097 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.2875, fwd=14.4601 cmp_logits=0.0858, sampling=123.7350, total=139.5695
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 14448.5 tokens/s, Avg generation throughput: 135.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.0%, CPU KV cache usage: 0.0%, Interval(ms): 140.6
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 140.4293 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([841]), positions.shape=torch.Size([841]) hidden_states.shape=torch.Size([841, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9651, fwd=14.1928 cmp_logits=0.0820, sampling=67.3633, total=82.6039
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 9825.7 tokens/s, Avg generation throughput: 239.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.1%, CPU KV cache usage: 0.0%, Interval(ms): 83.7
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 83.4360 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5693, fwd=7.6849 cmp_logits=0.0689, sampling=11.8232, total=20.1473
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.3 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9446 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5527, fwd=7.6294 cmp_logits=0.0675, sampling=11.7059, total=19.9564
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 903.9 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7381 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5538, fwd=7.5872 cmp_logits=0.0682, sampling=11.7426, total=19.9525
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 908.4 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7131 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5453, fwd=7.6282 cmp_logits=0.0672, sampling=11.6975, total=19.9397
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 908.3 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7164 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5217, fwd=7.6432 cmp_logits=0.0799, sampling=11.7490, total=19.9943
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 810.7 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7226 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4995, fwd=7.6337 cmp_logits=0.0679, sampling=11.0877, total=19.2897
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 792.1 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9740 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5107, fwd=7.6294 cmp_logits=0.0675, sampling=11.0836, total=19.2919
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 791.4 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9876 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5107, fwd=7.6776 cmp_logits=0.0682, sampling=11.0316, total=19.2893
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 793.2 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9766 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5007, fwd=7.6101 cmp_logits=0.0684, sampling=11.1158, total=19.2955
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 792.7 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9883 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5076, fwd=7.6122 cmp_logits=0.0687, sampling=11.1234, total=19.3126
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 792.3 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9993 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4973, fwd=7.6296 cmp_logits=0.0682, sampling=11.1341, total=19.3303
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 791.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0200 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4911, fwd=7.7043 cmp_logits=0.0801, sampling=10.9384, total=19.2149
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 744.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8848 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4735, fwd=7.6513 cmp_logits=0.0792, sampling=10.9890, total=19.1939
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 697.8 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8436 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4625, fwd=7.6361 cmp_logits=0.0763, sampling=10.4673, total=18.6429
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 668.0 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.2456 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4523, fwd=7.6387 cmp_logits=0.0689, sampling=10.4415, total=18.6024
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 670.1 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.4
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.2158 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4570, fwd=7.6089 cmp_logits=0.0679, sampling=10.5708, total=18.7054
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 666.3 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3231 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4625, fwd=7.6249 cmp_logits=0.0675, sampling=10.5512, total=18.7070
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 665.4 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3155 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4611, fwd=7.6158 cmp_logits=0.0670, sampling=10.5577, total=18.7025
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 667.0 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3088 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4673, fwd=7.6039 cmp_logits=0.0665, sampling=10.5669, total=18.7058
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 666.9 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3112 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4637, fwd=7.6034 cmp_logits=0.0668, sampling=10.5345, total=18.6694
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 668.1 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.2716 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4575, fwd=7.5793 cmp_logits=0.0670, sampling=10.5777, total=18.6825
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 667.2 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3012 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4442, fwd=7.6046 cmp_logits=0.0803, sampling=10.1984, total=18.3284
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 626.3 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9140 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4435, fwd=7.6134 cmp_logits=0.0665, sampling=10.1690, total=18.2934
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 629.6 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.1
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.8789 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4275, fwd=7.5979 cmp_logits=0.0820, sampling=10.0486, total=18.1568
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 581.3 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7078 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4334, fwd=7.6249 cmp_logits=0.0663, sampling=10.0336, total=18.1589
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 582.6 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7082 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4311, fwd=7.6408 cmp_logits=0.0663, sampling=10.0181, total=18.1575
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 582.5 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7085 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4292, fwd=7.6430 cmp_logits=0.0670, sampling=10.0362, total=18.1761
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 580.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.8%, CPU KV cache usage: 0.0%, Interval(ms): 19.0
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7387 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4210, fwd=7.5846 cmp_logits=0.0889, sampling=9.8810, total=17.9763
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 534.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5049 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3982, fwd=7.6511 cmp_logits=0.0660, sampling=9.4676, total=17.5836
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 492.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0752 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 01:52:44 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3974, fwd=7.6315 cmp_logits=0.0663, sampling=9.4736, total=17.5700
INFO 10-04 01:52:44 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 01:52:44 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0428 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 01:52:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3970, fwd=7.6303 cmp_logits=0.0660, sampling=9.4604, total=17.5550
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0402 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4013, fwd=7.7550 cmp_logits=0.0660, sampling=9.3136, total=17.5369
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0199 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3963, fwd=7.7307 cmp_logits=0.0660, sampling=9.3596, total=17.5536
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0488 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3996, fwd=7.6602 cmp_logits=0.0665, sampling=9.4018, total=17.5290
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 495.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0066 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4077, fwd=7.6239 cmp_logits=0.0753, sampling=9.4388, total=17.5464
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0299 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3989, fwd=7.6206 cmp_logits=0.0660, sampling=9.4464, total=17.5326
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 495.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0092 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3984, fwd=7.6492 cmp_logits=0.0658, sampling=9.4082, total=17.5223
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0006 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3967, fwd=7.6416 cmp_logits=0.0660, sampling=9.4242, total=17.5295
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 495.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0073 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3953, fwd=7.6499 cmp_logits=0.0663, sampling=9.4299, total=17.5424
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 495.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0163 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3939, fwd=7.7946 cmp_logits=0.0663, sampling=9.2852, total=17.5407
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0507 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3946, fwd=7.8280 cmp_logits=0.0691, sampling=8.9438, total=17.2362
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 446.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.6%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7014 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4098, fwd=7.8611 cmp_logits=0.0658, sampling=8.8761, total=17.2138
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 447.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.6%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6880 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3934, fwd=7.9653 cmp_logits=0.0689, sampling=8.7953, total=17.2238
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 448.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6880 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3924, fwd=7.8683 cmp_logits=0.0656, sampling=8.8668, total=17.1940
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 448.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6613 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3960, fwd=7.9646 cmp_logits=0.0689, sampling=8.7993, total=17.2303
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 447.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7104 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3686, fwd=7.9420 cmp_logits=0.0701, sampling=8.4610, total=16.8426
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3097 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3805, fwd=8.0400 cmp_logits=0.0687, sampling=8.3477, total=16.8376
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3192 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3822, fwd=8.0338 cmp_logits=0.0689, sampling=8.3416, total=16.8276
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2632 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3817, fwd=7.9532 cmp_logits=0.0677, sampling=8.4329, total=16.8364
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2777 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3746, fwd=8.0204 cmp_logits=0.0682, sampling=8.3876, total=16.8517
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3047 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3631, fwd=8.0786 cmp_logits=0.0830, sampling=7.9560, total=16.4821
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.9%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9454 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3502, fwd=8.0862 cmp_logits=0.0834, sampling=7.6387, total=16.1591
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5269 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3574, fwd=7.9153 cmp_logits=0.0761, sampling=7.7963, total=16.1455
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5343 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3459, fwd=8.0061 cmp_logits=0.0689, sampling=7.6857, total=16.1076
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.0 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5021 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3293, fwd=8.0247 cmp_logits=0.0844, sampling=7.1540, total=15.5933
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9452 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3138, fwd=7.9684 cmp_logits=0.0679, sampling=7.0441, total=15.3954
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7332 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3185, fwd=8.0173 cmp_logits=0.0687, sampling=6.9764, total=15.3818
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6801 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3095, fwd=8.0125 cmp_logits=0.0677, sampling=6.9666, total=15.3573
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6553 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3107, fwd=8.0001 cmp_logits=0.0672, sampling=7.0012, total=15.3801
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6760 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3076, fwd=7.9479 cmp_logits=0.0672, sampling=7.0317, total=15.3553
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6579 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3104, fwd=7.7434 cmp_logits=0.0672, sampling=7.2289, total=15.3513
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6786 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3111, fwd=7.7944 cmp_logits=0.0672, sampling=7.1881, total=15.3618
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6438 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3018, fwd=7.7994 cmp_logits=0.0660, sampling=7.1929, total=15.3606
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6546 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3073, fwd=7.6492 cmp_logits=0.0656, sampling=7.3254, total=15.3484
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6319 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2999, fwd=7.6516 cmp_logits=0.0646, sampling=7.3314, total=15.3487
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6319 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2968, fwd=7.6511 cmp_logits=0.0658, sampling=7.3264, total=15.3408
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6589 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3030, fwd=7.6292 cmp_logits=0.0660, sampling=7.3636, total=15.3627
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6472 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3014, fwd=7.6416 cmp_logits=0.0653, sampling=7.3326, total=15.3418
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6264 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2975, fwd=7.6249 cmp_logits=0.0648, sampling=7.3426, total=15.3308
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6362 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2799, fwd=7.6699 cmp_logits=0.0789, sampling=7.1354, total=15.1646
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4080 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2794, fwd=7.6196 cmp_logits=0.0644, sampling=7.1809, total=15.1453
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4245 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2840, fwd=7.6182 cmp_logits=0.0656, sampling=7.1898, total=15.1584
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4068 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2816, fwd=7.6284 cmp_logits=0.0641, sampling=7.1774, total=15.1525
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3997 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2806, fwd=7.6470 cmp_logits=0.0648, sampling=7.1621, total=15.1556
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4316 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2582, fwd=7.7960 cmp_logits=0.0660, sampling=7.1659, total=15.2872
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4872 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2699, fwd=7.8461 cmp_logits=0.0665, sampling=7.1192, total=15.3024
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5358 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2596, fwd=7.8058 cmp_logits=0.0653, sampling=7.1814, total=15.3129
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5146 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2580, fwd=7.8254 cmp_logits=0.0663, sampling=7.1304, total=15.2807
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4798 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2577, fwd=7.8087 cmp_logits=0.0660, sampling=7.1571, total=15.2903
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4898 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2589, fwd=7.7932 cmp_logits=0.0668, sampling=7.2374, total=15.3573
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5599 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2587, fwd=7.7386 cmp_logits=0.0653, sampling=7.2980, total=15.3613
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5945 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2565, fwd=7.7982 cmp_logits=0.0656, sampling=7.2265, total=15.3480
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5473 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2606, fwd=7.7891 cmp_logits=0.0660, sampling=7.2255, total=15.3422
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5413 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 01:52:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 01:52:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 01:52:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2568, fwd=7.7512 cmp_logits=0.0656, sampling=7.2656, total=15.3399
INFO 10-04 01:52:45 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 01:52:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5644 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.1645787, last_token_time=1728006764.418973, first_scheduled_time=1728006760.1949272, first_token_time=1728006761.9125056, time_in_queue=0.030348539352416992, finished_time=1728006764.4188902, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.1655405, last_token_time=1728006764.952603, first_scheduled_time=1728006760.1949272, first_token_time=1728006761.9125056, time_in_queue=0.02938675880432129, finished_time=1728006764.9525595, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.1674562, last_token_time=1728006764.6441267, first_scheduled_time=1728006760.1949272, first_token_time=1728006761.9125056, time_in_queue=0.027471065521240234, finished_time=1728006764.644071, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.1682243, last_token_time=1728006765.3649805, first_scheduled_time=1728006760.1949272, first_token_time=1728006763.6073985, time_in_queue=0.026702880859375, finished_time=1728006765.364953, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.1704135, last_token_time=1728006765.347829, first_scheduled_time=1728006761.9131103, first_token_time=1728006763.6073985, time_in_queue=1.742696762084961, finished_time=1728006765.3478043, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.1726675, last_token_time=1728006764.4818285, first_scheduled_time=1728006761.9131103, first_token_time=1728006763.6073985, time_in_queue=1.7404427528381348, finished_time=1728006764.481772, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.173123, last_token_time=1728006764.4818285, first_scheduled_time=1728006761.9131103, first_token_time=1728006763.6073985, time_in_queue=1.7399873733520508, finished_time=1728006764.4817762, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.1735756, last_token_time=1728006765.4312222, first_scheduled_time=1728006761.9131103, first_token_time=1728006763.742538, time_in_queue=1.739534616470337, finished_time=1728006765.4312034, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.1758249, last_token_time=1728006765.887119, first_scheduled_time=1728006763.6081073, first_token_time=1728006763.891688, time_in_queue=3.4322824478149414, finished_time=1728006765.8871164, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.1792712, last_token_time=1728006765.6525803, first_scheduled_time=1728006763.7430828, first_token_time=1728006764.037846, time_in_queue=3.5638115406036377, finished_time=1728006765.6525733, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.182034, last_token_time=1728006764.623984, first_scheduled_time=1728006763.8923464, first_token_time=1728006764.037846, time_in_queue=3.7103123664855957, finished_time=1728006764.6239505, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.1825979, last_token_time=1728006764.8200276, first_scheduled_time=1728006763.8923464, first_token_time=1728006764.037846, time_in_queue=3.7097485065460205, finished_time=1728006764.8199997, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.1834762, last_token_time=1728006765.1710904, first_scheduled_time=1728006763.8923464, first_token_time=1728006764.037846, time_in_queue=3.7088701725006104, finished_time=1728006765.1710758, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.185048, last_token_time=1728006765.2604733, first_scheduled_time=1728006763.8923464, first_token_time=1728006764.1730988, time_in_queue=3.7072982788085938, finished_time=1728006765.2604635, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.18675, last_token_time=1728006764.6641905, first_scheduled_time=1728006764.0385945, first_token_time=1728006764.1730988, time_in_queue=3.851844549179077, finished_time=1728006764.6641717, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.1873217, last_token_time=1728006764.9338915, first_scheduled_time=1728006764.0385945, first_token_time=1728006764.1730988, time_in_queue=3.8512728214263916, finished_time=1728006764.9338813, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.1883712, last_token_time=1728006764.8582456, first_scheduled_time=1728006764.0385945, first_token_time=1728006764.3136976, time_in_queue=3.8502233028411865, finished_time=1728006764.8582351, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.1892605, last_token_time=1728006764.5027983, first_scheduled_time=1728006764.1739404, first_token_time=1728006764.3136976, time_in_queue=3.984679937362671, finished_time=1728006764.5027883, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.1895332, last_token_time=1728006765.4150689, first_scheduled_time=1728006764.1739404, first_token_time=1728006764.3136976, time_in_queue=3.9844071865081787, finished_time=1728006765.4150622, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728006760.191709, last_token_time=1728006765.7304044, first_scheduled_time=1728006764.1739404, first_token_time=1728006764.3973262, time_in_queue=3.982231378555298, finished_time=1728006765.7304025, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 15868
End-to-End latency: 5.72 seconds
Throughput: 3.49 requests/s, 2772.79 tokens/s
Per_token_time: 0.361 ms

Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=4096, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=4096, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=4096, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=1585, output_len=80, sequence_len=1665...
User Requests: prompt_len=970, output_len=49, sequence_len=1019...
User Requests: prompt_len=2398, output_len=120, sequence_len=2518...
User Requests: prompt_len=2366, output_len=119, sequence_len=2485...
User Requests: prompt_len=652, output_len=33, sequence_len=685...
User Requests: prompt_len=658, output_len=33, sequence_len=691...
User Requests: prompt_len=2495, output_len=125, sequence_len=2620...
User Requests: prompt_len=3566, output_len=179, sequence_len=3745...
User Requests: prompt_len=2969, output_len=149, sequence_len=3118...
Selected required requests 10
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=1585, output_len=80, sequence_len=1665...
User Requests: prompt_len=970, output_len=49, sequence_len=1019...
User Requests: prompt_len=2398, output_len=120, sequence_len=2518...
User Requests: prompt_len=2366, output_len=119, sequence_len=2485...
User Requests: prompt_len=652, output_len=33, sequence_len=685...
User Requests: prompt_len=658, output_len=33, sequence_len=691...
User Requests: prompt_len=2495, output_len=125, sequence_len=2620...
User Requests: prompt_len=3566, output_len=179, sequence_len=3745...
User Requests: prompt_len=2969, output_len=149, sequence_len=3118...
Chunked prefill is enabled. max_num_batched_tokens=4096, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'finetuned', 'type', 'original_max_position_embeddings'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 02:38:35 config.py:654] [SchedulerConfig] max_num_batched_tokens: 4096 chunked_prefill_enabled: True
INFO 10-04 02:38:35 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 02:38:36 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 02:38:41 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 02:38:45 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 02:38:45 model_runner.py:183] Loaded model: 
INFO 10-04 02:38:45 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 02:38:45 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 02:38:45 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 02:38:45 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 02:38:45 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 02:38:45 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 02:38:45 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 02:38:45 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 02:38:45 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 02:38:45 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 02:38:45 model_runner.py:183]         )
INFO 10-04 02:38:45 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 02:38:45 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 02:38:45 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 02:38:45 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 02:38:45 model_runner.py:183]         )
INFO 10-04 02:38:45 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:38:45 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:38:45 model_runner.py:183]       )
INFO 10-04 02:38:45 model_runner.py:183]     )
INFO 10-04 02:38:45 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:38:45 model_runner.py:183]   )
INFO 10-04 02:38:45 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 02:38:45 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 02:38:45 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 02:38:45 model_runner.py:183] )
INFO 10-04 02:38:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 02:38:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 10.6294, fwd=372.9541 cmp_logits=0.2356, sampling=261.1709, total=644.9916
INFO 10-04 02:38:45 worker.py:164] Peak: 13.385 GB, Initial: 38.980 GB, Free: 25.595 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.039 GB
INFO 10-04 02:38:45 gpu_executor.py:117] # GPU blocks: 3077, # CPU blocks: 8192
INFO 10-04 02:39:17 worker.py:189] _init_cache_engine took 24.0625 GB
INFO 10-04 02:39:17 scheduler.py:307] Scheduler initialized with prompt limit: 4096
INFO 10-04 02:39:17 Start warmup...
INFO 10-04 02:39:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:39:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9224, fwd=2935.7390 cmp_logits=71.3832, sampling=0.9577, total=3009.0048
INFO 10-04 02:39:20 metrics.py:335] Avg prompt throughput: 339.5 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 3016.2
INFO 10-04 02:39:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 3009.5446 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3042, fwd=43.1149 cmp_logits=0.1161, sampling=6.6803, total=50.2174
INFO 10-04 02:39:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 51.1
INFO 10-04 02:39:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.6573 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3068, fwd=8.0588 cmp_logits=0.0687, sampling=7.1251, total=15.5611
INFO 10-04 02:39:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 02:39:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8491 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2973, fwd=7.9081 cmp_logits=0.0696, sampling=7.2582, total=15.5349
INFO 10-04 02:39:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 02:39:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7659 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2685, fwd=7.8566 cmp_logits=0.0687, sampling=7.3121, total=15.5077
INFO 10-04 02:39:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 02:39:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7285 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2677, fwd=7.9005 cmp_logits=0.0687, sampling=7.2391, total=15.4772
INFO 10-04 02:39:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:39:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6925 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2744, fwd=7.9055 cmp_logits=0.0727, sampling=7.2360, total=15.4901
INFO 10-04 02:39:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 02:39:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7132 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2680, fwd=7.8676 cmp_logits=0.0679, sampling=7.2668, total=15.4715
INFO 10-04 02:39:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 02:39:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7144 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2639, fwd=7.8313 cmp_logits=0.0658, sampling=7.3361, total=15.4982
INFO 10-04 02:39:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:39:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6946 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2649, fwd=7.9148 cmp_logits=0.0660, sampling=7.2422, total=15.4891
INFO 10-04 02:39:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:39:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7039 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:25 Start benchmarking...
INFO 10-04 02:39:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 02:39:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.7695, fwd=1588.5034 cmp_logits=0.1478, sampling=279.5353, total=1869.9572
INFO 10-04 02:39:27 metrics.py:335] Avg prompt throughput: 2137.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 11.3%, CPU KV cache usage: 0.0%, Interval(ms): 1916.7
INFO 10-04 02:39:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1870.6498 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 02:39:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 02:39:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.6763, fwd=14.2732 cmp_logits=0.0865, sampling=277.9050, total=293.9425
INFO 10-04 02:39:27 metrics.py:335] Avg prompt throughput: 13887.3 tokens/s, Avg generation throughput: 17.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 294.7
INFO 10-04 02:39:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 294.4691 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 02:39:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.6580, fwd=14.1683 cmp_logits=0.0834, sampling=259.8357, total=275.7466
INFO 10-04 02:39:27 metrics.py:335] Avg prompt throughput: 14793.7 tokens/s, Avg generation throughput: 28.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 31.1%, CPU KV cache usage: 0.0%, Interval(ms): 276.5
INFO 10-04 02:39:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 276.3681 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:39:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 02:39:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.6654, fwd=14.3714 cmp_logits=0.0846, sampling=278.8780, total=295.0003
INFO 10-04 02:39:27 metrics.py:335] Avg prompt throughput: 13821.1 tokens/s, Avg generation throughput: 30.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.1%, CPU KV cache usage: 0.0%, Interval(ms): 295.8
INFO 10-04 02:39:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 295.6016 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1864]), positions.shape=torch.Size([1864]) hidden_states.shape=torch.Size([1864, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1294, fwd=14.4835 cmp_logits=0.0849, sampling=145.6547, total=161.3533
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 11442.8 tokens/s, Avg generation throughput: 61.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.2%, CPU KV cache usage: 0.0%, Interval(ms): 162.1
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 161.9220 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4661, fwd=7.8568 cmp_logits=0.0684, sampling=12.6224, total=21.0145
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5638 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4730, fwd=7.7558 cmp_logits=0.0677, sampling=12.6755, total=20.9727
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5204 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4697, fwd=7.7171 cmp_logits=0.0679, sampling=12.7473, total=21.0032
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5421 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4649, fwd=7.7302 cmp_logits=0.0679, sampling=12.7282, total=20.9920
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5302 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4702, fwd=7.8037 cmp_logits=0.0691, sampling=12.6476, total=20.9916
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5681 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4618, fwd=7.7834 cmp_logits=0.0668, sampling=12.6803, total=20.9932
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5287 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4680, fwd=7.8044 cmp_logits=0.0675, sampling=12.6734, total=21.0140
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5580 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4618, fwd=7.7720 cmp_logits=0.0677, sampling=12.8143, total=21.1167
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 457.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6684 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4623, fwd=7.7364 cmp_logits=0.0679, sampling=12.7454, total=21.0130
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5590 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4594, fwd=7.7491 cmp_logits=0.0677, sampling=12.7089, total=20.9861
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5259 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4680, fwd=7.8411 cmp_logits=0.0679, sampling=12.6438, total=21.0218
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5921 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4678, fwd=7.7744 cmp_logits=0.0675, sampling=12.6655, total=20.9761
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 461.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5192 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4578, fwd=7.7744 cmp_logits=0.0668, sampling=12.7091, total=21.0092
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5459 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4606, fwd=7.7245 cmp_logits=0.0677, sampling=12.7637, total=21.0173
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5497 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4587, fwd=7.7646 cmp_logits=0.0677, sampling=12.7184, total=21.0106
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5528 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4680, fwd=7.8979 cmp_logits=0.0687, sampling=12.6243, total=21.0600
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 458.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6527 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4637, fwd=7.8421 cmp_logits=0.0682, sampling=12.6364, total=21.0116
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5471 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4623, fwd=7.8032 cmp_logits=0.0682, sampling=12.7831, total=21.1182
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 458.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6534 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4630, fwd=7.7300 cmp_logits=0.0687, sampling=12.8269, total=21.0893
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 458.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6331 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4644, fwd=7.7143 cmp_logits=0.0670, sampling=12.8639, total=21.1105
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 458.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6465 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4692, fwd=7.7031 cmp_logits=0.0677, sampling=12.8636, total=21.1046
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 457.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.9
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6813 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4632, fwd=7.8101 cmp_logits=0.0668, sampling=12.7630, total=21.1041
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 458.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6401 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4623, fwd=7.7429 cmp_logits=0.0663, sampling=12.8825, total=21.1551
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 457.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.9
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6904 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4587, fwd=7.7240 cmp_logits=0.0687, sampling=12.8505, total=21.1031
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 458.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6599 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4473, fwd=7.8402 cmp_logits=0.0679, sampling=12.4328, total=20.7891
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 418.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2984 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4437, fwd=7.7715 cmp_logits=0.0665, sampling=12.5337, total=20.8163
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 418.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.3335 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4456, fwd=7.7507 cmp_logits=0.0694, sampling=12.5115, total=20.7787
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2920 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4599, fwd=7.7534 cmp_logits=0.0737, sampling=12.5124, total=20.8006
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.3106 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4461, fwd=7.7324 cmp_logits=0.0677, sampling=12.5492, total=20.7961
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.3051 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4494, fwd=7.7150 cmp_logits=0.0668, sampling=12.5756, total=20.8077
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 418.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.3459 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4058, fwd=7.8101 cmp_logits=0.0799, sampling=11.9309, total=20.2272
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 334.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6811 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4015, fwd=7.7229 cmp_logits=0.0668, sampling=12.0013, total=20.1931
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6339 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3994, fwd=7.7455 cmp_logits=0.0670, sampling=11.9462, total=20.1590
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6029 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4027, fwd=7.7019 cmp_logits=0.0672, sampling=12.0497, total=20.2224
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6730 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4096, fwd=7.7486 cmp_logits=0.0675, sampling=12.0008, total=20.2277
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6733 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4065, fwd=7.7720 cmp_logits=0.0670, sampling=11.9715, total=20.2177
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 335.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6778 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4046, fwd=7.7431 cmp_logits=0.0675, sampling=11.9932, total=20.2091
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6625 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4048, fwd=7.6973 cmp_logits=0.0672, sampling=12.0199, total=20.1902
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6354 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:39:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3986, fwd=7.7477 cmp_logits=0.0670, sampling=11.9846, total=20.1988
INFO 10-04 02:39:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:39:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6463 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:39:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4041, fwd=7.7074 cmp_logits=0.0663, sampling=12.0292, total=20.2081
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6645 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4020, fwd=7.8111 cmp_logits=0.0670, sampling=11.9383, total=20.2191
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6680 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4106, fwd=7.7605 cmp_logits=0.0675, sampling=11.9655, total=20.2050
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6556 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4039, fwd=7.7603 cmp_logits=0.0658, sampling=11.9846, total=20.2157
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6659 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4013, fwd=7.8311 cmp_logits=0.0668, sampling=11.9002, total=20.2003
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6659 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3924, fwd=7.7848 cmp_logits=0.0896, sampling=11.6777, total=19.9459
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 291.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3598 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3831, fwd=7.7081 cmp_logits=0.0656, sampling=11.8053, total=19.9628
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3846 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3774, fwd=7.7055 cmp_logits=0.0663, sampling=11.7774, total=19.9277
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3557 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3860, fwd=7.8166 cmp_logits=0.0668, sampling=11.6813, total=19.9513
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3736 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3846, fwd=7.7422 cmp_logits=0.0682, sampling=11.7323, total=19.9282
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3519 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3846, fwd=7.7217 cmp_logits=0.0663, sampling=11.7209, total=19.8941
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3149 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3827, fwd=7.7517 cmp_logits=0.0670, sampling=11.7457, total=19.9478
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3691 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3858, fwd=7.7686 cmp_logits=0.0660, sampling=11.7004, total=19.9218
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3378 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3901, fwd=7.7710 cmp_logits=0.0668, sampling=11.7073, total=19.9358
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3729 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3827, fwd=7.8006 cmp_logits=0.0660, sampling=11.6773, total=19.9275
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3471 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3912, fwd=7.8454 cmp_logits=0.0658, sampling=11.6398, total=19.9432
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 291.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.4217 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3932, fwd=7.7858 cmp_logits=0.0672, sampling=11.7140, total=19.9614
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3812 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3850, fwd=7.7739 cmp_logits=0.0665, sampling=11.6808, total=19.9072
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3216 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3841, fwd=7.8681 cmp_logits=0.0665, sampling=11.5957, total=19.9151
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3259 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3815, fwd=7.7579 cmp_logits=0.0670, sampling=11.6715, total=19.8789
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2956 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3881, fwd=7.8270 cmp_logits=0.0665, sampling=11.7059, total=19.9885
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 291.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.4034 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3905, fwd=7.8554 cmp_logits=0.0663, sampling=11.6670, total=19.9802
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 291.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.4055 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3860, fwd=7.7684 cmp_logits=0.0663, sampling=11.6961, total=19.9180
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3359 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3858, fwd=7.8406 cmp_logits=0.0663, sampling=11.6096, total=19.9032
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3252 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3848, fwd=7.8135 cmp_logits=0.0672, sampling=11.6618, total=19.9280
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3457 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3848, fwd=7.8433 cmp_logits=0.0665, sampling=11.6386, total=19.9339
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3507 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3848, fwd=7.7806 cmp_logits=0.0672, sampling=11.7354, total=19.9690
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3848 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3867, fwd=7.7763 cmp_logits=0.0663, sampling=11.7240, total=19.9542
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3700 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3912, fwd=7.7317 cmp_logits=0.0670, sampling=11.8048, total=19.9959
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 291.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.4096 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4101, fwd=7.7937 cmp_logits=0.0672, sampling=11.7471, total=20.0193
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 291.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.4356 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3836, fwd=7.8270 cmp_logits=0.0663, sampling=11.6882, total=19.9659
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3767 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3855, fwd=7.8006 cmp_logits=0.0668, sampling=11.7035, total=19.9573
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 291.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.4155 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3834, fwd=7.7736 cmp_logits=0.0660, sampling=11.7419, total=19.9661
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3810 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3827, fwd=7.8013 cmp_logits=0.0658, sampling=11.6901, total=19.9411
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3593 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3829, fwd=7.7255 cmp_logits=0.0660, sampling=11.7674, total=19.9428
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3533 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3805, fwd=7.7412 cmp_logits=0.0660, sampling=11.8039, total=19.9926
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 291.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.4363 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3653, fwd=7.8170 cmp_logits=0.0658, sampling=11.4329, total=19.6819
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 246.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.3
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.1111 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3645, fwd=7.7381 cmp_logits=0.0675, sampling=11.4985, total=19.6693
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0474 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3617, fwd=7.7288 cmp_logits=0.0653, sampling=11.5373, total=19.6943
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0679 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3617, fwd=7.7119 cmp_logits=0.0656, sampling=11.5106, total=19.6507
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0295 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3591, fwd=7.6830 cmp_logits=0.0656, sampling=11.5576, total=19.6662
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0636 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3688, fwd=7.7059 cmp_logits=0.0653, sampling=11.5280, total=19.6691
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 246.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.3
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0920 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3638, fwd=7.7846 cmp_logits=0.0668, sampling=11.3974, total=19.6137
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9986 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3617, fwd=7.7262 cmp_logits=0.0656, sampling=11.4636, total=19.6180
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9983 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3629, fwd=7.7024 cmp_logits=0.0660, sampling=11.4856, total=19.6178
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0000 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3624, fwd=7.7615 cmp_logits=0.0668, sampling=11.4131, total=19.6049
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9902 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3703, fwd=7.7503 cmp_logits=0.0670, sampling=11.4145, total=19.6030
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0219 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3636, fwd=7.7767 cmp_logits=0.0665, sampling=11.4005, total=19.6083
INFO 10-04 02:39:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:39:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9938 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3660, fwd=7.7112 cmp_logits=0.0660, sampling=11.4720, total=19.6159
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9993 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3638, fwd=7.7605 cmp_logits=0.0670, sampling=11.4293, total=19.6218
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0031 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3631, fwd=7.8113 cmp_logits=0.0656, sampling=11.3766, total=19.6176
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9988 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3655, fwd=7.7765 cmp_logits=0.0658, sampling=11.4083, total=19.6171
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0398 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3653, fwd=7.8115 cmp_logits=0.0656, sampling=11.3609, total=19.6042
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9838 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3636, fwd=7.7848 cmp_logits=0.0660, sampling=11.4207, total=19.6362
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0176 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3660, fwd=7.7608 cmp_logits=0.0660, sampling=11.4233, total=19.6173
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9959 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3650, fwd=7.7713 cmp_logits=0.0665, sampling=11.4307, total=19.6342
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0176 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3762, fwd=7.7345 cmp_logits=0.0656, sampling=11.4462, total=19.6238
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0458 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3712, fwd=7.7143 cmp_logits=0.0732, sampling=11.4713, total=19.6307
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0102 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3622, fwd=7.8104 cmp_logits=0.0656, sampling=12.3544, total=20.5934
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 235.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1010 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3703, fwd=7.8290 cmp_logits=0.0670, sampling=11.4071, total=19.6743
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0617 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3910, fwd=7.8096 cmp_logits=0.0663, sampling=11.3332, total=19.6011
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9811 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3648, fwd=7.7324 cmp_logits=0.0665, sampling=11.4450, total=19.6097
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9988 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3624, fwd=7.7164 cmp_logits=0.0668, sampling=11.4865, total=19.6331
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0136 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3619, fwd=7.7372 cmp_logits=0.0665, sampling=11.4105, total=19.5770
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9628 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3669, fwd=7.7906 cmp_logits=0.0658, sampling=11.3690, total=19.5932
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9749 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3660, fwd=7.7865 cmp_logits=0.0656, sampling=11.3752, total=19.5942
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9771 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3653, fwd=7.9467 cmp_logits=0.0677, sampling=11.2181, total=19.5985
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0207 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3729, fwd=7.8292 cmp_logits=0.0658, sampling=11.3335, total=19.6023
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9857 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3574, fwd=7.7584 cmp_logits=0.0656, sampling=11.4076, total=19.5899
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9702 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3648, fwd=7.7119 cmp_logits=0.0660, sampling=11.4763, total=19.6199
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9997 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3583, fwd=7.6993 cmp_logits=0.0660, sampling=11.4751, total=19.5997
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9766 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3648, fwd=7.7507 cmp_logits=0.0672, sampling=11.3995, total=19.5832
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9740 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3622, fwd=7.7465 cmp_logits=0.0665, sampling=11.4496, total=19.6261
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0331 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3614, fwd=7.7622 cmp_logits=0.0653, sampling=11.4119, total=19.6018
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9819 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3605, fwd=7.7691 cmp_logits=0.0668, sampling=11.4307, total=19.6280
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0059 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3774, fwd=7.7660 cmp_logits=0.0665, sampling=11.4102, total=19.6211
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.6 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0398 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3467, fwd=7.7710 cmp_logits=0.0787, sampling=10.4096, total=18.6067
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9958 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3190, fwd=7.7972 cmp_logits=0.0653, sampling=9.9421, total=18.1248
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4290 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3140, fwd=7.6983 cmp_logits=0.0653, sampling=10.0222, total=18.1007
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4047 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3247, fwd=7.7074 cmp_logits=0.0651, sampling=9.9983, total=18.0967
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4186 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3183, fwd=7.7209 cmp_logits=0.0651, sampling=10.0563, total=18.1615
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4624 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3211, fwd=7.7343 cmp_logits=0.0651, sampling=9.9981, total=18.1198
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4262 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3216, fwd=7.7255 cmp_logits=0.0651, sampling=10.0091, total=18.1222
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4584 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2997, fwd=7.7612 cmp_logits=0.0784, sampling=9.7079, total=17.8483
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1043 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2933, fwd=7.7422 cmp_logits=0.0646, sampling=9.7311, total=17.8325
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0881 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2935, fwd=7.7615 cmp_logits=0.0651, sampling=9.7075, total=17.8282
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0845 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3240, fwd=7.7415 cmp_logits=0.0653, sampling=9.7399, total=17.8716
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1360 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3006, fwd=7.6952 cmp_logits=0.0648, sampling=9.7716, total=17.8332
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0869 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2909, fwd=7.7281 cmp_logits=0.0648, sampling=9.7446, total=17.8294
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0824 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2916, fwd=7.6854 cmp_logits=0.0653, sampling=9.7992, total=17.8425
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0991 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3107, fwd=7.7333 cmp_logits=0.0653, sampling=9.7711, total=17.8814
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1365 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2983, fwd=7.6766 cmp_logits=0.0653, sampling=9.8059, total=17.8471
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1127 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2918, fwd=7.7643 cmp_logits=0.0651, sampling=9.7136, total=17.8361
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0902 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2935, fwd=7.7889 cmp_logits=0.0648, sampling=9.6915, total=17.8399
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0972 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2961, fwd=7.7391 cmp_logits=0.0644, sampling=9.7477, total=17.8483
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1043 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3169, fwd=7.7457 cmp_logits=0.0656, sampling=9.7201, total=17.8494
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1055 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2949, fwd=7.7660 cmp_logits=0.0653, sampling=9.7146, total=17.8421
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1088 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2928, fwd=7.6821 cmp_logits=0.0648, sampling=9.8042, total=17.8452
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1022 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2933, fwd=7.7226 cmp_logits=0.0656, sampling=9.7673, total=17.8494
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1050 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2930, fwd=7.7050 cmp_logits=0.0646, sampling=9.7587, total=17.8223
INFO 10-04 02:39:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:39:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0786 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2952, fwd=7.7417 cmp_logits=0.0660, sampling=9.7392, total=17.8432
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0991 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2975, fwd=7.7138 cmp_logits=0.0653, sampling=9.7694, total=17.8475
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1105 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2906, fwd=7.7548 cmp_logits=0.0648, sampling=9.7301, total=17.8413
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0967 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2999, fwd=7.8268 cmp_logits=0.0653, sampling=9.6655, total=17.8585
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1162 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2928, fwd=7.7672 cmp_logits=0.0656, sampling=9.7156, total=17.8423
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0969 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2954, fwd=7.7620 cmp_logits=0.0653, sampling=9.7287, total=17.8525
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1243 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2949, fwd=7.7112 cmp_logits=0.0648, sampling=9.8598, total=17.9315
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1911 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2916, fwd=7.8003 cmp_logits=0.0653, sampling=9.7926, total=17.9508
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.2076 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2928, fwd=7.8185 cmp_logits=0.0658, sampling=9.7508, total=17.9288
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.2264 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2675, fwd=8.0152 cmp_logits=0.0672, sampling=9.6035, total=17.9541
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1608 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2732, fwd=7.9186 cmp_logits=0.0653, sampling=9.6874, total=17.9458
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1477 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2668, fwd=7.9343 cmp_logits=0.0780, sampling=9.6784, total=17.9586
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1653 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2642, fwd=7.9191 cmp_logits=0.0665, sampling=9.7058, total=17.9565
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1599 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2663, fwd=7.8931 cmp_logits=0.0665, sampling=9.7075, total=17.9343
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1372 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2642, fwd=7.9515 cmp_logits=0.0658, sampling=9.6714, total=17.9539
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1563 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2675, fwd=7.8549 cmp_logits=0.0663, sampling=9.7554, total=17.9451
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1475 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2725, fwd=7.8747 cmp_logits=0.0663, sampling=9.7458, total=17.9601
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1661 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2666, fwd=7.8430 cmp_logits=0.0653, sampling=9.7594, total=17.9355
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1441 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2656, fwd=7.9348 cmp_logits=0.0660, sampling=9.6762, total=17.9439
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1491 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2680, fwd=7.8828 cmp_logits=0.0658, sampling=9.7136, total=17.9315
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1360 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2639, fwd=7.8652 cmp_logits=0.0660, sampling=9.7554, total=17.9517
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1515 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2658, fwd=7.9429 cmp_logits=0.0663, sampling=9.6664, total=17.9424
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1487 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2654, fwd=7.8981 cmp_logits=0.0665, sampling=9.7108, total=17.9417
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1451 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2646, fwd=7.9076 cmp_logits=0.0656, sampling=9.7127, total=17.9517
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1553 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2794, fwd=7.9021 cmp_logits=0.0658, sampling=9.7258, total=17.9739
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1830 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2661, fwd=7.9687 cmp_logits=0.0663, sampling=9.6493, total=17.9513
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1530 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2675, fwd=7.8986 cmp_logits=0.0653, sampling=9.7148, total=17.9474
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1530 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2677, fwd=7.9198 cmp_logits=0.0656, sampling=9.6989, total=17.9529
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1575 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2656, fwd=7.8840 cmp_logits=0.0665, sampling=9.7578, total=17.9751
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1797 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2635, fwd=7.8838 cmp_logits=0.0675, sampling=9.7346, total=17.9503
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1525 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2649, fwd=7.8559 cmp_logits=0.0660, sampling=9.7389, total=17.9267
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1305 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2675, fwd=7.8692 cmp_logits=0.0663, sampling=9.7435, total=17.9474
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1546 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2663, fwd=7.9141 cmp_logits=0.0663, sampling=9.7291, total=17.9768
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1813 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2651, fwd=7.9241 cmp_logits=0.0663, sampling=9.6848, total=17.9412
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1432 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2654, fwd=7.9350 cmp_logits=0.0668, sampling=9.6855, total=17.9536
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1561 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2708, fwd=7.8902 cmp_logits=0.0660, sampling=9.7265, total=17.9546
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1565 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2670, fwd=7.8862 cmp_logits=0.0672, sampling=9.7578, total=17.9791
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1842 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:39:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:39:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:39:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2656, fwd=8.0283 cmp_logits=0.0787, sampling=9.5868, total=17.9603
INFO 10-04 02:39:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 02:39:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.2204 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009565.2247717, last_token_time=1728009568.6829712, first_scheduled_time=1728009565.2610512, first_token_time=1728009567.1311963, time_in_queue=0.03627943992614746, finished_time=1728009568.6829298, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009565.226615, last_token_time=1728009569.7398398, first_scheduled_time=1728009565.2610512, first_token_time=1728009567.1311963, time_in_queue=0.03443622589111328, finished_time=1728009569.7398133, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009565.2298946, last_token_time=1728009569.103486, first_scheduled_time=1728009565.2610512, first_token_time=1728009567.1311963, time_in_queue=0.031156539916992188, finished_time=1728009569.1034617, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009565.231627, last_token_time=1728009570.566987, first_scheduled_time=1728009565.2610512, first_token_time=1728009567.4259293, time_in_queue=0.029424190521240234, finished_time=1728009570.5669696, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009565.2364368, last_token_time=1728009570.5477571, first_scheduled_time=1728009567.1318371, first_token_time=1728009567.4259293, time_in_queue=1.8954002857208252, finished_time=1728009570.5477421, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009565.2408042, last_token_time=1728009568.8119166, first_scheduled_time=1728009567.1318371, first_token_time=1728009567.7023947, time_in_queue=1.8910329341888428, finished_time=1728009568.8118978, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009565.2420125, last_token_time=1728009568.8119166, first_scheduled_time=1728009567.4264958, first_token_time=1728009567.7023947, time_in_queue=2.184483289718628, finished_time=1728009568.8119025, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009565.243184, last_token_time=1728009570.6785421, first_scheduled_time=1728009567.4264958, first_token_time=1728009567.7023947, time_in_queue=2.183311700820923, finished_time=1728009570.6785288, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009565.247925, last_token_time=1728009571.6841867, first_scheduled_time=1728009567.4264958, first_token_time=1728009567.9981518, time_in_queue=2.1785707473754883, finished_time=1728009571.6841843, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728009565.2550416, last_token_time=1728009571.1534326, first_scheduled_time=1728009567.7029936, first_token_time=1728009568.1602583, time_in_queue=2.4479520320892334, finished_time=1728009571.1534307, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 19139
End-to-End latency: 6.46 seconds
Throughput: 1.55 requests/s, 2962.86 tokens/s
Per_token_time: 0.338 ms

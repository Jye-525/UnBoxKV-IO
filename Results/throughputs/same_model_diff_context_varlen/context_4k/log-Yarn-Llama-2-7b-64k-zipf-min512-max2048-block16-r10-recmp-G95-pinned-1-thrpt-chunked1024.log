Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=4096, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=1024, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Selected required requests 10
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Chunked prefill is enabled. max_num_batched_tokens=1024, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings', 'finetuned', 'type'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 02:06:15 config.py:654] [SchedulerConfig] max_num_batched_tokens: 1024 chunked_prefill_enabled: True
INFO 10-04 02:06:15 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 02:06:16 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 02:06:20 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 02:06:41 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 02:06:41 model_runner.py:183] Loaded model: 
INFO 10-04 02:06:41 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 02:06:41 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 02:06:41 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 02:06:41 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 02:06:41 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 02:06:41 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 02:06:41 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 02:06:41 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 02:06:41 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 02:06:41 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 02:06:41 model_runner.py:183]         )
INFO 10-04 02:06:41 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 02:06:41 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 02:06:41 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 02:06:41 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 02:06:41 model_runner.py:183]         )
INFO 10-04 02:06:41 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:06:41 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:06:41 model_runner.py:183]       )
INFO 10-04 02:06:41 model_runner.py:183]     )
INFO 10-04 02:06:41 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:06:41 model_runner.py:183]   )
INFO 10-04 02:06:41 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 02:06:41 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 02:06:41 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 02:06:41 model_runner.py:183] )
INFO 10-04 02:06:41 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:06:41 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 10.7989, fwd=340.8868 cmp_logits=0.2353, sampling=110.1339, total=462.0571
INFO 10-04 02:06:41 worker.py:164] Peak: 13.307 GB, Initial: 38.980 GB, Free: 25.673 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.118 GB
INFO 10-04 02:06:42 gpu_executor.py:117] # GPU blocks: 3087, # CPU blocks: 8192
INFO 10-04 02:07:17 worker.py:189] _init_cache_engine took 24.1250 GB
INFO 10-04 02:07:17 scheduler.py:307] Scheduler initialized with prompt limit: 4096
INFO 10-04 02:07:17 Start warmup...
INFO 10-04 02:07:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:07:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9189, fwd=1874.7087 cmp_logits=71.4972, sampling=0.9744, total=1948.1013
INFO 10-04 02:07:19 metrics.py:335] Avg prompt throughput: 523.7 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 1955.2
INFO 10-04 02:07:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1948.6265 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3066, fwd=38.6798 cmp_logits=0.1118, sampling=6.8471, total=45.9478
INFO 10-04 02:07:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 46.8
INFO 10-04 02:07:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.3860 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3164, fwd=7.8807 cmp_logits=0.0741, sampling=7.2370, total=15.5096
INFO 10-04 02:07:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 02:07:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8329 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2794, fwd=7.7293 cmp_logits=0.0687, sampling=7.3564, total=15.4355
INFO 10-04 02:07:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 02:07:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6851 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2751, fwd=7.6809 cmp_logits=0.0679, sampling=7.3493, total=15.3747
INFO 10-04 02:07:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:07:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5907 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2632, fwd=7.6623 cmp_logits=0.0679, sampling=6.2282, total=14.2231
INFO 10-04 02:07:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 02:07:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4360 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2687, fwd=7.7267 cmp_logits=0.0720, sampling=6.1729, total=14.2415
INFO 10-04 02:07:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 02:07:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4589 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2627, fwd=7.7033 cmp_logits=0.0663, sampling=6.1896, total=14.2226
INFO 10-04 02:07:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 02:07:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4572 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2601, fwd=7.6971 cmp_logits=0.0663, sampling=6.2139, total=14.2379
INFO 10-04 02:07:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 02:07:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4315 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2580, fwd=7.6270 cmp_logits=0.0656, sampling=6.2664, total=14.2176
INFO 10-04 02:07:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 02:07:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4296 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:24 Start benchmarking...
INFO 10-04 02:07:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:07:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8447, fwd=1576.2415 cmp_logits=0.1125, sampling=70.5404, total=1647.7406
INFO 10-04 02:07:26 metrics.py:335] Avg prompt throughput: 609.8 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%, Interval(ms): 1679.3
INFO 10-04 02:07:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1648.2320 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:07:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:07:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8318, fwd=14.1764 cmp_logits=0.1042, sampling=77.7159, total=92.8290
INFO 10-04 02:07:26 metrics.py:335] Avg prompt throughput: 10950.2 tokens/s, Avg generation throughput: 21.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%, Interval(ms): 93.4
INFO 10-04 02:07:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 93.1723 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:07:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:07:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8199, fwd=14.1747 cmp_logits=0.0844, sampling=62.4833, total=77.5630
INFO 10-04 02:07:26 metrics.py:335] Avg prompt throughput: 13093.8 tokens/s, Avg generation throughput: 38.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 7.2%, CPU KV cache usage: 0.0%, Interval(ms): 78.1
INFO 10-04 02:07:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.9066 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 02:07:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:07:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8328, fwd=14.2069 cmp_logits=0.0818, sampling=62.4695, total=77.5919
INFO 10-04 02:07:26 metrics.py:335] Avg prompt throughput: 13070.2 tokens/s, Avg generation throughput: 51.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 9.8%, CPU KV cache usage: 0.0%, Interval(ms): 78.1
INFO 10-04 02:07:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.9672 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:07:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8657, fwd=14.1642 cmp_logits=0.0823, sampling=62.2578, total=77.3709
INFO 10-04 02:07:26 metrics.py:335] Avg prompt throughput: 13088.3 tokens/s, Avg generation throughput: 64.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 11.0%, CPU KV cache usage: 0.0%, Interval(ms): 77.9
INFO 10-04 02:07:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.7752 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:07:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9003, fwd=14.1680 cmp_logits=0.0830, sampling=58.5363, total=73.6885
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 13704.8 tokens/s, Avg generation throughput: 94.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 15.0%, CPU KV cache usage: 0.0%, Interval(ms): 74.4
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 74.1956 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8762, fwd=14.1864 cmp_logits=0.0699, sampling=61.3518, total=76.4852
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 13181.9 tokens/s, Avg generation throughput: 90.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 15.0%, CPU KV cache usage: 0.0%, Interval(ms): 77.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.9250 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9098, fwd=14.2777 cmp_logits=0.0863, sampling=72.0870, total=87.3618
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 11553.0 tokens/s, Avg generation throughput: 90.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 88.0
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 87.8639 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9284, fwd=14.9753 cmp_logits=0.0889, sampling=70.9589, total=86.9524
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 11590.4 tokens/s, Avg generation throughput: 102.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.8%, CPU KV cache usage: 0.0%, Interval(ms): 87.7
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 87.4844 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9387, fwd=14.2112 cmp_logits=0.0691, sampling=60.9803, total=76.2000
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 13205.0 tokens/s, Avg generation throughput: 117.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.8%, CPU KV cache usage: 0.0%, Interval(ms): 76.9
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.6916 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([473]), positions.shape=torch.Size([473]) hidden_states.shape=torch.Size([473, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7653, fwd=14.3642 cmp_logits=0.0825, sampling=37.7033, total=52.9158
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 8648.5 tokens/s, Avg generation throughput: 186.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.8%, CPU KV cache usage: 0.0%, Interval(ms): 53.7
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.4780 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4466, fwd=7.5958 cmp_logits=0.0658, sampling=9.3987, total=17.5078
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0120 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4332, fwd=7.6256 cmp_logits=0.0675, sampling=9.3668, total=17.4937
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9980 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4337, fwd=7.6461 cmp_logits=0.0672, sampling=9.3529, total=17.5009
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0054 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4282, fwd=7.5817 cmp_logits=0.0668, sampling=9.4256, total=17.5035
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0185 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4354, fwd=7.6048 cmp_logits=0.0668, sampling=9.3784, total=17.4861
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0249 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4342, fwd=7.5958 cmp_logits=0.0663, sampling=9.3944, total=17.4916
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9992 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4344, fwd=7.5819 cmp_logits=0.0663, sampling=9.4242, total=17.5076
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0097 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4303, fwd=7.5297 cmp_logits=0.0658, sampling=9.4638, total=17.4904
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9906 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4301, fwd=7.5204 cmp_logits=0.0660, sampling=9.5034, total=17.5207
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0180 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4311, fwd=7.5891 cmp_logits=0.0665, sampling=9.4364, total=17.5238
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0616 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4339, fwd=7.5426 cmp_logits=0.0660, sampling=9.4736, total=17.5169
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0173 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4520, fwd=7.5238 cmp_logits=0.0668, sampling=9.5198, total=17.5631
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0664 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4337, fwd=7.5457 cmp_logits=0.0651, sampling=9.4712, total=17.5166
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0230 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4330, fwd=7.5471 cmp_logits=0.0665, sampling=9.4998, total=17.5474
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0550 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4339, fwd=7.5235 cmp_logits=0.0658, sampling=9.4922, total=17.5161
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0557 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4325, fwd=7.5138 cmp_logits=0.0660, sampling=9.5189, total=17.5323
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0459 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4263, fwd=7.5884 cmp_logits=0.0663, sampling=9.1918, total=17.2734
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 500.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7562 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4151, fwd=7.5419 cmp_logits=0.0658, sampling=9.2161, total=17.2400
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 503.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7169 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4187, fwd=7.5343 cmp_logits=0.0663, sampling=9.2118, total=17.2319
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 503.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7166 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4208, fwd=7.5815 cmp_logits=0.0668, sampling=9.2187, total=17.2887
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7684 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4184, fwd=7.5457 cmp_logits=0.0658, sampling=9.2397, total=17.2706
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7534 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4101, fwd=7.5319 cmp_logits=0.0660, sampling=9.2182, total=17.2272
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 503.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7004 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4153, fwd=7.5738 cmp_logits=0.0668, sampling=9.1894, total=17.2462
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7450 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3808, fwd=7.5579 cmp_logits=0.0656, sampling=8.7600, total=16.7651
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1778 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3800, fwd=7.5715 cmp_logits=0.0656, sampling=8.7717, total=16.7892
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1993 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3779, fwd=7.5240 cmp_logits=0.0665, sampling=8.7709, total=16.7403
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 404.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1535 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3819, fwd=7.5502 cmp_logits=0.0660, sampling=8.7602, total=16.7594
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1840 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3622, fwd=7.6039 cmp_logits=0.0815, sampling=8.5077, total=16.5565
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9392 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3633, fwd=7.5176 cmp_logits=0.0648, sampling=8.6017, total=16.5484
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 351.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9296 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3705, fwd=7.5564 cmp_logits=0.0660, sampling=8.6243, total=16.6180
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0007 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3636, fwd=7.5185 cmp_logits=0.0653, sampling=8.6131, total=16.5615
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9413 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:07:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3619, fwd=7.5495 cmp_logits=0.0651, sampling=8.5845, total=16.5622
INFO 10-04 02:07:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:07:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9435 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:07:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3641, fwd=7.5243 cmp_logits=0.0653, sampling=8.6236, total=16.5782
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9594 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3636, fwd=7.5507 cmp_logits=0.0651, sampling=8.5986, total=16.5792
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9578 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3586, fwd=7.5715 cmp_logits=0.0651, sampling=8.6372, total=16.6330
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0124 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3641, fwd=7.5731 cmp_logits=0.0656, sampling=8.5835, total=16.5870
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9687 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3686, fwd=7.5259 cmp_logits=0.0656, sampling=8.6536, total=16.6147
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9933 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3641, fwd=7.5607 cmp_logits=0.0658, sampling=8.6081, total=16.5997
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9821 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3617, fwd=7.5362 cmp_logits=0.0648, sampling=8.6203, total=16.5837
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9878 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3488, fwd=7.5784 cmp_logits=0.0646, sampling=8.3308, total=16.3233
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6740 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3514, fwd=7.4885 cmp_logits=0.0653, sampling=8.4031, total=16.3093
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6583 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3445, fwd=7.5345 cmp_logits=0.0651, sampling=8.3418, total=16.2868
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6340 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3479, fwd=7.5135 cmp_logits=0.0660, sampling=8.3859, total=16.3143
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6595 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3436, fwd=7.5321 cmp_logits=0.0651, sampling=8.3649, total=16.3066
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6600 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3486, fwd=7.5786 cmp_logits=0.0648, sampling=8.3106, total=16.3035
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6554 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3462, fwd=7.5405 cmp_logits=0.0653, sampling=8.2598, total=16.2125
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5720 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3495, fwd=7.5142 cmp_logits=0.0648, sampling=8.2884, total=16.2177
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5648 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3448, fwd=7.5500 cmp_logits=0.0651, sampling=8.2383, total=16.1991
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5501 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3467, fwd=7.5295 cmp_logits=0.0660, sampling=8.2436, total=16.1867
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5384 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3443, fwd=7.5939 cmp_logits=0.0651, sampling=8.2247, total=16.2289
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5775 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3471, fwd=7.5428 cmp_logits=0.0648, sampling=8.2479, total=16.2036
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5570 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3452, fwd=7.5438 cmp_logits=0.0646, sampling=8.2474, total=16.2022
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5491 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3443, fwd=7.5500 cmp_logits=0.0656, sampling=8.2731, total=16.2337
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5789 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3455, fwd=7.5462 cmp_logits=0.0648, sampling=8.2407, total=16.1982
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5522 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3479, fwd=7.5920 cmp_logits=0.0660, sampling=8.2607, total=16.2673
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6199 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3459, fwd=7.5388 cmp_logits=0.0648, sampling=8.2459, total=16.1965
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5458 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3550, fwd=7.4670 cmp_logits=0.0646, sampling=8.3373, total=16.2246
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5689 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3421, fwd=7.5340 cmp_logits=0.0651, sampling=8.2591, total=16.2010
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.0 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5703 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3240, fwd=7.5340 cmp_logits=0.0651, sampling=7.7016, total=15.6255
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9588 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3054, fwd=7.5796 cmp_logits=0.0648, sampling=7.3905, total=15.3413
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6236 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3030, fwd=7.4921 cmp_logits=0.0658, sampling=7.4265, total=15.2881
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 191.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5671 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3052, fwd=7.5104 cmp_logits=0.0646, sampling=7.4394, total=15.3201
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5964 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3042, fwd=7.4866 cmp_logits=0.0646, sampling=7.4525, total=15.3089
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5852 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3040, fwd=7.4463 cmp_logits=0.0644, sampling=7.5059, total=15.3213
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6224 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2873, fwd=7.5915 cmp_logits=0.0644, sampling=7.3283, total=15.2726
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5141 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2975, fwd=7.5617 cmp_logits=0.0648, sampling=7.3149, total=15.2402
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4836 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2859, fwd=7.5171 cmp_logits=0.0646, sampling=7.3597, total=15.2283
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4705 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2861, fwd=7.5200 cmp_logits=0.0648, sampling=7.3595, total=15.2311
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4691 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2856, fwd=7.4987 cmp_logits=0.0646, sampling=7.3657, total=15.2156
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4555 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2859, fwd=7.5562 cmp_logits=0.0644, sampling=7.3426, total=15.2497
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4889 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2842, fwd=7.5345 cmp_logits=0.0648, sampling=7.3380, total=15.2223
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4595 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2866, fwd=7.5321 cmp_logits=0.0644, sampling=7.3543, total=15.2383
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4772 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2868, fwd=7.5059 cmp_logits=0.0648, sampling=7.3657, total=15.2242
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4669 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2861, fwd=7.4985 cmp_logits=0.0639, sampling=7.3822, total=15.2316
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4707 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2859, fwd=7.5057 cmp_logits=0.0644, sampling=7.4084, total=15.2647
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5034 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2849, fwd=7.5181 cmp_logits=0.0646, sampling=7.3509, total=15.2194
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4698 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2854, fwd=7.4890 cmp_logits=0.0641, sampling=7.3769, total=15.2159
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4788 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2618, fwd=7.6990 cmp_logits=0.0658, sampling=7.3619, total=15.3897
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5854 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2604, fwd=7.6807 cmp_logits=0.0646, sampling=7.3521, total=15.3584
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5509 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2615, fwd=7.7052 cmp_logits=0.0656, sampling=7.3628, total=15.3959
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5909 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2599, fwd=7.6454 cmp_logits=0.0648, sampling=7.3862, total=15.3573
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5501 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2615, fwd=7.6637 cmp_logits=0.0656, sampling=7.4015, total=15.3930
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5871 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2613, fwd=7.6756 cmp_logits=0.0660, sampling=7.3586, total=15.3625
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5568 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2618, fwd=7.6747 cmp_logits=0.0668, sampling=7.3621, total=15.3663
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5602 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2680, fwd=7.7224 cmp_logits=0.0651, sampling=7.3531, total=15.4092
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6028 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2599, fwd=7.6859 cmp_logits=0.0648, sampling=7.3726, total=15.3842
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5778 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2601, fwd=7.6826 cmp_logits=0.0651, sampling=7.3507, total=15.3594
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5501 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:07:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 02:07:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:07:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2594, fwd=7.6783 cmp_logits=0.0658, sampling=7.3636, total=15.3680
INFO 10-04 02:07:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:07:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5878 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007644.959174, last_token_time=1728007647.7048156, first_scheduled_time=1728007644.980402, first_token_time=1728007646.6282961, time_in_queue=0.02122807502746582, finished_time=1728007647.7047749, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007644.96086, last_token_time=1728007648.1051672, first_scheduled_time=1728007644.980402, first_token_time=1728007646.7217393, time_in_queue=0.01954197883605957, finished_time=1728007648.1051412, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007644.9627998, last_token_time=1728007647.899627, first_scheduled_time=1728007646.6287973, first_token_time=1728007646.799769, time_in_queue=1.6659975051879883, finished_time=1728007647.899604, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007644.9640727, last_token_time=1728007648.423394, first_scheduled_time=1728007646.7220967, first_token_time=1728007646.8778627, time_in_queue=1.758023977279663, finished_time=1728007648.4233735, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007644.9667258, last_token_time=1728007648.4395468, first_scheduled_time=1728007646.8001575, first_token_time=1728007646.9557714, time_in_queue=1.8334317207336426, finished_time=1728007648.4395301, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007644.9690974, last_token_time=1728007647.8301961, first_scheduled_time=1728007646.8782759, first_token_time=1728007647.0300605, time_in_queue=1.9091784954071045, finished_time=1728007647.830178, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007644.970122, last_token_time=1728007647.8301961, first_scheduled_time=1728007646.956244, first_token_time=1728007647.0300605, time_in_queue=1.9861218929290771, finished_time=1728007647.8301823, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007644.9711592, last_token_time=1728007648.5183065, first_scheduled_time=1728007646.956244, first_token_time=1728007647.1952448, time_in_queue=1.9850847721099854, finished_time=1728007648.518294, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007644.9736886, last_token_time=1728007648.894167, first_scheduled_time=1728007647.1077514, first_token_time=1728007647.2828786, time_in_queue=2.1340627670288086, finished_time=1728007648.8941646, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007644.9772332, last_token_time=1728007648.7213712, first_scheduled_time=1728007647.1957903, first_token_time=1728007647.413368, time_in_queue=2.218557119369507, finished_time=1728007648.721369, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 11197
End-to-End latency: 3.94 seconds
Throughput: 2.54 requests/s, 2845.35 tokens/s
Per_token_time: 0.351 ms

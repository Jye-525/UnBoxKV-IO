Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=4096, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=2048, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Selected required requests 10
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Chunked prefill is enabled. max_num_batched_tokens=2048, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'type', 'finetuned', 'original_max_position_embeddings'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 02:04:38 config.py:654] [SchedulerConfig] max_num_batched_tokens: 2048 chunked_prefill_enabled: True
INFO 10-04 02:04:38 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 02:04:39 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 02:04:44 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 02:05:04 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 02:05:04 model_runner.py:183] Loaded model: 
INFO 10-04 02:05:04 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 02:05:04 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 02:05:04 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 02:05:04 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 02:05:04 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 02:05:04 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 02:05:04 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 02:05:04 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 02:05:04 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 02:05:04 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 02:05:04 model_runner.py:183]         )
INFO 10-04 02:05:04 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 02:05:04 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 02:05:04 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 02:05:04 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 02:05:04 model_runner.py:183]         )
INFO 10-04 02:05:04 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:05:04 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:05:04 model_runner.py:183]       )
INFO 10-04 02:05:04 model_runner.py:183]     )
INFO 10-04 02:05:04 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:05:04 model_runner.py:183]   )
INFO 10-04 02:05:04 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 02:05:04 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 02:05:04 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 02:05:04 model_runner.py:183] )
INFO 10-04 02:05:05 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 02:05:05 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 18.2211, fwd=379.9243 cmp_logits=0.2654, sampling=163.8846, total=562.2985
INFO 10-04 02:05:05 worker.py:164] Peak: 13.332 GB, Initial: 38.980 GB, Free: 25.648 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.092 GB
INFO 10-04 02:05:05 gpu_executor.py:117] # GPU blocks: 3083, # CPU blocks: 8192
INFO 10-04 02:05:37 worker.py:189] _init_cache_engine took 24.0859 GB
INFO 10-04 02:05:37 scheduler.py:307] Scheduler initialized with prompt limit: 4096
INFO 10-04 02:05:37 Start warmup...
INFO 10-04 02:05:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:37 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:05:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9255, fwd=1887.2640 cmp_logits=71.1768, sampling=0.9744, total=1960.3429
INFO 10-04 02:05:39 metrics.py:335] Avg prompt throughput: 520.5 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 1967.2
INFO 10-04 02:05:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1960.8991 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3140, fwd=39.9120 cmp_logits=0.1211, sampling=6.0818, total=46.4306
INFO 10-04 02:05:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 47.3
INFO 10-04 02:05:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.8824 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3052, fwd=8.0125 cmp_logits=0.0765, sampling=6.7184, total=15.1148
INFO 10-04 02:05:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:05:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4035 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2744, fwd=7.9658 cmp_logits=0.0713, sampling=6.7554, total=15.0685
INFO 10-04 02:05:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.4
INFO 10-04 02:05:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.2974 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2675, fwd=7.9710 cmp_logits=0.0703, sampling=6.7523, total=15.0623
INFO 10-04 02:05:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.4
INFO 10-04 02:05:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.2779 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2661, fwd=7.9596 cmp_logits=0.0706, sampling=6.7489, total=15.0461
INFO 10-04 02:05:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.4
INFO 10-04 02:05:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.2605 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2728, fwd=7.9889 cmp_logits=0.0739, sampling=6.7074, total=15.0445
INFO 10-04 02:05:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-04 02:05:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.2647 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2642, fwd=7.9706 cmp_logits=0.0694, sampling=6.7286, total=15.0337
INFO 10-04 02:05:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.4
INFO 10-04 02:05:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.2810 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2654, fwd=7.9651 cmp_logits=0.0679, sampling=6.7616, total=15.0609
INFO 10-04 02:05:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.4
INFO 10-04 02:05:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.2590 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2623, fwd=7.9606 cmp_logits=0.0684, sampling=6.7430, total=15.0352
INFO 10-04 02:05:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.4
INFO 10-04 02:05:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.2497 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:44 Start benchmarking...
INFO 10-04 02:05:44 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 02:05:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1270, fwd=1576.6544 cmp_logits=0.1528, sampling=134.7537, total=1712.6894
INFO 10-04 02:05:46 metrics.py:335] Avg prompt throughput: 1173.9 tokens/s, Avg generation throughput: 1.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%, Interval(ms): 1744.6
INFO 10-04 02:05:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1713.3060 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:05:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 02:05:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.0912, fwd=14.4572 cmp_logits=0.0839, sampling=126.8132, total=142.4463
INFO 10-04 02:05:46 metrics.py:335] Avg prompt throughput: 14291.8 tokens/s, Avg generation throughput: 27.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 9.8%, CPU KV cache usage: 0.0%, Interval(ms): 143.2
INFO 10-04 02:05:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 142.9060 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 02:05:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1084, fwd=14.4963 cmp_logits=0.0863, sampling=119.7727, total=135.4642
INFO 10-04 02:05:46 metrics.py:335] Avg prompt throughput: 15015.3 tokens/s, Avg generation throughput: 51.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 14.9%, CPU KV cache usage: 0.0%, Interval(ms): 136.1
INFO 10-04 02:05:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 135.9680 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 02:05:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 02:05:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1039, fwd=14.4768 cmp_logits=0.0849, sampling=120.7354, total=136.4012
INFO 10-04 02:05:46 metrics.py:335] Avg prompt throughput: 14889.9 tokens/s, Avg generation throughput: 58.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 18.6%, CPU KV cache usage: 0.0%, Interval(ms): 137.1
INFO 10-04 02:05:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 136.9040 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:05:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 02:05:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1110, fwd=14.3747 cmp_logits=0.0846, sampling=129.9992, total=145.5705
INFO 10-04 02:05:46 metrics.py:335] Avg prompt throughput: 13945.9 tokens/s, Avg generation throughput: 61.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.8%, CPU KV cache usage: 0.0%, Interval(ms): 146.3
INFO 10-04 02:05:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 146.1067 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([448]), positions.shape=torch.Size([448]) hidden_states.shape=torch.Size([448, 4096]) residual=None
INFO 10-04 02:05:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7193, fwd=14.6070 cmp_logits=0.0837, sampling=37.3368, total=52.7472
INFO 10-04 02:05:46 metrics.py:335] Avg prompt throughput: 8212.4 tokens/s, Avg generation throughput: 187.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 53.5
INFO 10-04 02:05:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.2773 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4187, fwd=7.8263 cmp_logits=0.0675, sampling=9.1770, total=17.4904
INFO 10-04 02:05:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9961 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4120, fwd=7.7391 cmp_logits=0.0684, sampling=9.2552, total=17.4756
INFO 10-04 02:05:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9734 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4122, fwd=7.7381 cmp_logits=0.0677, sampling=9.2630, total=17.4820
INFO 10-04 02:05:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9887 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4146, fwd=7.7846 cmp_logits=0.0682, sampling=9.1922, total=17.4603
INFO 10-04 02:05:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 02:05:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9660 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4163, fwd=7.7744 cmp_logits=0.0677, sampling=9.2199, total=17.4794
INFO 10-04 02:05:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9806 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4091, fwd=7.7507 cmp_logits=0.0689, sampling=9.3582, total=17.5879
INFO 10-04 02:05:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 546.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:05:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0845 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4184, fwd=7.7724 cmp_logits=0.0675, sampling=9.2278, total=17.4870
INFO 10-04 02:05:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9946 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4122, fwd=7.7541 cmp_logits=0.0684, sampling=9.2733, total=17.5087
INFO 10-04 02:05:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0147 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4284, fwd=7.6978 cmp_logits=0.0689, sampling=9.3129, total=17.5087
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0149 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4163, fwd=7.7753 cmp_logits=0.0684, sampling=9.2318, total=17.4925
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0385 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4151, fwd=7.8273 cmp_logits=0.0682, sampling=9.1925, total=17.5040
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0168 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4108, fwd=7.7357 cmp_logits=0.0675, sampling=9.2878, total=17.5030
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0128 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4151, fwd=7.7260 cmp_logits=0.0679, sampling=9.3098, total=17.5200
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0254 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4120, fwd=7.7670 cmp_logits=0.0694, sampling=9.2471, total=17.4966
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9975 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4148, fwd=7.7882 cmp_logits=0.0684, sampling=9.2356, total=17.5080
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0447 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4146, fwd=7.7991 cmp_logits=0.0679, sampling=9.2511, total=17.5340
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0373 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4103, fwd=7.7159 cmp_logits=0.0684, sampling=9.3403, total=17.5357
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0321 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4175, fwd=7.7710 cmp_logits=0.0679, sampling=9.2754, total=17.5326
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0445 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4094, fwd=7.7400 cmp_logits=0.0679, sampling=9.2986, total=17.5169
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0199 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4184, fwd=7.6866 cmp_logits=0.0682, sampling=9.3637, total=17.5378
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0748 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4151, fwd=7.7662 cmp_logits=0.0677, sampling=9.3191, total=17.5688
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0969 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4089, fwd=7.7724 cmp_logits=0.0670, sampling=9.0113, total=17.2608
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7360 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4063, fwd=7.7233 cmp_logits=0.0670, sampling=9.0313, total=17.2286
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 503.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7057 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3996, fwd=7.7317 cmp_logits=0.0672, sampling=9.0394, total=17.2389
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7245 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4025, fwd=7.7839 cmp_logits=0.0682, sampling=8.9898, total=17.2458
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7867 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3705, fwd=7.8034 cmp_logits=0.0675, sampling=8.5118, total=16.7539
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1716 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3650, fwd=7.7667 cmp_logits=0.0672, sampling=8.5423, total=16.7422
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 404.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1602 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3669, fwd=7.7536 cmp_logits=0.0682, sampling=8.5435, total=16.7334
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 404.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1468 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3688, fwd=7.7415 cmp_logits=0.0670, sampling=8.5499, total=16.7279
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 404.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1497 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3664, fwd=7.7646 cmp_logits=0.0691, sampling=8.5416, total=16.7427
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1995 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3669, fwd=7.7741 cmp_logits=0.0677, sampling=8.6393, total=16.8488
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2796 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3560, fwd=7.8113 cmp_logits=0.0792, sampling=8.3380, total=16.5854
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9685 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3507, fwd=7.7772 cmp_logits=0.0670, sampling=8.4012, total=16.5968
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9792 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3524, fwd=7.7500 cmp_logits=0.0672, sampling=8.4064, total=16.5770
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9597 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3622, fwd=7.7791 cmp_logits=0.0679, sampling=8.4047, total=16.6149
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9990 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3569, fwd=7.7274 cmp_logits=0.0675, sampling=8.4372, total=16.5896
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9752 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3579, fwd=7.7565 cmp_logits=0.0670, sampling=8.3811, total=16.5632
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 351.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9380 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3512, fwd=7.7457 cmp_logits=0.0672, sampling=8.4007, total=16.5658
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9501 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3507, fwd=7.7357 cmp_logits=0.0660, sampling=8.4240, total=16.5775
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9628 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3505, fwd=7.7581 cmp_logits=0.0670, sampling=8.4302, total=16.6068
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9833 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3521, fwd=7.7517 cmp_logits=0.0665, sampling=8.3976, total=16.5687
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9487 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3521, fwd=7.7264 cmp_logits=0.0668, sampling=8.4436, total=16.5901
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9756 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3505, fwd=7.7927 cmp_logits=0.0675, sampling=8.3604, total=16.5722
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9692 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3390, fwd=7.8428 cmp_logits=0.0784, sampling=8.0655, total=16.3264
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6757 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3386, fwd=7.7922 cmp_logits=0.0668, sampling=8.1086, total=16.3069
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6569 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3366, fwd=7.7794 cmp_logits=0.0668, sampling=8.1015, total=16.2852
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6433 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3388, fwd=7.7443 cmp_logits=0.0670, sampling=8.0621, total=16.2129
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5615 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3397, fwd=7.7655 cmp_logits=0.0682, sampling=8.0194, total=16.1936
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5422 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3402, fwd=7.7345 cmp_logits=0.0668, sampling=8.0771, total=16.2196
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5715 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3347, fwd=7.7691 cmp_logits=0.0665, sampling=8.1041, total=16.2752
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6240 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3366, fwd=7.7612 cmp_logits=0.0668, sampling=8.0438, total=16.2094
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5608 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3481, fwd=7.7717 cmp_logits=0.0670, sampling=8.0202, total=16.2079
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5539 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3383, fwd=7.7848 cmp_logits=0.0665, sampling=8.0464, total=16.2368
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5844 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3355, fwd=7.7403 cmp_logits=0.0670, sampling=8.0724, total=16.2156
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5617 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3381, fwd=7.7791 cmp_logits=0.0670, sampling=8.0471, total=16.2323
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5837 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3350, fwd=7.7620 cmp_logits=0.0672, sampling=8.0574, total=16.2222
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5710 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3464, fwd=7.7577 cmp_logits=0.0660, sampling=8.0557, total=16.2265
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5858 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3371, fwd=7.7486 cmp_logits=0.0679, sampling=8.0564, total=16.2108
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5629 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3357, fwd=7.7088 cmp_logits=0.0660, sampling=8.1031, total=16.2148
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.6%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5613 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3331, fwd=7.7937 cmp_logits=0.0660, sampling=8.0626, total=16.2563
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.6%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6075 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3350, fwd=7.7612 cmp_logits=0.0665, sampling=8.0535, total=16.2168
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5882 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3204, fwd=7.7939 cmp_logits=0.0677, sampling=7.4620, total=15.6450
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9881 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3026, fwd=7.7405 cmp_logits=0.0787, sampling=7.2103, total=15.3325
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6157 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3004, fwd=7.6923 cmp_logits=0.0665, sampling=7.2458, total=15.3058
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5859 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3002, fwd=7.7331 cmp_logits=0.0658, sampling=7.2417, total=15.3420
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6446 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2837, fwd=7.8046 cmp_logits=0.0663, sampling=7.1042, total=15.2595
INFO 10-04 02:05:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:05:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5039 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:05:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:05:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2816, fwd=7.7069 cmp_logits=0.0663, sampling=7.1788, total=15.2342
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4769 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2844, fwd=7.6830 cmp_logits=0.0665, sampling=7.1890, total=15.2240
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4655 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2832, fwd=7.6883 cmp_logits=0.0658, sampling=7.2017, total=15.2397
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4829 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2818, fwd=7.7667 cmp_logits=0.0665, sampling=7.1509, total=15.2671
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5051 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2851, fwd=7.7119 cmp_logits=0.0656, sampling=7.1676, total=15.2311
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4722 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2823, fwd=7.7093 cmp_logits=0.0658, sampling=7.1833, total=15.2416
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4815 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2832, fwd=7.7584 cmp_logits=0.0658, sampling=7.1256, total=15.2340
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4760 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2902, fwd=7.6811 cmp_logits=0.0656, sampling=7.2081, total=15.2459
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4853 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2799, fwd=7.7751 cmp_logits=0.0660, sampling=7.1445, total=15.2662
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5082 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2818, fwd=7.7569 cmp_logits=0.0670, sampling=7.1349, total=15.2416
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4834 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2825, fwd=7.6900 cmp_logits=0.0651, sampling=7.2031, total=15.2419
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5051 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2606, fwd=7.8833 cmp_logits=0.0670, sampling=7.1843, total=15.3959
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5942 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2611, fwd=7.9045 cmp_logits=0.0682, sampling=7.1492, total=15.3835
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5792 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2599, fwd=7.9443 cmp_logits=0.0679, sampling=7.1516, total=15.4245
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6198 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2604, fwd=7.8459 cmp_logits=0.0670, sampling=7.2083, total=15.3823
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5766 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2599, fwd=7.8769 cmp_logits=0.0670, sampling=7.1993, total=15.4037
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5990 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2611, fwd=7.8859 cmp_logits=0.0677, sampling=7.1657, total=15.3811
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5754 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2608, fwd=7.8728 cmp_logits=0.0675, sampling=7.1926, total=15.3944
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5900 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2615, fwd=7.9236 cmp_logits=0.0663, sampling=7.1766, total=15.4288
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6214 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2596, fwd=7.8654 cmp_logits=0.0675, sampling=7.2210, total=15.4142
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6095 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2668, fwd=7.8602 cmp_logits=0.0675, sampling=7.1843, total=15.3794
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5721 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2584, fwd=7.8721 cmp_logits=0.0663, sampling=7.1919, total=15.3897
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5833 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:05:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 02:05:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:05:48 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2594, fwd=7.8797 cmp_logits=0.0677, sampling=7.1783, total=15.3859
INFO 10-04 02:05:48 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 02:05:48 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6045 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007544.4896865, last_token_time=1728007547.222462, first_scheduled_time=1728007544.5111153, first_token_time=1728007546.2239857, time_in_queue=0.021428823471069336, finished_time=1728007547.222419, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007544.4916496, last_token_time=1728007547.6038725, first_scheduled_time=1728007544.5111153, first_token_time=1728007546.2239857, time_in_queue=0.01946568489074707, finished_time=1728007547.6038465, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007544.4935443, last_token_time=1728007547.3983078, first_scheduled_time=1728007544.5111153, first_token_time=1728007546.3671541, time_in_queue=0.017570972442626953, finished_time=1728007547.398285, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007544.4948153, last_token_time=1728007547.9052956, first_scheduled_time=1728007546.2245748, first_token_time=1728007546.3671541, time_in_queue=1.7297594547271729, finished_time=1728007547.9052737, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007544.4974408, last_token_time=1728007547.9214778, first_scheduled_time=1728007546.2245748, first_token_time=1728007546.5032134, time_in_queue=1.7271339893341064, finished_time=1728007547.92146, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007544.4998085, last_token_time=1728007547.2941456, first_scheduled_time=1728007546.3676186, first_token_time=1728007546.5032134, time_in_queue=1.8678100109100342, finished_time=1728007547.294126, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007544.5008376, last_token_time=1728007547.2941456, first_scheduled_time=1728007546.3676186, first_token_time=1728007546.5032134, time_in_queue=1.8667809963226318, finished_time=1728007547.2941308, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007544.5018635, last_token_time=1728007547.9688077, first_scheduled_time=1728007546.3676186, first_token_time=1728007546.6402764, time_in_queue=1.8657550811767578, finished_time=1728007547.9687946, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007544.5043929, last_token_time=1728007548.3451824, first_scheduled_time=1728007546.5037417, first_token_time=1728007546.7865288, time_in_queue=1.9993488788604736, finished_time=1728007548.3451798, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728007544.507907, last_token_time=1728007548.156397, first_scheduled_time=1728007546.6408195, first_token_time=1728007546.8399613, time_in_queue=2.1329126358032227, finished_time=1728007548.1563952, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 11197
End-to-End latency: 3.86 seconds
Throughput: 2.59 requests/s, 2904.01 tokens/s
Per_token_time: 0.344 ms

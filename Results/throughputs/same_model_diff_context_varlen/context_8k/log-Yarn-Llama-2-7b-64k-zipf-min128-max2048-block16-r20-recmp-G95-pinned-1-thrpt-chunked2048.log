Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=128, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=8192, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=2048, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Selected required requests 20
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Chunked prefill is enabled. max_num_batched_tokens=2048, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'finetuned', 'original_max_position_embeddings', 'type'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 03:50:01 config.py:654] [SchedulerConfig] max_num_batched_tokens: 2048 chunked_prefill_enabled: True
INFO 10-04 03:50:01 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 03:50:02 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 03:50:06 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 03:50:10 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 03:50:10 model_runner.py:183] Loaded model: 
INFO 10-04 03:50:10 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 03:50:10 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 03:50:10 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:50:10 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 03:50:10 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 03:50:10 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 03:50:10 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:50:10 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:50:10 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 03:50:10 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 03:50:10 model_runner.py:183]         )
INFO 10-04 03:50:10 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 03:50:10 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:50:10 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:50:10 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 03:50:10 model_runner.py:183]         )
INFO 10-04 03:50:10 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:50:10 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:50:10 model_runner.py:183]       )
INFO 10-04 03:50:10 model_runner.py:183]     )
INFO 10-04 03:50:10 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:50:10 model_runner.py:183]   )
INFO 10-04 03:50:10 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:50:10 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 03:50:10 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 03:50:10 model_runner.py:183] )
INFO 10-04 03:50:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 03:50:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 9.4168, fwd=343.4157 cmp_logits=0.2296, sampling=156.1530, total=509.2175
INFO 10-04 03:50:11 worker.py:164] Peak: 13.332 GB, Initial: 38.980 GB, Free: 25.648 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.092 GB
INFO 10-04 03:50:11 gpu_executor.py:117] # GPU blocks: 3083, # CPU blocks: 8192
INFO 10-04 03:50:42 worker.py:189] _init_cache_engine took 24.0859 GB
INFO 10-04 03:50:42 scheduler.py:307] Scheduler initialized with prompt limit: 8192
INFO 10-04 03:50:42 Start warmup...
INFO 10-04 03:50:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:50:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9263, fwd=341.5048 cmp_logits=71.0857, sampling=1.0076, total=414.5272
INFO 10-04 03:50:43 metrics.py:335] Avg prompt throughput: 2428.2 tokens/s, Avg generation throughput: 2.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 421.7
INFO 10-04 03:50:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 415.0803 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3133, fwd=43.7784 cmp_logits=0.1225, sampling=6.3806, total=50.5965
INFO 10-04 03:50:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 51.5
INFO 10-04 03:50:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 51.0621 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3200, fwd=8.2769 cmp_logits=0.0784, sampling=6.8836, total=15.5606
INFO 10-04 03:50:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 61.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 03:50:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9080 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2892, fwd=8.1866 cmp_logits=0.0739, sampling=6.9597, total=15.5110
INFO 10-04 03:50:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:50:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7721 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2797, fwd=8.1739 cmp_logits=0.0722, sampling=6.9997, total=15.5275
INFO 10-04 03:50:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:50:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7671 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2737, fwd=8.1375 cmp_logits=0.0720, sampling=5.7943, total=14.2787
INFO 10-04 03:50:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:50:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5144 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2747, fwd=8.1332 cmp_logits=0.0746, sampling=5.8067, total=14.2910
INFO 10-04 03:50:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:50:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5187 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2689, fwd=8.1418 cmp_logits=0.0694, sampling=5.7943, total=14.2756
INFO 10-04 03:50:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:50:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5264 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2766, fwd=8.1398 cmp_logits=0.0696, sampling=5.8150, total=14.3018
INFO 10-04 03:50:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:50:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5111 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2637, fwd=8.1117 cmp_logits=0.0689, sampling=5.8632, total=14.3082
INFO 10-04 03:50:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:50:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5295 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:48 Start benchmarking...
INFO 10-04 03:50:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 03:50:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1456, fwd=1578.0432 cmp_logits=0.1531, sampling=143.5115, total=1722.8549
INFO 10-04 03:50:50 metrics.py:335] Avg prompt throughput: 1161.3 tokens/s, Avg generation throughput: 1.7 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%, Interval(ms): 1763.6
INFO 10-04 03:50:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1723.5079 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:50:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 03:50:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1859, fwd=39.4170 cmp_logits=0.1054, sampling=123.9548, total=164.6645
INFO 10-04 03:50:50 metrics.py:335] Avg prompt throughput: 12356.8 tokens/s, Avg generation throughput: 42.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 10.6%, CPU KV cache usage: 0.0%, Interval(ms): 165.5
INFO 10-04 03:50:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 165.2696 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:50:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 03:50:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1318, fwd=14.4699 cmp_logits=0.0901, sampling=118.2094, total=133.9023
INFO 10-04 03:50:50 metrics.py:335] Avg prompt throughput: 15159.6 tokens/s, Avg generation throughput: 59.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 14.3%, CPU KV cache usage: 0.0%, Interval(ms): 134.6
INFO 10-04 03:50:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 134.4314 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:50:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 03:50:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1237, fwd=14.5159 cmp_logits=0.0882, sampling=133.1038, total=148.8326
INFO 10-04 03:50:50 metrics.py:335] Avg prompt throughput: 13639.8 tokens/s, Avg generation throughput: 60.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 149.6
INFO 10-04 03:50:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 149.3752 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:50:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 03:50:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.2190, fwd=14.6253 cmp_logits=0.0865, sampling=130.0962, total=146.0280
INFO 10-04 03:50:50 metrics.py:335] Avg prompt throughput: 13874.7 tokens/s, Avg generation throughput: 88.5 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 22.6%, CPU KV cache usage: 0.0%, Interval(ms): 147.0
INFO 10-04 03:50:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 146.7762 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:50:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 03:50:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.2364, fwd=14.6945 cmp_logits=0.0892, sampling=118.8145, total=134.8355
INFO 10-04 03:50:50 metrics.py:335] Avg prompt throughput: 14984.2 tokens/s, Avg generation throughput: 117.8 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 25.5%, CPU KV cache usage: 0.0%, Interval(ms): 135.8
INFO 10-04 03:50:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 135.6032 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-04 03:50:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 03:50:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.2906, fwd=14.6577 cmp_logits=0.0973, sampling=123.8575, total=139.9043
INFO 10-04 03:50:50 metrics.py:335] Avg prompt throughput: 14413.5 tokens/s, Avg generation throughput: 134.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.0%, CPU KV cache usage: 0.0%, Interval(ms): 141.0
INFO 10-04 03:50:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 140.7661 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 03:50:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([841]), positions.shape=torch.Size([841]) hidden_states.shape=torch.Size([841, 4096]) residual=None
INFO 10-04 03:50:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9696, fwd=14.6697 cmp_logits=0.0868, sampling=67.2746, total=83.0014
INFO 10-04 03:50:50 metrics.py:335] Avg prompt throughput: 9781.8 tokens/s, Avg generation throughput: 238.0 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.1%, CPU KV cache usage: 0.0%, Interval(ms): 84.0
INFO 10-04 03:50:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 83.8141 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 03:50:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 03:50:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5670, fwd=7.8485 cmp_logits=0.0710, sampling=11.6591, total=20.1461
INFO 10-04 03:50:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.8 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-04 03:50:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9310 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 03:50:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5598, fwd=7.7879 cmp_logits=0.0694, sampling=11.5893, total=20.0074
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 903.0 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7670 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5460, fwd=7.7810 cmp_logits=0.0701, sampling=11.5550, total=19.9528
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 908.4 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7088 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5558, fwd=7.7617 cmp_logits=0.0684, sampling=11.5740, total=19.9609
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 907.5 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7257 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5248, fwd=7.7958 cmp_logits=0.0803, sampling=11.6389, total=20.0412
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 809.8 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7479 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5050, fwd=7.7815 cmp_logits=0.0689, sampling=10.9506, total=19.3069
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 791.9 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9792 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5059, fwd=7.7288 cmp_logits=0.0701, sampling=10.9837, total=19.2897
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 792.3 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9659 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5074, fwd=7.8125 cmp_logits=0.0694, sampling=10.9015, total=19.2919
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 793.5 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9697 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5066, fwd=7.7813 cmp_logits=0.0703, sampling=11.0409, total=19.4001
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 789.2 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.3
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0772 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5052, fwd=7.7841 cmp_logits=0.0694, sampling=10.9661, total=19.3255
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 792.2 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0028 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5090, fwd=7.7636 cmp_logits=0.0701, sampling=11.0364, total=19.3801
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 789.4 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.3
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0708 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5031, fwd=7.8294 cmp_logits=0.0813, sampling=10.8759, total=19.2904
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 741.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9561 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4878, fwd=7.8115 cmp_logits=0.0856, sampling=10.8330, total=19.2184
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 697.1 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8607 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4818, fwd=7.8290 cmp_logits=0.0696, sampling=10.2832, total=18.6646
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 667.2 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.2683 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4652, fwd=7.7698 cmp_logits=0.0699, sampling=10.3428, total=18.6486
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 668.6 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.4
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.2592 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4663, fwd=7.7155 cmp_logits=0.0684, sampling=10.4702, total=18.7211
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 666.2 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3281 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4685, fwd=7.7832 cmp_logits=0.0687, sampling=10.3931, total=18.7140
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 665.4 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3167 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4621, fwd=7.7820 cmp_logits=0.0679, sampling=10.3996, total=18.7125
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 666.9 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3074 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4606, fwd=7.7658 cmp_logits=0.0694, sampling=10.4244, total=18.7209
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 666.4 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3219 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4592, fwd=7.7481 cmp_logits=0.0694, sampling=10.4251, total=18.7025
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 667.1 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3002 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4611, fwd=7.7701 cmp_logits=0.0684, sampling=10.4172, total=18.7178
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 666.0 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3307 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4556, fwd=7.7903 cmp_logits=0.0837, sampling=10.0181, total=18.3485
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 626.2 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9137 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4461, fwd=7.7226 cmp_logits=0.0696, sampling=10.0965, total=18.3353
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 628.5 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.1
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9128 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4344, fwd=7.8070 cmp_logits=0.0827, sampling=9.8405, total=18.1656
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 581.8 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7008 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4342, fwd=7.7872 cmp_logits=0.0684, sampling=9.8901, total=18.1804
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 582.3 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7156 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4458, fwd=7.7581 cmp_logits=0.0794, sampling=9.9192, total=18.2030
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 581.4 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7435 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4358, fwd=7.7429 cmp_logits=0.0687, sampling=9.9618, total=18.2099
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 579.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.8%, CPU KV cache usage: 0.0%, Interval(ms): 19.0
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7614 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4230, fwd=7.8535 cmp_logits=0.0825, sampling=9.6185, total=17.9782
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 534.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5153 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4027, fwd=7.8425 cmp_logits=0.0691, sampling=9.2697, total=17.5846
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 492.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0678 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3986, fwd=7.7424 cmp_logits=0.0675, sampling=9.3756, total=17.5853
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0588 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4048, fwd=7.7281 cmp_logits=0.0679, sampling=9.3923, total=17.5941
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0662 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4046, fwd=7.7617 cmp_logits=0.0679, sampling=9.3176, total=17.5529
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0268 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4010, fwd=7.7629 cmp_logits=0.0684, sampling=9.3327, total=17.5655
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0383 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4046, fwd=7.7369 cmp_logits=0.0682, sampling=9.3381, total=17.5488
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0194 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4013, fwd=7.7426 cmp_logits=0.0682, sampling=9.3553, total=17.5679
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0461 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4067, fwd=7.7710 cmp_logits=0.0677, sampling=9.3219, total=17.5681
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0330 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4056, fwd=7.7677 cmp_logits=0.0689, sampling=9.3038, total=17.5467
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0283 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3974, fwd=7.7460 cmp_logits=0.0677, sampling=9.3603, total=17.5722
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0445 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3998, fwd=7.7538 cmp_logits=0.0679, sampling=9.3398, total=17.5624
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0357 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4001, fwd=7.7531 cmp_logits=0.0675, sampling=9.3424, total=17.5641
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0581 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3874, fwd=7.8273 cmp_logits=0.0675, sampling=8.9240, total=17.2069
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 448.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.6%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6525 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3924, fwd=7.7798 cmp_logits=0.0679, sampling=8.9939, total=17.2346
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 447.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.6%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6892 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3941, fwd=7.7972 cmp_logits=0.0682, sampling=8.9595, total=17.2193
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 448.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6766 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3848, fwd=7.7815 cmp_logits=0.0682, sampling=8.9824, total=17.2179
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 448.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6694 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3850, fwd=7.8003 cmp_logits=0.0689, sampling=8.9617, total=17.2167
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 448.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6818 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3703, fwd=7.8526 cmp_logits=0.0689, sampling=8.5564, total=16.8495
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2672 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3777, fwd=7.7751 cmp_logits=0.0665, sampling=8.6107, total=16.8312
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2527 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3746, fwd=7.7090 cmp_logits=0.0682, sampling=8.7323, total=16.8850
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2973 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3710, fwd=7.7643 cmp_logits=0.0679, sampling=8.6458, total=16.8500
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2634 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3715, fwd=7.7610 cmp_logits=0.0670, sampling=8.6727, total=16.8731
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3059 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3600, fwd=7.8094 cmp_logits=0.0820, sampling=8.2574, total=16.5093
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.9%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9411 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3445, fwd=7.8297 cmp_logits=0.0803, sampling=7.8619, total=16.1171
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.4671 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3448, fwd=7.8013 cmp_logits=0.0679, sampling=7.8993, total=16.1142
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.6
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.4685 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:50:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3402, fwd=7.7858 cmp_logits=0.0675, sampling=7.9079, total=16.1021
INFO 10-04 03:50:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.7 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.6
INFO 10-04 03:50:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.4711 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:50:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3233, fwd=7.8387 cmp_logits=0.0830, sampling=7.3588, total=15.6043
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9431 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3059, fwd=7.8280 cmp_logits=0.0670, sampling=7.1635, total=15.3654
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6822 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3049, fwd=7.7837 cmp_logits=0.0668, sampling=7.1919, total=15.3480
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6276 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3047, fwd=7.7491 cmp_logits=0.0670, sampling=7.2320, total=15.3537
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6343 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3021, fwd=7.7097 cmp_logits=0.0675, sampling=7.2677, total=15.3477
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6255 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3045, fwd=7.7546 cmp_logits=0.0658, sampling=7.2291, total=15.3544
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6357 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3040, fwd=7.7622 cmp_logits=0.0668, sampling=7.2029, total=15.3365
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6567 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3045, fwd=7.7429 cmp_logits=0.0660, sampling=7.2424, total=15.3568
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6407 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3040, fwd=7.7195 cmp_logits=0.0670, sampling=7.2570, total=15.3487
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6310 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3033, fwd=7.7531 cmp_logits=0.0665, sampling=7.2300, total=15.3534
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6374 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3059, fwd=7.7355 cmp_logits=0.0665, sampling=7.2522, total=15.3608
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6424 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3076, fwd=7.7698 cmp_logits=0.0670, sampling=7.2241, total=15.3694
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6913 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3078, fwd=7.7431 cmp_logits=0.0672, sampling=7.2546, total=15.3737
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6550 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3049, fwd=7.7679 cmp_logits=0.0668, sampling=7.2093, total=15.3496
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6293 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3195, fwd=7.7782 cmp_logits=0.0660, sampling=7.2007, total=15.3651
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6689 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2992, fwd=7.8022 cmp_logits=0.1225, sampling=6.9830, total=15.2080
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4538 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2866, fwd=7.7586 cmp_logits=0.0663, sampling=7.0806, total=15.1930
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4719 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2863, fwd=7.7314 cmp_logits=0.0665, sampling=7.1099, total=15.1951
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4402 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2863, fwd=7.7929 cmp_logits=0.0670, sampling=7.0257, total=15.1727
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4195 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2892, fwd=7.7085 cmp_logits=0.0665, sampling=7.1235, total=15.1885
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4550 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2673, fwd=7.9765 cmp_logits=0.0675, sampling=7.0357, total=15.3477
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5473 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2654, fwd=7.9470 cmp_logits=0.0679, sampling=7.0570, total=15.3382
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5675 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2680, fwd=7.9854 cmp_logits=0.0665, sampling=7.0744, total=15.3947
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5945 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2654, fwd=7.9463 cmp_logits=0.0670, sampling=7.0615, total=15.3408
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5408 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2720, fwd=7.9489 cmp_logits=0.0682, sampling=7.0632, total=15.3532
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5585 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2670, fwd=7.9217 cmp_logits=0.0670, sampling=7.1483, total=15.4049
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6050 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2635, fwd=7.9327 cmp_logits=0.0675, sampling=7.1752, total=15.4397
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6777 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2770, fwd=7.9219 cmp_logits=0.0689, sampling=7.1392, total=15.4076
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6050 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2646, fwd=7.9427 cmp_logits=0.0684, sampling=7.1266, total=15.4030
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5995 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:50:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:50:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:50:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2635, fwd=7.9434 cmp_logits=0.0675, sampling=7.1292, total=15.4045
INFO 10-04 03:50:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:50:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6255 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.2674565, last_token_time=1728013850.999553, first_scheduled_time=1728013848.2977042, first_token_time=1728013850.0207393, time_in_queue=0.03024768829345703, finished_time=1728013850.9994736, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.268524, last_token_time=1728013851.5337012, first_scheduled_time=1728013848.2977042, first_token_time=1728013850.0207393, time_in_queue=0.029180288314819336, finished_time=1728013851.5336494, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.270183, last_token_time=1728013851.224959, first_scheduled_time=1728013848.2977042, first_token_time=1728013850.0207393, time_in_queue=0.027521133422851562, finished_time=1728013851.2249036, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.2709482, last_token_time=1728013851.946051, first_scheduled_time=1728013848.2977042, first_token_time=1728013850.1861773, time_in_queue=0.02675604820251465, finished_time=1728013851.9460244, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.273164, last_token_time=1728013851.9289095, first_scheduled_time=1728013850.0213578, first_token_time=1728013850.1861773, time_in_queue=1.7481937408447266, finished_time=1728013851.9288857, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.2753448, last_token_time=1728013851.0624464, first_scheduled_time=1728013850.0213578, first_token_time=1728013850.1861773, time_in_queue=1.7460129261016846, finished_time=1728013851.0623896, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.2757912, last_token_time=1728013851.0624464, first_scheduled_time=1728013850.0213578, first_token_time=1728013850.1861773, time_in_queue=1.7455666065216064, finished_time=1728013851.0623937, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.276241, last_token_time=1728013852.0120971, first_scheduled_time=1728013850.0213578, first_token_time=1728013850.3208368, time_in_queue=1.7451167106628418, finished_time=1728013852.0120783, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.278499, last_token_time=1728013852.4685543, first_scheduled_time=1728013850.1867962, first_token_time=1728013850.4703739, time_in_queue=1.9082973003387451, finished_time=1728013852.4685516, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.2819185, last_token_time=1728013852.23324, first_scheduled_time=1728013850.3213954, first_token_time=1728013850.6172285, time_in_queue=2.0394768714904785, finished_time=1728013852.2332335, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.2846673, last_token_time=1728013851.2047415, first_scheduled_time=1728013850.4710398, first_token_time=1728013850.6172285, time_in_queue=2.1863725185394287, finished_time=1728013851.2047083, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.2852328, last_token_time=1728013851.4010298, first_scheduled_time=1728013850.4710398, first_token_time=1728013850.6172285, time_in_queue=2.1858069896698, finished_time=1728013851.4010031, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.2861338, last_token_time=1728013851.752357, first_scheduled_time=1728013850.4710398, first_token_time=1728013850.6172285, time_in_queue=2.184906005859375, finished_time=1728013851.7523432, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.287704, last_token_time=1728013851.8416312, first_scheduled_time=1728013850.4710398, first_token_time=1728013850.7529795, time_in_queue=2.183335781097412, finished_time=1728013851.841621, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.2893946, last_token_time=1728013851.2450418, first_scheduled_time=1728013850.617975, first_token_time=1728013850.7529795, time_in_queue=2.328580379486084, finished_time=1728013851.2450242, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.2899861, last_token_time=1728013851.5149786, first_scheduled_time=1728013850.617975, first_token_time=1728013850.7529795, time_in_queue=2.327988862991333, finished_time=1728013851.5149689, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.2910576, last_token_time=1728013851.4392836, first_scheduled_time=1728013850.617975, first_token_time=1728013850.8939102, time_in_queue=2.3269174098968506, finished_time=1728013851.439274, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.2920263, last_token_time=1728013851.08344, first_scheduled_time=1728013850.7538247, first_token_time=1728013850.8939102, time_in_queue=2.4617984294891357, finished_time=1728013851.08343, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.2923074, last_token_time=1728013851.995966, first_scheduled_time=1728013850.7538247, first_token_time=1728013850.8939102, time_in_queue=2.461517333984375, finished_time=1728013851.9959598, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013848.2944746, last_token_time=1728013852.3112283, first_scheduled_time=1728013850.7538247, first_token_time=1728013850.9779344, time_in_queue=2.459350109100342, finished_time=1728013852.3112264, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 15868
End-to-End latency: 4.20 seconds
Throughput: 4.76 requests/s, 3776.92 tokens/s
Per_token_time: 0.265 ms

Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=4096, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=8192, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=4096, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=1585, output_len=80, sequence_len=1665...
User Requests: prompt_len=970, output_len=49, sequence_len=1019...
User Requests: prompt_len=2398, output_len=120, sequence_len=2518...
User Requests: prompt_len=2366, output_len=119, sequence_len=2485...
User Requests: prompt_len=652, output_len=33, sequence_len=685...
User Requests: prompt_len=658, output_len=33, sequence_len=691...
User Requests: prompt_len=2495, output_len=125, sequence_len=2620...
User Requests: prompt_len=3566, output_len=179, sequence_len=3745...
User Requests: prompt_len=2969, output_len=149, sequence_len=3118...
Selected required requests 10
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=1585, output_len=80, sequence_len=1665...
User Requests: prompt_len=970, output_len=49, sequence_len=1019...
User Requests: prompt_len=2398, output_len=120, sequence_len=2518...
User Requests: prompt_len=2366, output_len=119, sequence_len=2485...
User Requests: prompt_len=652, output_len=33, sequence_len=685...
User Requests: prompt_len=658, output_len=33, sequence_len=691...
User Requests: prompt_len=2495, output_len=125, sequence_len=2620...
User Requests: prompt_len=3566, output_len=179, sequence_len=3745...
User Requests: prompt_len=2969, output_len=149, sequence_len=3118...
Chunked prefill is enabled. max_num_batched_tokens=4096, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings', 'finetuned', 'type'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 02:55:28 config.py:654] [SchedulerConfig] max_num_batched_tokens: 4096 chunked_prefill_enabled: True
INFO 10-04 02:55:28 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 02:55:29 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 02:55:34 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 02:55:42 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 02:55:42 model_runner.py:183] Loaded model: 
INFO 10-04 02:55:42 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 02:55:42 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 02:55:42 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 02:55:42 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 02:55:42 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 02:55:42 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 02:55:42 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 02:55:42 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 02:55:42 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 02:55:42 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 02:55:42 model_runner.py:183]         )
INFO 10-04 02:55:42 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 02:55:42 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 02:55:42 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 02:55:42 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 02:55:42 model_runner.py:183]         )
INFO 10-04 02:55:42 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:55:42 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:55:42 model_runner.py:183]       )
INFO 10-04 02:55:42 model_runner.py:183]     )
INFO 10-04 02:55:42 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 02:55:42 model_runner.py:183]   )
INFO 10-04 02:55:42 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 02:55:42 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 02:55:42 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 02:55:42 model_runner.py:183] )
INFO 10-04 02:55:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 02:55:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 6.5207, fwd=349.3395 cmp_logits=0.2446, sampling=256.2308, total=612.3378
INFO 10-04 02:55:43 worker.py:164] Peak: 13.385 GB, Initial: 38.980 GB, Free: 25.595 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.039 GB
INFO 10-04 02:55:43 gpu_executor.py:117] # GPU blocks: 3077, # CPU blocks: 8192
INFO 10-04 02:56:14 worker.py:189] _init_cache_engine took 24.0625 GB
INFO 10-04 02:56:14 scheduler.py:307] Scheduler initialized with prompt limit: 8192
INFO 10-04 02:56:14 Start warmup...
INFO 10-04 02:56:14 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:14 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 02:56:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9117, fwd=1887.1479 cmp_logits=70.9808, sampling=0.9546, total=1959.9972
INFO 10-04 02:56:16 metrics.py:335] Avg prompt throughput: 520.7 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 1966.8
INFO 10-04 02:56:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1960.5420 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3071, fwd=39.2089 cmp_logits=0.1259, sampling=6.4988, total=46.1431
INFO 10-04 02:56:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 47.0
INFO 10-04 02:56:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.5946 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3281, fwd=8.1003 cmp_logits=0.0770, sampling=6.1593, total=14.6668
INFO 10-04 02:56:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.3
INFO 10-04 02:56:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.9946 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2823, fwd=8.0099 cmp_logits=0.0777, sampling=6.1984, total=14.5698
INFO 10-04 02:56:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.0
INFO 10-04 02:56:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.8287 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2739, fwd=8.0178 cmp_logits=0.0854, sampling=6.2022, total=14.5810
INFO 10-04 02:56:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.0
INFO 10-04 02:56:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.8265 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2770, fwd=8.1053 cmp_logits=0.0703, sampling=6.1367, total=14.5917
INFO 10-04 02:56:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.0
INFO 10-04 02:56:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.8184 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2778, fwd=8.0795 cmp_logits=0.0846, sampling=6.1281, total=14.5714
INFO 10-04 02:56:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.0
INFO 10-04 02:56:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.8003 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2673, fwd=8.0850 cmp_logits=0.0820, sampling=6.1057, total=14.5411
INFO 10-04 02:56:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.9
INFO 10-04 02:56:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.7884 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2666, fwd=8.0123 cmp_logits=0.0679, sampling=6.2103, total=14.5581
INFO 10-04 02:56:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.9
INFO 10-04 02:56:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.7576 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2630, fwd=8.1325 cmp_logits=0.0675, sampling=6.0916, total=14.5557
INFO 10-04 02:56:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.9
INFO 10-04 02:56:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.7755 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:21 Start benchmarking...
INFO 10-04 02:56:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:21 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 02:56:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.7200, fwd=1611.1796 cmp_logits=0.1557, sampling=289.8099, total=1902.8668
INFO 10-04 02:56:23 metrics.py:335] Avg prompt throughput: 2100.4 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 11.3%, CPU KV cache usage: 0.0%, Interval(ms): 1950.1
INFO 10-04 02:56:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1903.6655 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 02:56:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 02:56:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.6844, fwd=14.5116 cmp_logits=0.0868, sampling=277.6656, total=293.9494
INFO 10-04 02:56:23 metrics.py:335] Avg prompt throughput: 13885.6 tokens/s, Avg generation throughput: 17.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 294.8
INFO 10-04 02:56:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 294.4953 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.7309, fwd=14.3421 cmp_logits=0.0865, sampling=258.9822, total=275.1427
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 14824.2 tokens/s, Avg generation throughput: 29.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 31.1%, CPU KV cache usage: 0.0%, Interval(ms): 276.0
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 275.7974 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.6780, fwd=14.4916 cmp_logits=0.0870, sampling=278.5571, total=294.8143
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 13828.5 tokens/s, Avg generation throughput: 30.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.1%, CPU KV cache usage: 0.0%, Interval(ms): 295.6
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 295.4412 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1864]), positions.shape=torch.Size([1864]) hidden_states.shape=torch.Size([1864, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1537, fwd=14.7715 cmp_logits=0.0868, sampling=145.3292, total=161.3426
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 11444.3 tokens/s, Avg generation throughput: 61.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.2%, CPU KV cache usage: 0.0%, Interval(ms): 162.1
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 161.9046 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4694, fwd=7.9467 cmp_logits=0.0689, sampling=12.5480, total=21.0340
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5862 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4690, fwd=7.9269 cmp_logits=0.0679, sampling=12.5098, total=20.9744
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5466 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5081, fwd=7.8633 cmp_logits=0.0691, sampling=12.5706, total=21.0121
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5709 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4616, fwd=7.8447 cmp_logits=0.0677, sampling=12.6023, total=20.9773
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 461.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5096 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4604, fwd=7.9553 cmp_logits=0.0684, sampling=12.4972, total=20.9820
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5230 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4692, fwd=7.9000 cmp_logits=0.0689, sampling=12.5473, total=20.9861
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5516 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4716, fwd=7.8189 cmp_logits=0.0694, sampling=12.6257, total=20.9863
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5247 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4678, fwd=7.8809 cmp_logits=0.0689, sampling=12.6572, total=21.0757
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 458.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6198 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4649, fwd=7.9498 cmp_logits=0.0687, sampling=12.4776, total=20.9620
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5344 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4935, fwd=7.8723 cmp_logits=0.0679, sampling=12.6011, total=21.0359
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5688 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4683, fwd=7.9486 cmp_logits=0.0672, sampling=12.5160, total=21.0011
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5709 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4685, fwd=7.9374 cmp_logits=0.0682, sampling=12.5341, total=21.0092
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5430 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4907, fwd=7.8509 cmp_logits=0.0684, sampling=12.5990, total=21.0097
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5516 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4592, fwd=7.8270 cmp_logits=0.0679, sampling=12.6274, total=20.9827
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5178 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4606, fwd=7.9176 cmp_logits=0.0677, sampling=12.5389, total=20.9861
INFO 10-04 02:56:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 02:56:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5228 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4640, fwd=7.8938 cmp_logits=0.0670, sampling=12.5988, total=21.0245
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5921 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4759, fwd=7.8795 cmp_logits=0.0687, sampling=12.6269, total=21.0519
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5840 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4640, fwd=7.9370 cmp_logits=0.0699, sampling=12.6188, total=21.0905
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 458.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6324 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4637, fwd=7.8974 cmp_logits=0.0682, sampling=12.6951, total=21.1253
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 457.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.9
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6894 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4878, fwd=7.8957 cmp_logits=0.0672, sampling=12.6355, total=21.0874
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 458.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6224 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4640, fwd=7.9181 cmp_logits=0.0687, sampling=12.6233, total=21.0752
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 458.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6491 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4606, fwd=7.8692 cmp_logits=0.0679, sampling=12.6929, total=21.0919
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 458.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6255 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4730, fwd=7.8714 cmp_logits=0.0677, sampling=12.7547, total=21.1675
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 456.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.9
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.7264 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4942, fwd=7.8826 cmp_logits=0.0670, sampling=12.6486, total=21.0934
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 458.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6413 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4802, fwd=7.9203 cmp_logits=0.0691, sampling=12.3377, total=20.8082
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 418.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.3139 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4468, fwd=7.8888 cmp_logits=0.0682, sampling=12.3937, total=20.7982
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 417.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.3671 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4463, fwd=7.8223 cmp_logits=0.0710, sampling=12.4166, total=20.7567
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2693 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4451, fwd=7.9064 cmp_logits=0.0679, sampling=12.3384, total=20.7586
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2784 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4468, fwd=7.9274 cmp_logits=0.0677, sampling=12.3465, total=20.7894
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2922 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4718, fwd=7.8530 cmp_logits=0.0677, sampling=12.3949, total=20.7882
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 418.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.3227 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4056, fwd=7.9803 cmp_logits=0.0801, sampling=11.7075, total=20.1745
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 335.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6227 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4072, fwd=7.8793 cmp_logits=0.0668, sampling=11.8039, total=20.1581
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5996 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4051, fwd=7.8471 cmp_logits=0.0672, sampling=11.8268, total=20.1471
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.7
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5882 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4025, fwd=7.9012 cmp_logits=0.0670, sampling=11.7810, total=20.1523
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5960 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4056, fwd=7.9589 cmp_logits=0.0670, sampling=11.7350, total=20.1678
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6099 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4308, fwd=7.8514 cmp_logits=0.0672, sampling=11.9212, total=20.2713
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 335.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7140 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4275, fwd=7.8006 cmp_logits=0.0670, sampling=11.8916, total=20.1871
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6714 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4020, fwd=7.8809 cmp_logits=0.0682, sampling=11.8055, total=20.1576
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6089 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4013, fwd=7.9019 cmp_logits=0.0672, sampling=11.7791, total=20.1504
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5963 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4075, fwd=7.8616 cmp_logits=0.0672, sampling=11.8177, total=20.1552
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5994 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4048, fwd=7.9231 cmp_logits=0.0708, sampling=11.7822, total=20.1819
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6239 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4060, fwd=7.8781 cmp_logits=0.0677, sampling=11.8532, total=20.2060
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6571 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4036, fwd=7.8542 cmp_logits=0.0670, sampling=11.8518, total=20.1778
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6242 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4120, fwd=7.8735 cmp_logits=0.0677, sampling=11.8370, total=20.1914
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6740 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4039, fwd=7.9076 cmp_logits=0.0801, sampling=11.5323, total=19.9249
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 291.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3609 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3834, fwd=8.0130 cmp_logits=0.0672, sampling=11.4691, total=19.9335
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3466 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3846, fwd=7.8382 cmp_logits=0.0803, sampling=11.5850, total=19.8891
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3085 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3867, fwd=7.8669 cmp_logits=0.0675, sampling=11.6127, total=19.9347
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3640 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3841, fwd=7.8609 cmp_logits=0.0820, sampling=11.5688, total=19.8967
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3142 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3855, fwd=7.8514 cmp_logits=0.0668, sampling=11.5731, total=19.8777
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2973 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3831, fwd=7.9088 cmp_logits=0.0725, sampling=11.5435, total=19.9089
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3230 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3819, fwd=7.8399 cmp_logits=0.0672, sampling=11.5860, total=19.8758
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.4
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2887 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3800, fwd=7.8604 cmp_logits=0.0677, sampling=11.5743, total=19.8832
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2971 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3848, fwd=7.8802 cmp_logits=0.0670, sampling=11.5480, total=19.8810
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2978 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3860, fwd=7.8690 cmp_logits=0.0665, sampling=11.5426, total=19.8650
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.4
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2885 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4227, fwd=7.9784 cmp_logits=0.0703, sampling=11.4882, total=19.9609
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3793 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3822, fwd=7.8800 cmp_logits=0.0675, sampling=11.5175, total=19.8486
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.4
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2680 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3881, fwd=7.9262 cmp_logits=0.0732, sampling=11.5073, total=19.8960
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3123 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3994, fwd=7.9346 cmp_logits=0.0675, sampling=11.4822, total=19.8848
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3471 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3843, fwd=7.8683 cmp_logits=0.0665, sampling=11.6553, total=19.9757
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3907 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3972, fwd=7.9079 cmp_logits=0.0670, sampling=11.5602, total=19.9330
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3843 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3841, fwd=7.8812 cmp_logits=0.0672, sampling=11.5802, total=19.9134
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3233 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3815, fwd=7.9730 cmp_logits=0.0668, sampling=11.4708, total=19.8929
INFO 10-04 02:56:25 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3307 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3936, fwd=7.8390 cmp_logits=0.0756, sampling=11.6076, total=19.9168
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3295 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3808, fwd=7.8979 cmp_logits=0.0679, sampling=11.5430, total=19.8903
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3080 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3841, fwd=7.9482 cmp_logits=0.0672, sampling=11.5609, total=19.9614
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3712 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3855, fwd=7.8788 cmp_logits=0.0796, sampling=11.5659, total=19.9111
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3238 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3829, fwd=7.9393 cmp_logits=0.0675, sampling=11.5356, total=19.9261
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3347 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3853, fwd=7.8821 cmp_logits=0.0746, sampling=11.5809, total=19.9237
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3359 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3793, fwd=7.9086 cmp_logits=0.0670, sampling=11.5688, total=19.9246
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3404 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3920, fwd=7.8571 cmp_logits=0.0670, sampling=11.6045, total=19.9213
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3726 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4001, fwd=7.8816 cmp_logits=0.0691, sampling=11.6162, total=19.9680
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 291.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.4279 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3927, fwd=7.9489 cmp_logits=0.0670, sampling=11.5304, total=19.9399
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3526 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3798, fwd=7.8399 cmp_logits=0.0679, sampling=11.6372, total=19.9261
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3650 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3910, fwd=7.8621 cmp_logits=0.0682, sampling=11.6184, total=19.9401
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3819 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3653, fwd=7.8766 cmp_logits=0.0677, sampling=11.3189, total=19.6295
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 246.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.3
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0748 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3607, fwd=7.8530 cmp_logits=0.0675, sampling=11.2982, total=19.5801
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9864 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3612, fwd=7.8933 cmp_logits=0.0663, sampling=11.2975, total=19.6192
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0098 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3650, fwd=7.8182 cmp_logits=0.0672, sampling=11.3535, total=19.6049
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9862 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3593, fwd=7.7837 cmp_logits=0.0660, sampling=11.4338, total=19.6443
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0219 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3672, fwd=7.8070 cmp_logits=0.0803, sampling=11.3673, total=19.6228
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0381 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3610, fwd=7.8683 cmp_logits=0.0670, sampling=11.2548, total=19.5515
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 249.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9316 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3808, fwd=7.8900 cmp_logits=0.0668, sampling=11.2402, total=19.5789
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9637 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3684, fwd=7.8478 cmp_logits=0.0670, sampling=11.2872, total=19.5713
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9749 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3607, fwd=7.7891 cmp_logits=0.0672, sampling=11.3273, total=19.5453
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9363 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3684, fwd=7.8084 cmp_logits=0.0663, sampling=11.3277, total=19.5715
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0040 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3614, fwd=7.8030 cmp_logits=0.0668, sampling=11.3461, total=19.5785
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9907 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3722, fwd=7.9036 cmp_logits=0.0694, sampling=11.2512, total=19.5971
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9857 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3731, fwd=7.8602 cmp_logits=0.0677, sampling=11.2846, total=19.5866
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9640 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3741, fwd=7.7810 cmp_logits=0.0658, sampling=11.3783, total=19.5997
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9757 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3886, fwd=7.7713 cmp_logits=0.0675, sampling=11.3823, total=19.6106
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0388 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3872, fwd=7.8034 cmp_logits=0.0675, sampling=11.3406, total=19.5997
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9845 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3622, fwd=7.8132 cmp_logits=0.0665, sampling=11.3230, total=19.5656
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9463 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3610, fwd=7.9010 cmp_logits=0.0672, sampling=11.2433, total=19.5735
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9556 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3624, fwd=7.8120 cmp_logits=0.0672, sampling=11.3187, total=19.5611
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9611 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3655, fwd=7.7796 cmp_logits=0.0663, sampling=11.3263, total=19.5384
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9604 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3643, fwd=7.8816 cmp_logits=0.0665, sampling=11.2417, total=19.5553
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9366 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3624, fwd=7.8013 cmp_logits=0.0658, sampling=12.3377, total=20.5679
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 235.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0826 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3674, fwd=7.9663 cmp_logits=0.0696, sampling=11.2309, total=19.6350
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0222 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3643, fwd=7.7846 cmp_logits=0.0670, sampling=11.3459, total=19.5627
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9435 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3669, fwd=7.7596 cmp_logits=0.0758, sampling=11.3707, total=19.5739
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9664 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3629, fwd=7.8177 cmp_logits=0.0789, sampling=11.3013, total=19.5615
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9430 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3650, fwd=7.8120 cmp_logits=0.0751, sampling=11.3044, total=19.5575
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9378 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3850, fwd=7.8077 cmp_logits=0.0665, sampling=11.3053, total=19.5656
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9559 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3633, fwd=7.8599 cmp_logits=0.0670, sampling=11.2717, total=19.5630
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9442 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3626, fwd=7.8115 cmp_logits=0.0672, sampling=11.2944, total=19.5370
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9227 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3645, fwd=7.7786 cmp_logits=0.0660, sampling=11.3215, total=19.5317
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9161 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3645, fwd=7.8661 cmp_logits=0.0668, sampling=11.2622, total=19.5606
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9397 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3638, fwd=7.8681 cmp_logits=0.0672, sampling=11.2488, total=19.5489
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9606 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3693, fwd=7.8702 cmp_logits=0.0672, sampling=11.2090, total=19.5167
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 249.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9251 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3641, fwd=7.8669 cmp_logits=0.0668, sampling=11.2481, total=19.5467
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9363 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3757, fwd=7.8278 cmp_logits=0.0677, sampling=11.3227, total=19.5949
INFO 10-04 02:56:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9757 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3717, fwd=7.7589 cmp_logits=0.0741, sampling=11.3294, total=19.5353
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 249.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9165 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3822, fwd=7.7724 cmp_logits=0.0672, sampling=11.3759, total=19.5987
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9766 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3672, fwd=7.8490 cmp_logits=0.0672, sampling=11.3256, total=19.6099
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.7 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0303 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3676, fwd=7.8821 cmp_logits=0.0811, sampling=10.2224, total=18.5540
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9428 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3195, fwd=7.8397 cmp_logits=0.0672, sampling=9.8550, total=18.0821
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3828 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3185, fwd=7.8750 cmp_logits=0.0675, sampling=9.8603, total=18.1220
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4197 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3304, fwd=7.8382 cmp_logits=0.0668, sampling=9.8472, total=18.0838
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4064 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3219, fwd=7.8967 cmp_logits=0.0663, sampling=9.8288, total=18.1143
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4121 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3200, fwd=7.8728 cmp_logits=0.0660, sampling=9.8062, total=18.0659
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3878 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3231, fwd=7.8568 cmp_logits=0.0665, sampling=9.8536, total=18.1012
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4295 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2956, fwd=7.9648 cmp_logits=0.0794, sampling=9.4657, total=17.8065
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0814 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2937, fwd=7.9091 cmp_logits=0.0670, sampling=9.5515, total=17.8223
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0776 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2933, fwd=7.8163 cmp_logits=0.0665, sampling=9.6161, total=17.7929
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0457 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2975, fwd=7.8199 cmp_logits=0.0656, sampling=9.6033, total=17.7872
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0497 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2956, fwd=7.8931 cmp_logits=0.0656, sampling=9.5465, total=17.8015
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0564 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2942, fwd=7.8328 cmp_logits=0.0663, sampling=9.5804, total=17.7743
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0411 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2956, fwd=7.8454 cmp_logits=0.0656, sampling=9.6087, total=17.8161
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0848 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2942, fwd=7.8330 cmp_logits=0.0658, sampling=9.6269, total=17.8208
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0769 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3047, fwd=7.9305 cmp_logits=0.0665, sampling=9.5229, total=17.8258
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0898 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2935, fwd=7.8578 cmp_logits=0.0784, sampling=9.5775, total=17.8082
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0612 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3028, fwd=7.8132 cmp_logits=0.0656, sampling=9.6233, total=17.8058
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0809 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2952, fwd=7.8776 cmp_logits=0.0668, sampling=9.5615, total=17.8020
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0533 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2968, fwd=7.8433 cmp_logits=0.0663, sampling=9.6304, total=17.8382
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0931 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2968, fwd=7.8776 cmp_logits=0.0670, sampling=9.5863, total=17.8287
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0955 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2944, fwd=7.8661 cmp_logits=0.0670, sampling=9.5813, total=17.8099
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0655 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2961, fwd=7.8950 cmp_logits=0.0668, sampling=9.5525, total=17.8113
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0762 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2966, fwd=7.8616 cmp_logits=0.0663, sampling=9.5785, total=17.8039
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0659 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2949, fwd=7.8831 cmp_logits=0.0663, sampling=9.5897, total=17.8347
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0898 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2971, fwd=7.9417 cmp_logits=0.0665, sampling=9.5232, total=17.8294
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0902 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2949, fwd=7.8814 cmp_logits=0.0784, sampling=9.5468, total=17.8025
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0576 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2949, fwd=7.7825 cmp_logits=0.0784, sampling=9.6364, total=17.7934
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0497 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2947, fwd=7.8509 cmp_logits=0.0658, sampling=9.5911, total=17.8034
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0764 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2947, fwd=7.8123 cmp_logits=0.0658, sampling=9.6233, total=17.7968
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0542 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2992, fwd=7.8301 cmp_logits=0.0658, sampling=9.7241, total=17.9198
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1794 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2971, fwd=7.8793 cmp_logits=0.0663, sampling=9.6588, total=17.9024
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1577 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2966, fwd=7.8573 cmp_logits=0.0658, sampling=9.6686, total=17.8893
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.2147 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2687, fwd=8.0380 cmp_logits=0.0682, sampling=9.5296, total=17.9055
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1124 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2677, fwd=8.0879 cmp_logits=0.0687, sampling=9.5105, total=17.9358
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1391 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2687, fwd=8.0428 cmp_logits=0.0668, sampling=9.5320, total=17.9110
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1174 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2699, fwd=8.0392 cmp_logits=0.0694, sampling=9.5735, total=17.9529
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1804 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2685, fwd=8.0030 cmp_logits=0.0670, sampling=9.5892, total=17.9286
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1386 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2680, fwd=8.0690 cmp_logits=0.0792, sampling=9.5057, total=17.9229
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1260 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2944, fwd=8.1134 cmp_logits=0.0668, sampling=9.4655, total=17.9410
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1427 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2697, fwd=8.0271 cmp_logits=0.0665, sampling=9.5787, total=17.9434
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1713 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2675, fwd=8.1086 cmp_logits=0.0668, sampling=9.4635, total=17.9074
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1129 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2680, fwd=8.0733 cmp_logits=0.0670, sampling=9.4790, total=17.8883
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0922 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2666, fwd=8.0020 cmp_logits=0.0801, sampling=9.5687, total=17.9183
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1220 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2687, fwd=8.0459 cmp_logits=0.0668, sampling=9.5377, total=17.9203
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1234 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2682, fwd=8.0309 cmp_logits=0.0668, sampling=9.5665, total=17.9334
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1417 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2701, fwd=7.9799 cmp_logits=0.0675, sampling=9.6104, total=17.9288
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1322 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2944, fwd=8.0309 cmp_logits=0.0677, sampling=9.5363, total=17.9305
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1334 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2685, fwd=7.9594 cmp_logits=0.0672, sampling=9.6536, total=17.9496
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1754 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2682, fwd=8.0512 cmp_logits=0.0670, sampling=9.5155, total=17.9029
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1053 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2694, fwd=8.0748 cmp_logits=0.0761, sampling=9.5379, total=17.9589
INFO 10-04 02:56:27 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1649 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2882, fwd=8.0216 cmp_logits=0.0706, sampling=9.5484, total=17.9298
INFO 10-04 02:56:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1324 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2759, fwd=7.9820 cmp_logits=0.0670, sampling=9.6059, total=17.9317
INFO 10-04 02:56:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1515 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2654, fwd=8.0879 cmp_logits=0.0668, sampling=9.4926, total=17.9136
INFO 10-04 02:56:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1170 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2689, fwd=7.9944 cmp_logits=0.0665, sampling=9.5804, total=17.9110
INFO 10-04 02:56:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1150 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3045, fwd=8.0066 cmp_logits=0.0675, sampling=9.5735, total=17.9529
INFO 10-04 02:56:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1603 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2794, fwd=8.0683 cmp_logits=0.0668, sampling=9.5229, total=17.9381
INFO 10-04 02:56:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1780 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2687, fwd=8.0183 cmp_logits=0.0668, sampling=9.5756, total=17.9303
INFO 10-04 02:56:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1332 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2677, fwd=8.0304 cmp_logits=0.0670, sampling=9.5501, total=17.9164
INFO 10-04 02:56:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1191 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2670, fwd=8.0326 cmp_logits=0.0706, sampling=9.5181, total=17.8895
INFO 10-04 02:56:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 02:56:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0922 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2706, fwd=8.0447 cmp_logits=0.0677, sampling=9.5518, total=17.9355
INFO 10-04 02:56:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1420 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 02:56:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 02:56:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 02:56:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2685, fwd=8.0860 cmp_logits=0.0675, sampling=9.5050, total=17.9279
INFO 10-04 02:56:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 02:56:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1835 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728010581.6994598, last_token_time=1728010585.1901934, first_scheduled_time=1728010581.7361975, first_token_time=1728010583.639254, time_in_queue=0.036737680435180664, finished_time=1728010585.190154, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728010581.7015262, last_token_time=1728010586.2456505, first_scheduled_time=1728010581.7361975, first_token_time=1728010583.639254, time_in_queue=0.03467130661010742, finished_time=1728010586.2456243, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728010581.704798, last_token_time=1728010585.6102302, first_scheduled_time=1728010581.7361975, first_token_time=1728010583.639254, time_in_queue=0.03139948844909668, finished_time=1728010585.6102064, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728010581.7065606, last_token_time=1728010587.0711763, first_scheduled_time=1728010581.7361975, first_token_time=1728010583.9340277, time_in_queue=0.029636859893798828, finished_time=1728010587.0711596, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728010581.7113254, last_token_time=1728010587.0520003, first_scheduled_time=1728010583.639919, first_token_time=1728010583.9340277, time_in_queue=1.928593635559082, finished_time=1728010587.0519853, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728010581.715743, last_token_time=1728010585.3190942, first_scheduled_time=1728010583.639919, first_token_time=1728010584.209891, time_in_queue=1.9241759777069092, finished_time=1728010585.3190758, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728010581.7169597, last_token_time=1728010585.3190942, first_scheduled_time=1728010583.9345949, first_token_time=1728010584.209891, time_in_queue=2.217635154724121, finished_time=1728010585.31908, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728010581.7181385, last_token_time=1728010587.1825876, first_scheduled_time=1728010583.9345949, first_token_time=1728010584.209891, time_in_queue=2.216456413269043, finished_time=1728010587.182575, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728010581.7228973, last_token_time=1728010588.1868806, first_scheduled_time=1728010583.9345949, first_token_time=1728010584.5054927, time_in_queue=2.211697578430176, finished_time=1728010588.186878, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728010581.730044, last_token_time=1728010587.6566792, first_scheduled_time=1728010584.2105186, first_token_time=1728010584.6676078, time_in_queue=2.4804747104644775, finished_time=1728010587.6566775, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 19139
End-to-End latency: 6.49 seconds
Throughput: 1.54 requests/s, 2950.07 tokens/s
Per_token_time: 0.339 ms

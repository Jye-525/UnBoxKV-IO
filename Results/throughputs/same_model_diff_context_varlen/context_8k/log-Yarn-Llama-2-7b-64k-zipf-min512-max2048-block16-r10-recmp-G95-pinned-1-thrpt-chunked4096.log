Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=8192, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=4096, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Selected required requests 10
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Chunked prefill is enabled. max_num_batched_tokens=4096, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings', 'finetuned', 'type'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 03:20:37 config.py:654] [SchedulerConfig] max_num_batched_tokens: 4096 chunked_prefill_enabled: True
INFO 10-04 03:20:37 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 03:20:38 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 03:20:42 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 03:20:46 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 03:20:46 model_runner.py:183] Loaded model: 
INFO 10-04 03:20:46 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 03:20:46 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 03:20:46 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:20:46 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 03:20:46 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 03:20:46 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 03:20:46 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:20:46 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:20:46 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 03:20:46 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 03:20:46 model_runner.py:183]         )
INFO 10-04 03:20:46 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 03:20:46 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:20:46 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:20:46 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 03:20:46 model_runner.py:183]         )
INFO 10-04 03:20:46 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:20:46 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:20:46 model_runner.py:183]       )
INFO 10-04 03:20:46 model_runner.py:183]     )
INFO 10-04 03:20:46 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:20:46 model_runner.py:183]   )
INFO 10-04 03:20:46 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:20:46 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 03:20:46 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 03:20:46 model_runner.py:183] )
INFO 10-04 03:20:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 03:20:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 3.4246, fwd=364.2499 cmp_logits=0.2370, sampling=257.6959, total=625.6092
INFO 10-04 03:20:47 worker.py:164] Peak: 13.385 GB, Initial: 38.980 GB, Free: 25.595 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.039 GB
INFO 10-04 03:20:47 gpu_executor.py:117] # GPU blocks: 3077, # CPU blocks: 8192
INFO 10-04 03:21:18 worker.py:189] _init_cache_engine took 24.0625 GB
INFO 10-04 03:21:18 scheduler.py:307] Scheduler initialized with prompt limit: 8192
INFO 10-04 03:21:18 Start warmup...
INFO 10-04 03:21:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:21:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8874, fwd=1905.2165 cmp_logits=71.0073, sampling=0.9539, total=1978.0674
INFO 10-04 03:21:20 metrics.py:335] Avg prompt throughput: 515.9 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 1984.8
INFO 10-04 03:21:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1978.5886 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3045, fwd=43.7024 cmp_logits=0.1171, sampling=6.5653, total=50.6909
INFO 10-04 03:21:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 51.5
INFO 10-04 03:21:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 51.1200 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3052, fwd=8.1112 cmp_logits=0.0715, sampling=7.0584, total=15.5478
INFO 10-04 03:21:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 03:21:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8291 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2708, fwd=8.0709 cmp_logits=0.0694, sampling=7.0798, total=15.4924
INFO 10-04 03:21:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:21:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7223 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2711, fwd=8.0640 cmp_logits=0.0694, sampling=6.4335, total=14.8396
INFO 10-04 03:21:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.2
INFO 10-04 03:21:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.0583 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2639, fwd=8.0411 cmp_logits=0.0691, sampling=5.9252, total=14.3008
INFO 10-04 03:21:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:21:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5159 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2704, fwd=8.1055 cmp_logits=0.0739, sampling=5.8451, total=14.2961
INFO 10-04 03:21:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:21:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5166 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2630, fwd=8.0760 cmp_logits=0.0677, sampling=5.8732, total=14.2808
INFO 10-04 03:21:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:21:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5221 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2592, fwd=8.0330 cmp_logits=0.0682, sampling=5.9404, total=14.3018
INFO 10-04 03:21:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 03:21:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4994 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2584, fwd=8.0490 cmp_logits=0.0675, sampling=5.8994, total=14.2753
INFO 10-04 03:21:20 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 03:21:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4897 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:25 Start benchmarking...
INFO 10-04 03:21:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 03:21:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.6637, fwd=1575.2676 cmp_logits=0.1559, sampling=267.0743, total=1844.1627
INFO 10-04 03:21:27 metrics.py:335] Avg prompt throughput: 2183.1 tokens/s, Avg generation throughput: 2.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 9.8%, CPU KV cache usage: 0.0%, Interval(ms): 1876.2
INFO 10-04 03:21:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1844.8718 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.6916, fwd=14.5824 cmp_logits=0.0863, sampling=244.4093, total=260.7710
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 15637.5 tokens/s, Avg generation throughput: 30.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 261.7
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 261.4136 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2478]), positions.shape=torch.Size([2478]) hidden_states.shape=torch.Size([2478, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.2453, fwd=14.7114 cmp_logits=0.0849, sampling=160.8596, total=176.9021
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 13903.7 tokens/s, Avg generation throughput: 56.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.8%, CPU KV cache usage: 0.0%, Interval(ms): 177.7
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 177.4659 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4218, fwd=8.0407 cmp_logits=0.0691, sampling=9.0029, total=17.5352
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0342 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4141, fwd=7.9932 cmp_logits=0.0672, sampling=9.0420, total=17.5176
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0140 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4184, fwd=7.9465 cmp_logits=0.0679, sampling=9.0904, total=17.5242
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0466 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4234, fwd=7.9818 cmp_logits=0.0687, sampling=9.0644, total=17.5393
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0538 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4139, fwd=7.9849 cmp_logits=0.0687, sampling=9.0501, total=17.5185
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0230 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4122, fwd=7.9584 cmp_logits=0.0687, sampling=9.0821, total=17.5223
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0302 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4122, fwd=7.9558 cmp_logits=0.0682, sampling=9.0730, total=17.5102
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0233 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4132, fwd=7.9997 cmp_logits=0.0689, sampling=9.0451, total=17.5281
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0359 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4141, fwd=7.9761 cmp_logits=0.0675, sampling=9.0725, total=17.5309
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0366 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4125, fwd=7.9651 cmp_logits=0.0687, sampling=9.0742, total=17.5214
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0290 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4115, fwd=8.0061 cmp_logits=0.0687, sampling=9.1374, total=17.6249
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 546.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1291 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4108, fwd=7.9322 cmp_logits=0.0668, sampling=9.1274, total=17.5383
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0461 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4277, fwd=7.9384 cmp_logits=0.0694, sampling=9.1267, total=17.5631
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0655 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4115, fwd=7.9103 cmp_logits=0.0682, sampling=9.1424, total=17.5333
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0340 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4103, fwd=7.9081 cmp_logits=0.0682, sampling=9.1441, total=17.5316
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0366 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4125, fwd=7.9162 cmp_logits=0.0679, sampling=9.1481, total=17.5455
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0542 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4115, fwd=7.9479 cmp_logits=0.0679, sampling=9.1271, total=17.5552
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0547 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4165, fwd=7.9348 cmp_logits=0.0675, sampling=9.1505, total=17.5703
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1010 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4144, fwd=7.9160 cmp_logits=0.0665, sampling=9.1588, total=17.5567
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0581 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4182, fwd=7.9172 cmp_logits=0.0682, sampling=9.1524, total=17.5569
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0616 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4113, fwd=7.9427 cmp_logits=0.0677, sampling=9.1281, total=17.5512
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0533 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4275, fwd=7.9417 cmp_logits=0.0670, sampling=9.1553, total=17.5927
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0917 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4163, fwd=7.9541 cmp_logits=0.0682, sampling=9.1257, total=17.5653
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 546.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1134 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4096, fwd=7.9234 cmp_logits=0.0675, sampling=9.1558, total=17.5574
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0717 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3996, fwd=8.0531 cmp_logits=0.0813, sampling=8.7647, total=17.2997
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 500.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7820 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3951, fwd=8.0037 cmp_logits=0.0687, sampling=8.8000, total=17.2689
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7517 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4029, fwd=7.9851 cmp_logits=0.0670, sampling=8.8241, total=17.2803
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7941 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3726, fwd=8.0354 cmp_logits=0.0799, sampling=8.3170, total=16.8056
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2622 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3679, fwd=8.0054 cmp_logits=0.0675, sampling=8.3187, total=16.7603
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1735 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3679, fwd=7.9668 cmp_logits=0.0672, sampling=8.3911, total=16.7940
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2002 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3669, fwd=7.9961 cmp_logits=0.0670, sampling=8.3508, total=16.7818
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2017 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3688, fwd=8.0202 cmp_logits=0.0677, sampling=8.3859, total=16.8433
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2610 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3633, fwd=7.9911 cmp_logits=0.0679, sampling=8.4407, total=16.8641
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3280 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3517, fwd=8.0185 cmp_logits=0.0813, sampling=8.1673, total=16.6197
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0000 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3579, fwd=7.9234 cmp_logits=0.0675, sampling=8.2662, total=16.6159
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0040 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3567, fwd=7.9389 cmp_logits=0.0668, sampling=8.2402, total=16.6032
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9942 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3521, fwd=7.9303 cmp_logits=0.0665, sampling=8.2648, total=16.6149
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9985 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3507, fwd=7.9741 cmp_logits=0.0670, sampling=8.2223, total=16.6152
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0343 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3524, fwd=7.8952 cmp_logits=0.0665, sampling=8.3101, total=16.6252
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0114 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3545, fwd=7.9579 cmp_logits=0.0670, sampling=8.2452, total=16.6261
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0233 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3526, fwd=7.9625 cmp_logits=0.0663, sampling=8.2512, total=16.6335
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0150 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3550, fwd=7.9803 cmp_logits=0.0670, sampling=8.2123, total=16.6159
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9973 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3529, fwd=7.9477 cmp_logits=0.0663, sampling=8.2653, total=16.6330
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0515 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3521, fwd=7.9670 cmp_logits=0.0675, sampling=8.2369, total=16.6245
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0066 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:21:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3519, fwd=7.9918 cmp_logits=0.0665, sampling=8.1933, total=16.6044
INFO 10-04 03:21:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:21:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9883 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:21:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3529, fwd=7.9696 cmp_logits=0.0670, sampling=8.2400, total=16.6304
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0259 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3376, fwd=8.0173 cmp_logits=0.0780, sampling=7.9114, total=16.3455
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 295.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7017 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3364, fwd=8.0020 cmp_logits=0.0670, sampling=7.8626, total=16.2690
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6233 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3388, fwd=8.0130 cmp_logits=0.0679, sampling=7.8340, total=16.2544
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6073 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3386, fwd=7.9942 cmp_logits=0.0658, sampling=7.8444, total=16.2439
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5915 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3366, fwd=7.9973 cmp_logits=0.0665, sampling=7.8464, total=16.2482
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6030 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3397, fwd=7.9913 cmp_logits=0.0672, sampling=7.8738, total=16.2733
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6242 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3371, fwd=8.0070 cmp_logits=0.0675, sampling=7.8619, total=16.2745
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6261 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3340, fwd=7.9660 cmp_logits=0.0663, sampling=7.8928, total=16.2601
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6099 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3388, fwd=7.9467 cmp_logits=0.0668, sampling=7.8974, total=16.2506
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6035 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3362, fwd=7.9763 cmp_logits=0.0658, sampling=7.8955, total=16.2749
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6264 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3369, fwd=8.0421 cmp_logits=0.0663, sampling=7.8459, total=16.2921
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6447 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3352, fwd=8.0209 cmp_logits=0.0675, sampling=7.8926, total=16.3169
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6628 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3352, fwd=7.9875 cmp_logits=0.0665, sampling=7.8640, total=16.2544
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6066 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3338, fwd=7.9606 cmp_logits=0.0665, sampling=7.8952, total=16.2570
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.6%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6111 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3335, fwd=7.9241 cmp_logits=0.0660, sampling=7.9496, total=16.2742
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.6%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6242 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3407, fwd=7.9052 cmp_logits=0.0665, sampling=7.9556, total=16.2687
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.6%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6147 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3324, fwd=7.9806 cmp_logits=0.0670, sampling=7.9029, total=16.2838
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.5 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6581 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3190, fwd=8.0144 cmp_logits=0.0668, sampling=7.3473, total=15.7483
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.0897 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3002, fwd=7.9958 cmp_logits=0.0772, sampling=6.9954, total=15.3699
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6522 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2985, fwd=7.9525 cmp_logits=0.0765, sampling=7.0267, total=15.3556
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6567 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2832, fwd=8.0082 cmp_logits=0.0794, sampling=6.9246, total=15.2965
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5425 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2854, fwd=7.9861 cmp_logits=0.0663, sampling=6.9861, total=15.3248
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5683 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2797, fwd=7.9210 cmp_logits=0.0663, sampling=7.0055, total=15.2740
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5134 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2856, fwd=7.9708 cmp_logits=0.0663, sampling=6.9685, total=15.2922
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5354 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2840, fwd=7.9093 cmp_logits=0.0653, sampling=7.0324, total=15.2919
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5330 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2925, fwd=7.9172 cmp_logits=0.0665, sampling=7.0286, total=15.3058
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5497 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2899, fwd=7.9811 cmp_logits=0.0668, sampling=6.9885, total=15.3272
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5718 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2835, fwd=7.9377 cmp_logits=0.0656, sampling=6.9849, total=15.2726
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5153 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2813, fwd=7.9546 cmp_logits=0.0675, sampling=6.9656, total=15.2700
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5134 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2842, fwd=7.9556 cmp_logits=0.0660, sampling=6.9599, total=15.2667
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5127 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2809, fwd=7.8924 cmp_logits=0.0658, sampling=7.0412, total=15.2817
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5461 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2618, fwd=8.1143 cmp_logits=0.0665, sampling=7.0102, total=15.4541
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6527 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2599, fwd=8.0905 cmp_logits=0.0668, sampling=7.0090, total=15.4276
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6238 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2611, fwd=8.0972 cmp_logits=0.0668, sampling=6.9964, total=15.4223
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6174 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2596, fwd=8.0891 cmp_logits=0.0663, sampling=7.0083, total=15.4243
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6202 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2618, fwd=8.0912 cmp_logits=0.0672, sampling=7.0088, total=15.4297
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6248 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2589, fwd=8.1563 cmp_logits=0.0665, sampling=7.0064, total=15.4893
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6846 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2604, fwd=8.1129 cmp_logits=0.0665, sampling=6.9826, total=15.4233
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6245 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2596, fwd=8.0709 cmp_logits=0.0665, sampling=7.0162, total=15.4142
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6085 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2596, fwd=8.0872 cmp_logits=0.0682, sampling=6.9914, total=15.4073
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6136 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2687, fwd=8.0223 cmp_logits=0.0672, sampling=7.0899, total=15.4490
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6431 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2613, fwd=8.1742 cmp_logits=0.0682, sampling=6.9633, total=15.4676
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6643 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2623, fwd=8.0738 cmp_logits=0.0668, sampling=7.0171, total=15.4209
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6174 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:21:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:21:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:21:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2596, fwd=8.0838 cmp_logits=0.0660, sampling=7.0171, total=15.4274
INFO 10-04 03:21:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:21:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6488 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012085.8793721, last_token_time=1728012088.6223977, first_scheduled_time=1728012085.9009101, first_token_time=1728012087.745249, time_in_queue=0.02153801918029785, finished_time=1728012088.6223578, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012085.8814125, last_token_time=1728012089.0040095, first_scheduled_time=1728012085.9009101, first_token_time=1728012087.745249, time_in_queue=0.01949763298034668, finished_time=1728012089.0039833, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012085.88331, last_token_time=1728012088.7807465, first_scheduled_time=1728012085.9009101, first_token_time=1728012087.745249, time_in_queue=0.017600059509277344, finished_time=1728012088.7807217, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012085.884586, last_token_time=1728012089.2893186, first_scheduled_time=1728012085.9009101, first_token_time=1728012087.745249, time_in_queue=0.01632404327392578, finished_time=1728012089.2892964, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012085.8872085, last_token_time=1728012089.3055992, first_scheduled_time=1728012085.9009101, first_token_time=1728012088.0068898, time_in_queue=0.013701677322387695, finished_time=1728012089.3055813, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012085.8895812, last_token_time=1728012088.6762807, first_scheduled_time=1728012087.7459621, first_token_time=1728012088.0068898, time_in_queue=1.8563809394836426, finished_time=1728012088.6762614, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012085.8906062, last_token_time=1728012088.6762807, first_scheduled_time=1728012087.7459621, first_token_time=1728012088.0068898, time_in_queue=1.855355978012085, finished_time=1728012088.676266, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012085.8916411, last_token_time=1728012089.3372421, first_scheduled_time=1728012087.7459621, first_token_time=1728012088.0068898, time_in_queue=1.854321002960205, finished_time=1728012089.3372293, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012085.8941603, last_token_time=1728012089.7147977, first_scheduled_time=1728012087.7459621, first_token_time=1728012088.1845207, time_in_queue=1.851801872253418, finished_time=1728012089.7147954, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012085.8976653, last_token_time=1728012089.509749, first_scheduled_time=1728012088.0074751, first_token_time=1728012088.1845207, time_in_queue=2.1098098754882812, finished_time=1728012089.5097473, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 11197
End-to-End latency: 3.84 seconds
Throughput: 2.61 requests/s, 2919.21 tokens/s
Per_token_time: 0.343 ms

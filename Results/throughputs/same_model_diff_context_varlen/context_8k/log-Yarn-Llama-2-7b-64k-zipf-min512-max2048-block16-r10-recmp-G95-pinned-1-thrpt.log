Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=8192, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=8192, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Selected required requests 10
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Running vLLM with default batching strategy. max_num_batched_tokens=8192, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'finetuned', 'original_max_position_embeddings', 'type'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 03:17:54 config.py:654] [SchedulerConfig] max_num_batched_tokens: 8192 chunked_prefill_enabled: False
INFO 10-04 03:17:54 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 03:17:54 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 03:17:59 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 03:18:17 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 03:18:17 model_runner.py:183] Loaded model: 
INFO 10-04 03:18:17 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 03:18:17 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 03:18:17 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:18:17 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 03:18:17 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 03:18:17 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 03:18:17 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:18:17 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:18:17 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 03:18:17 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 03:18:17 model_runner.py:183]         )
INFO 10-04 03:18:17 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 03:18:17 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:18:17 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:18:17 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 03:18:17 model_runner.py:183]         )
INFO 10-04 03:18:17 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:18:17 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:18:17 model_runner.py:183]       )
INFO 10-04 03:18:17 model_runner.py:183]     )
INFO 10-04 03:18:17 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:18:17 model_runner.py:183]   )
INFO 10-04 03:18:17 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:18:17 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 03:18:17 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 03:18:17 model_runner.py:183] )
INFO 10-04 03:18:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8192]), positions.shape=torch.Size([8192]) hidden_states.shape=torch.Size([8192, 4096]) residual=None
INFO 10-04 03:18:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 14.0140, fwd=372.4422 cmp_logits=0.2503, sampling=449.0273, total=835.7356
INFO 10-04 03:18:18 worker.py:164] Peak: 13.771 GB, Initial: 38.980 GB, Free: 25.208 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 23.653 GB
INFO 10-04 03:18:18 gpu_executor.py:117] # GPU blocks: 3027, # CPU blocks: 8192
INFO 10-04 03:18:49 worker.py:189] _init_cache_engine took 23.6484 GB
INFO 10-04 03:18:49 scheduler.py:307] Scheduler initialized with prompt limit: 8192
INFO 10-04 03:18:49 Start warmup...
INFO 10-04 03:18:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:18:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8783, fwd=11.4880 cmp_logits=72.9816, sampling=0.9456, total=86.2956
INFO 10-04 03:18:49 metrics.py:335] Avg prompt throughput: 11034.7 tokens/s, Avg generation throughput: 10.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 92.8
INFO 10-04 03:18:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.7026 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3006, fwd=43.3497 cmp_logits=0.1235, sampling=6.1829, total=49.9587
INFO 10-04 03:18:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 50.8
INFO 10-04 03:18:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.3795 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3228, fwd=8.1854 cmp_logits=0.0799, sampling=6.4342, total=15.0242
INFO 10-04 03:18:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:18:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3146 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2780, fwd=8.0478 cmp_logits=0.0715, sampling=6.5286, total=14.9274
INFO 10-04 03:18:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.4
INFO 10-04 03:18:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.1441 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2723, fwd=8.0163 cmp_logits=0.0689, sampling=6.5801, total=14.9393
INFO 10-04 03:18:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.3
INFO 10-04 03:18:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.1467 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2706, fwd=7.8795 cmp_logits=0.0699, sampling=6.7232, total=14.9441
INFO 10-04 03:18:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.3
INFO 10-04 03:18:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.1496 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2904, fwd=8.2457 cmp_logits=0.0775, sampling=6.3639, total=14.9789
INFO 10-04 03:18:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.4
INFO 10-04 03:18:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.1954 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2806, fwd=8.2438 cmp_logits=0.0718, sampling=6.3465, total=14.9436
INFO 10-04 03:18:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.4
INFO 10-04 03:18:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.1713 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2763, fwd=8.2712 cmp_logits=0.0703, sampling=6.3522, total=14.9717
INFO 10-04 03:18:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.3
INFO 10-04 03:18:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.1627 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2766, fwd=8.2970 cmp_logits=0.0694, sampling=6.2847, total=14.9281
INFO 10-04 03:18:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.3
INFO 10-04 03:18:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.1346 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:54 Start benchmarking...
INFO 10-04 03:18:54 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7293]), positions.shape=torch.Size([7293]) hidden_states.shape=torch.Size([7293, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 2.6042, fwd=10.8278 cmp_logits=0.1140, sampling=466.8629, total=480.4106
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 14226.1 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 15.2%, CPU KV cache usage: 0.0%, Interval(ms): 512.6
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 481.0994 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3365]), positions.shape=torch.Size([3365]) hidden_states.shape=torch.Size([3365, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.2195, fwd=9.4156 cmp_logits=0.0813, sampling=191.8886, total=202.6057
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 16563.8 tokens/s, Avg generation throughput: 9.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 203.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 202.9257 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4177, fwd=7.7531 cmp_logits=0.0801, sampling=9.3498, total=17.6013
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1217 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4139, fwd=7.6594 cmp_logits=0.0679, sampling=9.3746, total=17.5169
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9780 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4094, fwd=7.6962 cmp_logits=0.0670, sampling=9.3272, total=17.5004
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9625 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4108, fwd=7.7388 cmp_logits=0.0672, sampling=9.3546, total=17.5724
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0264 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4137, fwd=7.7164 cmp_logits=0.0679, sampling=9.3381, total=17.5371
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0109 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4144, fwd=7.6926 cmp_logits=0.0687, sampling=9.3291, total=17.5054
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9627 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4077, fwd=7.7431 cmp_logits=0.0677, sampling=9.3076, total=17.5266
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9904 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4151, fwd=7.7355 cmp_logits=0.0682, sampling=9.2905, total=17.5099
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9703 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4191, fwd=7.6759 cmp_logits=0.0682, sampling=9.3627, total=17.5269
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9825 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4125, fwd=7.6849 cmp_logits=0.0675, sampling=9.3322, total=17.4978
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9598 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4148, fwd=7.6246 cmp_logits=0.0675, sampling=9.4047, total=17.5126
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9787 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4115, fwd=7.6895 cmp_logits=0.0679, sampling=9.3448, total=17.5145
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9801 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4179, fwd=7.6861 cmp_logits=0.0749, sampling=9.3486, total=17.5281
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0020 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4117, fwd=7.7057 cmp_logits=0.0679, sampling=9.3327, total=17.5188
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9818 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4101, fwd=7.6826 cmp_logits=0.0670, sampling=9.3415, total=17.5021
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9653 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4094, fwd=7.7085 cmp_logits=0.0689, sampling=9.3353, total=17.5228
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9954 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4222, fwd=7.6630 cmp_logits=0.0677, sampling=9.3880, total=17.5419
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0001 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4086, fwd=7.6697 cmp_logits=0.0679, sampling=9.3687, total=17.5157
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9796 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4144, fwd=7.7136 cmp_logits=0.0679, sampling=9.3393, total=17.5359
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9985 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4120, fwd=7.7000 cmp_logits=0.0670, sampling=9.3853, total=17.5653
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0318 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4084, fwd=7.7176 cmp_logits=0.0687, sampling=9.3389, total=17.5343
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9989 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4084, fwd=7.7403 cmp_logits=0.0672, sampling=9.3424, total=17.5591
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0221 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4117, fwd=7.7288 cmp_logits=0.0677, sampling=9.3179, total=17.5269
INFO 10-04 03:18:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9884 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4199, fwd=7.6547 cmp_logits=0.0665, sampling=9.4104, total=17.5524
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0631 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4134, fwd=7.6740 cmp_logits=0.0675, sampling=9.3675, total=17.5233
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9830 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4084, fwd=7.7169 cmp_logits=0.0675, sampling=9.3553, total=17.5490
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0259 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3977, fwd=7.7577 cmp_logits=0.0799, sampling=9.0261, total=17.2625
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7011 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4077, fwd=7.7286 cmp_logits=0.0675, sampling=9.0735, total=17.2784
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7503 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3664, fwd=7.7837 cmp_logits=0.0780, sampling=8.5216, total=16.7506
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1344 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3631, fwd=7.6799 cmp_logits=0.0684, sampling=8.6875, total=16.7997
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1874 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3660, fwd=7.6997 cmp_logits=0.0668, sampling=8.5950, total=16.7284
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 405.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1063 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3667, fwd=7.7143 cmp_logits=0.0668, sampling=8.6625, total=16.8109
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1895 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3648, fwd=7.6776 cmp_logits=0.0668, sampling=8.7247, total=16.8347
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2167 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3653, fwd=7.6263 cmp_logits=0.0663, sampling=8.7881, total=16.8469
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2281 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3669, fwd=7.6876 cmp_logits=0.0684, sampling=8.7028, total=16.8269
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2195 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3655, fwd=7.7326 cmp_logits=0.0789, sampling=8.5154, total=16.6934
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 347.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0457 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3524, fwd=7.6857 cmp_logits=0.0670, sampling=8.5530, total=16.6585
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0074 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3533, fwd=7.6716 cmp_logits=0.0672, sampling=8.5399, total=16.6330
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9799 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3548, fwd=7.7255 cmp_logits=0.0672, sampling=8.4805, total=16.6285
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9826 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3502, fwd=7.6914 cmp_logits=0.0670, sampling=8.5161, total=16.6256
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9754 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3493, fwd=7.6895 cmp_logits=0.0670, sampling=8.4984, total=16.6054
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9568 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3548, fwd=7.6823 cmp_logits=0.0670, sampling=8.5187, total=16.6237
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9728 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3617, fwd=7.6776 cmp_logits=0.0670, sampling=8.5397, total=16.6469
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9923 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3510, fwd=7.7131 cmp_logits=0.0670, sampling=8.4620, total=16.5937
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9439 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3500, fwd=7.6852 cmp_logits=0.0672, sampling=8.4989, total=16.6020
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9468 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3531, fwd=7.6833 cmp_logits=0.0675, sampling=8.5068, total=16.6116
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9573 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3493, fwd=7.7100 cmp_logits=0.0663, sampling=8.4743, total=16.6008
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9871 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3560, fwd=7.7007 cmp_logits=0.0665, sampling=8.5683, total=16.6922
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0605 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3369, fwd=7.7534 cmp_logits=0.0777, sampling=8.0597, total=16.2287
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5501 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3359, fwd=7.7150 cmp_logits=0.0663, sampling=8.1055, total=16.2234
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5436 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3400, fwd=7.6962 cmp_logits=0.0668, sampling=8.1434, total=16.2470
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5701 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3400, fwd=7.7071 cmp_logits=0.0679, sampling=8.1069, total=16.2230
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5482 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3366, fwd=7.7770 cmp_logits=0.0665, sampling=8.0817, total=16.2625
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5839 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3374, fwd=7.7260 cmp_logits=0.0665, sampling=8.1003, total=16.2313
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5520 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3352, fwd=7.6771 cmp_logits=0.0660, sampling=8.1418, total=16.2208
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5396 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3366, fwd=7.6985 cmp_logits=0.0665, sampling=8.1105, total=16.2132
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5346 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3393, fwd=7.6983 cmp_logits=0.0672, sampling=8.1294, total=16.2346
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5529 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3338, fwd=7.6809 cmp_logits=0.0670, sampling=8.1403, total=16.2227
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5391 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3381, fwd=7.7212 cmp_logits=0.0665, sampling=8.1267, total=16.2535
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5768 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3338, fwd=7.7226 cmp_logits=0.0663, sampling=8.0919, total=16.2153
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5346 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3345, fwd=7.6811 cmp_logits=0.0665, sampling=8.1561, total=16.2392
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5546 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3428, fwd=7.6869 cmp_logits=0.0675, sampling=8.1322, total=16.2306
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5479 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3343, fwd=7.6687 cmp_logits=0.0665, sampling=8.1828, total=16.2535
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5763 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3357, fwd=7.7102 cmp_logits=0.0701, sampling=8.0731, total=16.1898
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5105 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3383, fwd=7.6606 cmp_logits=0.0660, sampling=8.1568, total=16.2225
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.2%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5915 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3049, fwd=7.7379 cmp_logits=0.0772, sampling=7.2184, total=15.3389
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5916 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3026, fwd=7.6849 cmp_logits=0.0670, sampling=7.2532, total=15.3086
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5823 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2871, fwd=7.7238 cmp_logits=0.0663, sampling=7.1912, total=15.2688
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4872 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2842, fwd=7.6571 cmp_logits=0.0656, sampling=7.2420, total=15.2495
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4676 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2832, fwd=7.6685 cmp_logits=0.0663, sampling=7.2460, total=15.2652
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5156 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2866, fwd=7.6833 cmp_logits=0.0665, sampling=7.2467, total=15.2838
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5027 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2859, fwd=7.7076 cmp_logits=0.0658, sampling=7.1909, total=15.2509
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4674 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2825, fwd=7.7055 cmp_logits=0.0668, sampling=7.1890, total=15.2445
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4610 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2890, fwd=7.6375 cmp_logits=0.0656, sampling=7.2799, total=15.2729
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4905 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2882, fwd=7.7083 cmp_logits=0.0656, sampling=7.2141, total=15.2769
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4953 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2828, fwd=7.7155 cmp_logits=0.0663, sampling=7.2370, total=15.3024
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5230 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2863, fwd=7.6842 cmp_logits=0.0660, sampling=7.2215, total=15.2588
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5005 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2656, fwd=7.8757 cmp_logits=0.0677, sampling=7.2188, total=15.4285
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6033 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2651, fwd=7.8475 cmp_logits=0.0668, sampling=7.2138, total=15.3942
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5685 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2630, fwd=7.8528 cmp_logits=0.0672, sampling=7.2300, total=15.4140
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5885 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2611, fwd=7.8478 cmp_logits=0.0665, sampling=7.2238, total=15.4002
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5740 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2656, fwd=7.8316 cmp_logits=0.0668, sampling=7.2715, total=15.4362
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6126 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2608, fwd=7.8101 cmp_logits=0.0694, sampling=7.2603, total=15.4016
INFO 10-04 03:18:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:18:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5759 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2623, fwd=7.7851 cmp_logits=0.0668, sampling=7.2980, total=15.4130
INFO 10-04 03:18:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:18:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5871 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2613, fwd=7.7970 cmp_logits=0.0668, sampling=7.2730, total=15.3990
INFO 10-04 03:18:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:18:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5725 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2618, fwd=7.7960 cmp_logits=0.0670, sampling=7.3102, total=15.4359
INFO 10-04 03:18:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:18:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6124 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2639, fwd=7.7913 cmp_logits=0.0668, sampling=7.2649, total=15.3878
INFO 10-04 03:18:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:18:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5613 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2730, fwd=7.8039 cmp_logits=0.0675, sampling=7.2718, total=15.4171
INFO 10-04 03:18:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:18:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5931 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2656, fwd=7.8058 cmp_logits=0.0679, sampling=7.2770, total=15.4171
INFO 10-04 03:18:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:18:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5919 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:18:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:18:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2596, fwd=7.8218 cmp_logits=0.0675, sampling=7.2761, total=15.4257
INFO 10-04 03:18:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:18:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6267 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728011934.867914, last_token_time=1728011936.0461404, first_scheduled_time=1728011934.8895931, first_token_time=1728011935.370175, time_in_queue=0.021679162979125977, finished_time=1728011936.0461, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728011934.869939, last_token_time=1728011936.426416, first_scheduled_time=1728011934.8895931, first_token_time=1728011935.370175, time_in_queue=0.019654035568237305, finished_time=1728011936.4263902, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728011934.8718395, last_token_time=1728011936.2034307, first_scheduled_time=1728011934.8895931, first_token_time=1728011935.370175, time_in_queue=0.01775360107421875, finished_time=1728011936.2034075, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728011934.8731337, last_token_time=1728011936.7105324, first_scheduled_time=1728011934.8895931, first_token_time=1728011935.370175, time_in_queue=0.01645946502685547, finished_time=1728011936.7105107, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728011934.875645, last_token_time=1728011936.7105324, first_scheduled_time=1728011934.8895931, first_token_time=1728011935.370175, time_in_queue=0.013948202133178711, finished_time=1728011936.7105172, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728011934.8781452, last_token_time=1728011936.0819695, first_scheduled_time=1728011934.8895931, first_token_time=1728011935.370175, time_in_queue=0.011447906494140625, finished_time=1728011936.0819504, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728011934.8792021, last_token_time=1728011936.0819695, first_scheduled_time=1728011934.8895931, first_token_time=1728011935.370175, time_in_queue=0.010390996932983398, finished_time=1728011936.0819554, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728011934.880276, last_token_time=1728011936.7420616, first_scheduled_time=1728011934.8895931, first_token_time=1728011935.370175, time_in_queue=0.009317159652709961, finished_time=1728011936.7420497, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728011934.882848, last_token_time=1728011937.1029255, first_scheduled_time=1728011935.3707924, first_token_time=1728011935.5735075, time_in_queue=0.4879443645477295, finished_time=1728011937.102923, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728011934.8863955, last_token_time=1728011936.8983953, first_scheduled_time=1728011935.3707924, first_token_time=1728011935.5735075, time_in_queue=0.48439693450927734, finished_time=1728011936.8983936, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 11197
End-to-End latency: 2.24 seconds
Throughput: 4.47 requests/s, 5009.37 tokens/s
Per_token_time: 0.200 ms

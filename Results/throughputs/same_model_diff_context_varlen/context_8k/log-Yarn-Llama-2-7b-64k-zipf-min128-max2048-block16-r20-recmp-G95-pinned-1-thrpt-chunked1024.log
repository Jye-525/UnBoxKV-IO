Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=128, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=8192, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=1024, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Selected required requests 20
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Chunked prefill is enabled. max_num_batched_tokens=1024, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'finetuned', 'type', 'original_max_position_embeddings'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 03:51:18 config.py:654] [SchedulerConfig] max_num_batched_tokens: 1024 chunked_prefill_enabled: True
INFO 10-04 03:51:18 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 03:51:19 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 03:51:23 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 03:51:27 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 03:51:27 model_runner.py:183] Loaded model: 
INFO 10-04 03:51:27 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 03:51:27 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 03:51:27 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:51:27 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 03:51:27 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 03:51:27 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 03:51:27 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:51:27 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:51:27 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 03:51:27 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 03:51:27 model_runner.py:183]         )
INFO 10-04 03:51:27 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 03:51:27 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:51:27 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:51:27 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 03:51:27 model_runner.py:183]         )
INFO 10-04 03:51:27 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:51:27 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:51:27 model_runner.py:183]       )
INFO 10-04 03:51:27 model_runner.py:183]     )
INFO 10-04 03:51:27 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:51:27 model_runner.py:183]   )
INFO 10-04 03:51:27 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:51:27 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 03:51:27 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 03:51:27 model_runner.py:183] )
INFO 10-04 03:51:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:51:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 3.5851, fwd=347.2512 cmp_logits=0.2365, sampling=105.4337, total=456.5084
INFO 10-04 03:51:27 worker.py:164] Peak: 13.307 GB, Initial: 38.980 GB, Free: 25.673 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.118 GB
INFO 10-04 03:51:27 gpu_executor.py:117] # GPU blocks: 3087, # CPU blocks: 8192
INFO 10-04 03:51:59 worker.py:189] _init_cache_engine took 24.1250 GB
INFO 10-04 03:51:59 scheduler.py:307] Scheduler initialized with prompt limit: 8192
INFO 10-04 03:51:59 Start warmup...
INFO 10-04 03:51:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:51:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:52:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8781, fwd=1942.6818 cmp_logits=71.3351, sampling=0.9394, total=2015.8365
INFO 10-04 03:52:01 metrics.py:335] Avg prompt throughput: 506.3 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 2022.7
INFO 10-04 03:52:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 2016.3572 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:52:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:52:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2971, fwd=43.8025 cmp_logits=0.1147, sampling=6.5632, total=50.7796
INFO 10-04 03:52:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 51.6
INFO 10-04 03:52:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 51.2278 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:52:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:52:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3197, fwd=8.1387 cmp_logits=0.0768, sampling=7.0086, total=15.5454
INFO 10-04 03:52:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 61.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 03:52:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8629 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:52:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:52:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2863, fwd=8.0459 cmp_logits=0.0734, sampling=7.0560, total=15.4631
INFO 10-04 03:52:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:52:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7266 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:52:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:52:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2747, fwd=8.0817 cmp_logits=0.0713, sampling=7.1170, total=15.5456
INFO 10-04 03:52:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:52:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7828 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:52:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:52:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2735, fwd=8.0118 cmp_logits=0.0696, sampling=7.0815, total=15.4376
INFO 10-04 03:52:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:52:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6705 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:52:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:52:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2756, fwd=8.0264 cmp_logits=0.0749, sampling=6.9544, total=15.3325
INFO 10-04 03:52:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:52:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5573 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:52:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:52:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2708, fwd=7.9753 cmp_logits=0.0696, sampling=5.9187, total=14.2350
INFO 10-04 03:52:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 03:52:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4804 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:52:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:52:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2635, fwd=7.9870 cmp_logits=0.0682, sampling=5.9352, total=14.2546
INFO 10-04 03:52:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 03:52:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4536 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:52:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:52:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2606, fwd=8.0218 cmp_logits=0.0677, sampling=5.9285, total=14.2794
INFO 10-04 03:52:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 03:52:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4954 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:52:06 Start benchmarking...
INFO 10-04 03:52:06 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:06 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:52:08 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8698, fwd=1594.8098 cmp_logits=0.1481, sampling=71.1350, total=1666.9636
INFO 10-04 03:52:08 metrics.py:335] Avg prompt throughput: 599.7 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 17 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%, Interval(ms): 1707.4
INFO 10-04 03:52:08 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1667.5308 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:52:08 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:08 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:52:09 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8268, fwd=1582.1471 cmp_logits=0.1380, sampling=69.4418, total=1652.5548
INFO 10-04 03:52:09 metrics.py:335] Avg prompt throughput: 618.2 tokens/s, Avg generation throughput: 1.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%, Interval(ms): 1653.2
INFO 10-04 03:52:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1653.0275 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:52:09 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:09 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:52:09 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8316, fwd=14.5380 cmp_logits=0.0942, sampling=70.5514, total=86.0169
INFO 10-04 03:52:09 metrics.py:335] Avg prompt throughput: 11780.6 tokens/s, Avg generation throughput: 46.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 86.7
INFO 10-04 03:52:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.4449 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:52:09 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:09 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:52:09 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8690, fwd=14.5626 cmp_logits=0.0892, sampling=60.5507, total=76.0727
INFO 10-04 03:52:09 metrics.py:335] Avg prompt throughput: 13285.1 tokens/s, Avg generation throughput: 91.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 10.7%, CPU KV cache usage: 0.0%, Interval(ms): 76.8
INFO 10-04 03:52:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.6048 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:52:09 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:09 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8264, fwd=14.5245 cmp_logits=0.0741, sampling=59.0482, total=74.4741
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 13535.7 tokens/s, Avg generation throughput: 93.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 10.7%, CPU KV cache usage: 0.0%, Interval(ms): 75.1
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 74.9545 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8566, fwd=14.5645 cmp_logits=0.0880, sampling=72.2139, total=87.7237
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 11504.0 tokens/s, Avg generation throughput: 90.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 14.3%, CPU KV cache usage: 0.0%, Interval(ms): 88.4
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 88.2311 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8762, fwd=14.5907 cmp_logits=0.0870, sampling=71.7456, total=87.3008
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 11543.0 tokens/s, Avg generation throughput: 102.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 88.0
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 87.8382 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8798, fwd=14.5774 cmp_logits=0.0725, sampling=61.9662, total=77.4970
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 12983.4 tokens/s, Avg generation throughput: 115.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 78.2
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.9972 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9298, fwd=14.5702 cmp_logits=0.0865, sampling=66.9875, total=82.5751
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 12174.3 tokens/s, Avg generation throughput: 131.9 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 18.5%, CPU KV cache usage: 0.0%, Interval(ms): 83.4
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 83.1878 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9410, fwd=14.7004 cmp_logits=0.0861, sampling=63.5967, total=79.3250
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 12635.8 tokens/s, Avg generation throughput: 149.5 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 80.2
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 79.9828 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9460, fwd=14.4908 cmp_logits=0.0851, sampling=60.6587, total=76.1819
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 13140.8 tokens/s, Avg generation throughput: 168.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 22.9%, CPU KV cache usage: 0.0%, Interval(ms): 77.0
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.8223 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9749, fwd=14.5025 cmp_logits=0.0885, sampling=60.4706, total=76.0374
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 13140.1 tokens/s, Avg generation throughput: 195.0 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 25.1%, CPU KV cache usage: 0.0%, Interval(ms): 76.9
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.7462 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.0147, fwd=14.4925 cmp_logits=0.0880, sampling=62.0530, total=77.6494
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 12830.5 tokens/s, Avg generation throughput: 216.2 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 27.7%, CPU KV cache usage: 0.0%, Interval(ms): 78.6
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 78.4378 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.0266, fwd=14.6093 cmp_logits=0.0851, sampling=65.1410, total=80.8630
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 12296.0 tokens/s, Avg generation throughput: 219.8 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.8%, CPU KV cache usage: 0.0%, Interval(ms): 81.9
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 81.6851 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([898]), positions.shape=torch.Size([898]) hidden_states.shape=torch.Size([898, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9515, fwd=14.6253 cmp_logits=0.0732, sampling=68.0971, total=83.7481
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 10401.6 tokens/s, Avg generation throughput: 200.5 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.9%, CPU KV cache usage: 0.0%, Interval(ms): 84.8
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 84.4858 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5198, fwd=7.9501 cmp_logits=0.0699, sampling=11.5001, total=20.0408
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 810.9 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7553 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5167, fwd=7.8876 cmp_logits=0.0694, sampling=11.5323, total=20.0064
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 812.5 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7198 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5155, fwd=7.8766 cmp_logits=0.0691, sampling=11.5936, total=20.0555
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 810.8 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7675 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5174, fwd=7.8697 cmp_logits=0.0701, sampling=11.5621, total=20.0202
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 810.9 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7646 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5126, fwd=7.9012 cmp_logits=0.0813, sampling=10.8142, total=19.3102
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 789.2 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.3
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0045 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:52:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5119, fwd=7.8905 cmp_logits=0.0701, sampling=10.8628, total=19.3362
INFO 10-04 03:52:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 790.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 03:52:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0326 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:52:10 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5016, fwd=7.9331 cmp_logits=0.0784, sampling=10.6895, total=19.2037
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 746.6 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8650 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4983, fwd=7.8738 cmp_logits=0.0792, sampling=10.7388, total=19.1910
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 747.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8677 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4845, fwd=7.9141 cmp_logits=0.0825, sampling=10.7062, total=19.1884
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 698.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.0
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8252 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4876, fwd=7.8547 cmp_logits=0.0687, sampling=10.7558, total=19.1679
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 698.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.0
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8143 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4833, fwd=7.8912 cmp_logits=0.0701, sampling=10.7219, total=19.1674
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 700.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.0
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8078 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4816, fwd=7.8733 cmp_logits=0.0706, sampling=10.7722, total=19.1989
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 698.6 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.0
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8503 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4752, fwd=7.9603 cmp_logits=0.0703, sampling=10.2079, total=18.7147
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 664.8 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3343 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4611, fwd=7.8766 cmp_logits=0.0687, sampling=10.3126, total=18.7201
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 665.8 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3372 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4685, fwd=7.8707 cmp_logits=0.0691, sampling=10.3149, total=18.7244
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 664.1 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3453 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4623, fwd=7.8602 cmp_logits=0.0699, sampling=10.3230, total=18.7161
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 666.4 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3193 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4625, fwd=7.8614 cmp_logits=0.0701, sampling=10.3121, total=18.7070
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 666.7 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3176 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4616, fwd=7.9060 cmp_logits=0.0699, sampling=10.2961, total=18.7342
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 665.4 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3505 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4609, fwd=7.8557 cmp_logits=0.0687, sampling=10.3865, total=18.7726
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 663.4 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.4092 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4528, fwd=7.8876 cmp_logits=0.0689, sampling=9.9280, total=18.3382
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 625.6 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9257 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4635, fwd=7.7927 cmp_logits=0.0772, sampling=10.0021, total=18.3363
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 627.8 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.1
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9314 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4413, fwd=7.9436 cmp_logits=0.0691, sampling=9.5716, total=18.0266
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 585.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.8
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5883 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4206, fwd=7.9143 cmp_logits=0.0844, sampling=9.4318, total=17.8521
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 538.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3725 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4182, fwd=7.8573 cmp_logits=0.0687, sampling=9.5029, total=17.8483
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 537.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4095 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4275, fwd=7.8287 cmp_logits=0.0684, sampling=9.5491, total=17.8747
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 538.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3918 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4201, fwd=7.8630 cmp_logits=0.0684, sampling=9.5029, total=17.8554
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 538.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3876 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4108, fwd=7.9012 cmp_logits=0.0682, sampling=9.2204, total=17.6013
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 492.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0898 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4072, fwd=7.8509 cmp_logits=0.0679, sampling=9.2089, total=17.5359
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0199 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4003, fwd=7.8609 cmp_logits=0.0687, sampling=9.1839, total=17.5149
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0271 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4034, fwd=7.8392 cmp_logits=0.0694, sampling=9.2287, total=17.5412
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0159 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4025, fwd=7.8304 cmp_logits=0.0679, sampling=9.2399, total=17.5414
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0247 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4027, fwd=7.8528 cmp_logits=0.0684, sampling=9.2237, total=17.5486
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0380 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3996, fwd=7.8330 cmp_logits=0.0679, sampling=9.2375, total=17.5390
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0230 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4013, fwd=7.8332 cmp_logits=0.0691, sampling=9.2359, total=17.5400
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0550 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4017, fwd=7.8406 cmp_logits=0.0675, sampling=9.2459, total=17.5564
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0762 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4203, fwd=7.9808 cmp_logits=0.0691, sampling=9.0990, total=17.5703
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0519 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4029, fwd=7.8721 cmp_logits=0.0682, sampling=9.2175, total=17.5619
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0457 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4036, fwd=7.8788 cmp_logits=0.0682, sampling=9.2075, total=17.5588
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0583 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3927, fwd=7.8800 cmp_logits=0.0691, sampling=8.8634, total=17.2060
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 446.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.6%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7066 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3884, fwd=7.8144 cmp_logits=0.0675, sampling=8.9047, total=17.1759
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 449.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.6%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6342 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3874, fwd=7.8495 cmp_logits=0.0684, sampling=8.8916, total=17.1976
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 449.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6477 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3963, fwd=7.8478 cmp_logits=0.0689, sampling=8.8825, total=17.1964
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 449.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6532 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3855, fwd=7.8351 cmp_logits=0.0679, sampling=8.8997, total=17.1895
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 448.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6697 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3860, fwd=7.8673 cmp_logits=0.0744, sampling=8.5633, total=16.8920
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 398.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.6
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3540 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3757, fwd=7.8747 cmp_logits=0.0682, sampling=8.5347, total=16.8543
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3128 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3431, fwd=7.8907 cmp_logits=0.0823, sampling=7.8094, total=16.1264
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.4809 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3362, fwd=7.8404 cmp_logits=0.0675, sampling=7.8912, total=16.1362
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.6
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.4869 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3393, fwd=7.8347 cmp_logits=0.0675, sampling=7.8845, total=16.1266
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.6
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.4850 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3374, fwd=7.8571 cmp_logits=0.0660, sampling=7.8311, total=16.0923
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.6
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.4740 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3397, fwd=7.8335 cmp_logits=0.0677, sampling=7.9091, total=16.1507
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.4983 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3402, fwd=7.8549 cmp_logits=0.0672, sampling=7.8714, total=16.1343
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.1 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5064 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3207, fwd=7.9064 cmp_logits=0.0675, sampling=7.5064, total=15.8017
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.2 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1140 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3216, fwd=7.8428 cmp_logits=0.0684, sampling=7.5598, total=15.7938
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.9 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1166 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3247, fwd=7.8232 cmp_logits=0.0670, sampling=7.5891, total=15.8057
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 244.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1800 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3073, fwd=7.8857 cmp_logits=0.0675, sampling=7.1046, total=15.3656
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6498 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3047, fwd=7.8640 cmp_logits=0.0675, sampling=7.1144, total=15.3515
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6422 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3033, fwd=7.8375 cmp_logits=0.0665, sampling=7.1306, total=15.3387
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6286 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3045, fwd=7.8926 cmp_logits=0.0679, sampling=7.0808, total=15.3465
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6362 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3045, fwd=7.8292 cmp_logits=0.0665, sampling=7.1387, total=15.3399
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6627 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3128, fwd=7.8127 cmp_logits=0.0665, sampling=7.1750, total=15.3673
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6538 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3047, fwd=7.8380 cmp_logits=0.0675, sampling=7.1418, total=15.3532
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6384 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:52:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3021, fwd=7.8149 cmp_logits=0.0668, sampling=7.1592, total=15.3444
INFO 10-04 03:52:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:52:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6295 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:52:11 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3009, fwd=7.8330 cmp_logits=0.0670, sampling=7.1559, total=15.3580
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6429 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3035, fwd=7.8208 cmp_logits=0.0668, sampling=7.1588, total=15.3506
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6715 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3042, fwd=7.8416 cmp_logits=0.0668, sampling=7.1454, total=15.3594
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6453 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3033, fwd=7.8430 cmp_logits=0.0675, sampling=7.1554, total=15.3699
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6791 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2861, fwd=7.8735 cmp_logits=0.0668, sampling=6.9704, total=15.1978
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4426 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2849, fwd=7.8499 cmp_logits=0.0663, sampling=6.9926, total=15.1947
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4467 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2880, fwd=7.8483 cmp_logits=0.0668, sampling=6.9983, total=15.2020
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4479 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2928, fwd=7.7989 cmp_logits=0.0737, sampling=7.0188, total=15.1851
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4300 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2861, fwd=7.8237 cmp_logits=0.0665, sampling=7.0107, total=15.1880
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4333 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2854, fwd=7.8206 cmp_logits=0.0675, sampling=7.0052, total=15.1794
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4283 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2866, fwd=7.7953 cmp_logits=0.0675, sampling=7.0262, total=15.1768
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4207 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2866, fwd=7.8232 cmp_logits=0.0670, sampling=7.0558, total=15.2330
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5020 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2606, fwd=7.9968 cmp_logits=0.0677, sampling=7.0112, total=15.3375
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5401 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2606, fwd=8.0178 cmp_logits=0.0675, sampling=7.0624, total=15.4092
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6081 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2618, fwd=7.9727 cmp_logits=0.0672, sampling=7.0844, total=15.3868
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5878 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2673, fwd=7.9632 cmp_logits=0.0677, sampling=7.1037, total=15.4028
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6033 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2625, fwd=8.0462 cmp_logits=0.0668, sampling=7.0415, total=15.4176
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6174 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:52:12 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:52:12 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:52:12 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2608, fwd=8.0028 cmp_logits=0.0670, sampling=7.0779, total=15.4095
INFO 10-04 03:52:12 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:52:12 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6415 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.4578161, last_token_time=1728013930.3849175, first_scheduled_time=1728013926.4878278, first_token_time=1728013928.1549609, time_in_queue=0.030011653900146484, finished_time=1728013930.3848822, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.458782, last_token_time=1728013931.2841396, first_scheduled_time=1728013926.4878278, first_token_time=1728013928.1549609, time_in_queue=0.029045820236206055, finished_time=1728013931.284089, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.4605308, last_token_time=1728013930.9887736, first_scheduled_time=1728013926.4878278, first_token_time=1728013929.808193, time_in_queue=0.027297019958496094, finished_time=1728013930.9887145, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.4612985, last_token_time=1728013931.720072, first_scheduled_time=1728013928.1554756, first_token_time=1728013929.8948762, time_in_queue=1.6941771507263184, finished_time=1728013931.720041, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.463469, last_token_time=1728013931.720072, first_scheduled_time=1728013929.808733, first_token_time=1728013929.971567, time_in_queue=3.345263957977295, finished_time=1728013931.720047, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.465699, last_token_time=1728013930.7796547, first_scheduled_time=1728013929.8953605, first_token_time=1728013929.971567, time_in_queue=3.429661512374878, finished_time=1728013930.7796042, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.4661536, last_token_time=1728013930.7796547, first_scheduled_time=1728013929.8953605, first_token_time=1728013929.971567, time_in_queue=3.4292068481445312, finished_time=1728013930.7796087, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.4666042, last_token_time=1728013931.8200088, first_scheduled_time=1728013929.8953605, first_token_time=1728013930.1351168, time_in_queue=3.4287562370300293, finished_time=1728013931.8199875, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.4688241, last_token_time=1728013932.2777073, first_scheduled_time=1728013930.04726, first_token_time=1728013930.223115, time_in_queue=3.5784358978271484, finished_time=1728013932.2777047, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.4722831, last_token_time=1728013932.0585158, first_scheduled_time=1728013930.1356752, first_token_time=1728013930.3846018, time_in_queue=3.6633920669555664, finished_time=1728013932.058509, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.475045, last_token_time=1728013931.0289273, first_scheduled_time=1728013930.3018785, first_token_time=1728013930.3846018, time_in_queue=3.826833486557007, finished_time=1728013931.0288928, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.4756021, last_token_time=1728013931.2458453, first_scheduled_time=1728013930.3018785, first_token_time=1728013930.4648376, time_in_queue=3.8262763023376465, finished_time=1728013931.2458196, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.4764762, last_token_time=1728013931.595853, first_scheduled_time=1728013930.3853598, first_token_time=1728013930.4648376, time_in_queue=3.908883571624756, finished_time=1728013931.5958383, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.4780438, last_token_time=1728013931.6850352, first_scheduled_time=1728013930.3853598, first_token_time=1728013930.5418448, time_in_queue=3.907315969467163, finished_time=1728013931.6850247, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.4797373, last_token_time=1728013931.1090515, first_scheduled_time=1728013930.4655063, first_token_time=1728013930.6187415, time_in_queue=3.985769033432007, finished_time=1728013931.1090336, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.4803057, last_token_time=1728013931.3772418, first_scheduled_time=1728013930.5425406, first_token_time=1728013930.6187415, time_in_queue=4.062234878540039, finished_time=1728013931.3772311, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.481348, last_token_time=1728013931.3029442, first_scheduled_time=1728013930.5425406, first_token_time=1728013930.6973274, time_in_queue=4.061192512512207, finished_time=1728013931.3029332, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.4822483, last_token_time=1728013930.9482675, first_scheduled_time=1728013930.619501, first_token_time=1728013930.6973274, time_in_queue=4.1372528076171875, finished_time=1728013930.948256, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.4825726, last_token_time=1728013931.868921, first_scheduled_time=1728013930.619501, first_token_time=1728013930.7791922, time_in_queue=4.136928558349609, finished_time=1728013931.8689144, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013926.4846683, last_token_time=1728013932.183253, first_scheduled_time=1728013930.69815, first_token_time=1728013930.8640327, time_in_queue=4.213481664657593, finished_time=1728013932.1832511, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 15868
End-to-End latency: 5.82 seconds
Throughput: 3.44 requests/s, 2726.41 tokens/s
Per_token_time: 0.367 ms

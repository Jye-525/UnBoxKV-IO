Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=8192, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=1024, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Selected required requests 10
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Chunked prefill is enabled. max_num_batched_tokens=1024, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'finetuned', 'type', 'original_max_position_embeddings'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 03:23:13 config.py:654] [SchedulerConfig] max_num_batched_tokens: 1024 chunked_prefill_enabled: True
INFO 10-04 03:23:13 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 03:23:14 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 03:23:18 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 03:23:22 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 03:23:22 model_runner.py:183] Loaded model: 
INFO 10-04 03:23:22 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 03:23:22 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 03:23:22 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:23:22 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 03:23:22 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 03:23:22 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 03:23:22 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:23:22 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:23:22 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 03:23:22 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 03:23:22 model_runner.py:183]         )
INFO 10-04 03:23:22 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 03:23:22 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:23:22 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:23:22 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 03:23:22 model_runner.py:183]         )
INFO 10-04 03:23:22 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:23:22 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:23:22 model_runner.py:183]       )
INFO 10-04 03:23:22 model_runner.py:183]     )
INFO 10-04 03:23:22 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:23:22 model_runner.py:183]   )
INFO 10-04 03:23:22 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:23:22 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 03:23:22 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 03:23:22 model_runner.py:183] )
INFO 10-04 03:23:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:23:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 3.4673, fwd=400.3820 cmp_logits=0.2444, sampling=108.0954, total=512.1908
INFO 10-04 03:23:23 worker.py:164] Peak: 13.307 GB, Initial: 38.980 GB, Free: 25.673 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.118 GB
INFO 10-04 03:23:23 gpu_executor.py:117] # GPU blocks: 3087, # CPU blocks: 8192
INFO 10-04 03:23:54 worker.py:189] _init_cache_engine took 24.1250 GB
INFO 10-04 03:23:54 scheduler.py:307] Scheduler initialized with prompt limit: 8192
INFO 10-04 03:23:54 Start warmup...
INFO 10-04 03:23:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:23:54 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:23:54 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9134, fwd=317.4813 cmp_logits=71.1942, sampling=0.9449, total=390.5363
INFO 10-04 03:23:54 metrics.py:335] Avg prompt throughput: 2576.9 tokens/s, Avg generation throughput: 2.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 397.4
INFO 10-04 03:23:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 391.0651 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:23:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:23:54 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:23:54 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2995, fwd=40.3326 cmp_logits=0.1190, sampling=6.7122, total=47.4651
INFO 10-04 03:23:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 48.4
INFO 10-04 03:23:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.9140 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:23:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:23:54 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:23:54 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3104, fwd=7.9193 cmp_logits=0.0715, sampling=6.2070, total=14.5102
INFO 10-04 03:23:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.1
INFO 10-04 03:23:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.8056 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:23:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:23:54 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:23:54 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2792, fwd=7.8785 cmp_logits=0.0703, sampling=6.1238, total=14.3528
INFO 10-04 03:23:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:23:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5884 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:23:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:23:54 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:23:54 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2637, fwd=7.9479 cmp_logits=0.0696, sampling=6.0647, total=14.3471
INFO 10-04 03:23:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:23:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5724 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:23:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:23:54 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:23:54 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2682, fwd=7.8983 cmp_logits=0.0713, sampling=6.1059, total=14.3454
INFO 10-04 03:23:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:23:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5767 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:23:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:23:54 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:23:54 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2692, fwd=7.9355 cmp_logits=0.0737, sampling=6.0554, total=14.3347
INFO 10-04 03:23:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.8
INFO 10-04 03:23:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5683 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:23:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:23:54 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:23:54 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2601, fwd=7.9379 cmp_logits=0.0684, sampling=6.0637, total=14.3313
INFO 10-04 03:23:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:23:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5781 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:23:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:23:54 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:23:54 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2575, fwd=7.9358 cmp_logits=0.0677, sampling=6.0730, total=14.3349
INFO 10-04 03:23:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:23:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5357 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:23:54 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:23:54 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:23:54 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2570, fwd=7.9296 cmp_logits=0.0677, sampling=6.0589, total=14.3142
INFO 10-04 03:23:54 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:23:54 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5354 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:23:59 Start benchmarking...
INFO 10-04 03:23:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:23:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:23:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8361, fwd=30.5684 cmp_logits=0.0904, sampling=70.4956, total=101.9919
INFO 10-04 03:23:59 metrics.py:335] Avg prompt throughput: 7662.8 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%, Interval(ms): 133.6
INFO 10-04 03:23:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 102.4666 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:23:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:23:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:23:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7441, fwd=14.4613 cmp_logits=0.1030, sampling=77.1821, total=92.4911
INFO 10-04 03:23:59 metrics.py:335] Avg prompt throughput: 10996.7 tokens/s, Avg generation throughput: 21.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%, Interval(ms): 93.0
INFO 10-04 03:23:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 92.7997 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:23:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:23:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7772, fwd=14.4954 cmp_logits=0.0825, sampling=82.2914, total=97.6472
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 10412.8 tokens/s, Avg generation throughput: 30.6 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 7.2%, CPU KV cache usage: 0.0%, Interval(ms): 98.1
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 97.9970 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7751, fwd=14.5581 cmp_logits=0.0834, sampling=62.1841, total=77.6014
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 13067.4 tokens/s, Avg generation throughput: 51.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 9.8%, CPU KV cache usage: 0.0%, Interval(ms): 78.1
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.9779 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8016, fwd=14.7116 cmp_logits=0.0837, sampling=61.8050, total=77.4028
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 13081.5 tokens/s, Avg generation throughput: 64.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 11.0%, CPU KV cache usage: 0.0%, Interval(ms): 78.0
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.8160 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8297, fwd=14.5710 cmp_logits=0.0832, sampling=58.2983, total=73.7829
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 13686.2 tokens/s, Avg generation throughput: 94.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 15.0%, CPU KV cache usage: 0.0%, Interval(ms): 74.5
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 74.2910 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8268, fwd=14.4465 cmp_logits=0.0701, sampling=60.9288, total=76.2734
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 13215.4 tokens/s, Avg generation throughput: 91.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 15.0%, CPU KV cache usage: 0.0%, Interval(ms): 77.0
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.7369 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8316, fwd=14.3311 cmp_logits=0.0865, sampling=72.1273, total=87.3773
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 11547.3 tokens/s, Avg generation throughput: 90.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 88.1
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 87.9014 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8581, fwd=14.9553 cmp_logits=0.0846, sampling=71.0635, total=86.9622
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 11585.5 tokens/s, Avg generation throughput: 102.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.8%, CPU KV cache usage: 0.0%, Interval(ms): 87.7
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 87.5180 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8521, fwd=14.4448 cmp_logits=0.0706, sampling=60.7762, total=76.1447
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 13208.7 tokens/s, Avg generation throughput: 117.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.8%, CPU KV cache usage: 0.0%, Interval(ms): 76.8
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.6623 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([473]), positions.shape=torch.Size([473]) hidden_states.shape=torch.Size([473, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7310, fwd=14.5693 cmp_logits=0.0904, sampling=37.5767, total=52.9683
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 8642.1 tokens/s, Avg generation throughput: 186.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.8%, CPU KV cache usage: 0.0%, Interval(ms): 53.7
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.5116 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4079, fwd=7.7934 cmp_logits=0.0684, sampling=9.2263, total=17.4968
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0197 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4113, fwd=7.7147 cmp_logits=0.0682, sampling=9.3045, total=17.4994
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0187 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4063, fwd=7.7064 cmp_logits=0.0682, sampling=9.3105, total=17.4921
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0075 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4098, fwd=7.7484 cmp_logits=0.0677, sampling=9.2478, total=17.4747
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0056 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4063, fwd=7.7114 cmp_logits=0.0677, sampling=9.2809, total=17.4673
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0290 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4063, fwd=7.7064 cmp_logits=0.0679, sampling=9.2907, total=17.4720
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9977 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4056, fwd=7.7443 cmp_logits=0.0677, sampling=9.2781, total=17.4963
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0235 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4041, fwd=7.7243 cmp_logits=0.0684, sampling=9.3050, total=17.5030
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0254 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4077, fwd=7.8530 cmp_logits=0.0682, sampling=9.1486, total=17.4780
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9906 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4058, fwd=7.7267 cmp_logits=0.0684, sampling=9.2950, total=17.4968
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0516 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4065, fwd=7.7879 cmp_logits=0.0689, sampling=9.2409, total=17.5052
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0202 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4017, fwd=7.7920 cmp_logits=0.0687, sampling=9.2766, total=17.5397
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0578 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4096, fwd=7.6747 cmp_logits=0.0772, sampling=9.3360, total=17.4985
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0240 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4125, fwd=7.6907 cmp_logits=0.0670, sampling=9.3384, total=17.5097
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0318 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4051, fwd=7.7174 cmp_logits=0.0665, sampling=9.3250, total=17.5145
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0655 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4044, fwd=7.7274 cmp_logits=0.0682, sampling=9.3236, total=17.5250
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0588 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:24:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3917, fwd=7.8201 cmp_logits=0.0679, sampling=8.9531, total=17.2338
INFO 10-04 03:24:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:24:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7276 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:24:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3905, fwd=7.6902 cmp_logits=0.0679, sampling=9.0897, total=17.2393
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7360 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3970, fwd=7.6566 cmp_logits=0.0677, sampling=9.1026, total=17.2248
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7205 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3917, fwd=7.7684 cmp_logits=0.0670, sampling=9.0041, total=17.2322
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7591 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3965, fwd=7.7400 cmp_logits=0.0689, sampling=9.0106, total=17.2167
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 503.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7057 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3922, fwd=7.7853 cmp_logits=0.0684, sampling=8.9867, total=17.2336
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7209 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3924, fwd=7.8115 cmp_logits=0.0682, sampling=8.9655, total=17.2386
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7512 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3569, fwd=7.7455 cmp_logits=0.0684, sampling=8.5480, total=16.7193
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1432 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3586, fwd=7.7693 cmp_logits=0.0682, sampling=8.5614, total=16.7584
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1835 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3583, fwd=7.7808 cmp_logits=0.0677, sampling=8.4989, total=16.7065
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 404.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1368 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3576, fwd=7.7066 cmp_logits=0.0670, sampling=8.5907, total=16.7227
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1692 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3479, fwd=7.8001 cmp_logits=0.0808, sampling=8.3232, total=16.5527
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9432 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3467, fwd=7.7312 cmp_logits=0.0668, sampling=8.3990, total=16.5451
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9356 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3452, fwd=7.8201 cmp_logits=0.0727, sampling=8.3599, total=16.5987
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9883 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3507, fwd=7.7517 cmp_logits=0.0794, sampling=8.3778, total=16.5606
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9487 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3459, fwd=7.7806 cmp_logits=0.0668, sampling=8.3373, total=16.5319
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 351.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9251 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3412, fwd=7.7074 cmp_logits=0.0675, sampling=8.4467, total=16.5637
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9578 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3433, fwd=7.7457 cmp_logits=0.0677, sampling=8.4243, total=16.5818
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9692 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3440, fwd=7.7868 cmp_logits=0.0665, sampling=8.4186, total=16.6166
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0057 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3445, fwd=7.8087 cmp_logits=0.0668, sampling=8.3561, total=16.5772
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9661 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3424, fwd=7.7233 cmp_logits=0.0668, sampling=8.4291, total=16.5625
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9487 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3455, fwd=7.7674 cmp_logits=0.0677, sampling=8.3876, total=16.5689
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9575 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3538, fwd=7.7286 cmp_logits=0.0792, sampling=8.4004, total=16.5627
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9778 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3331, fwd=7.8034 cmp_logits=0.0670, sampling=8.1296, total=16.3338
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6914 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3288, fwd=7.8101 cmp_logits=0.0672, sampling=8.0695, total=16.2764
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6502 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3304, fwd=7.7236 cmp_logits=0.0670, sampling=8.1482, total=16.2702
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6237 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3293, fwd=7.8111 cmp_logits=0.0689, sampling=8.1151, total=16.3260
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6843 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3304, fwd=7.7252 cmp_logits=0.0672, sampling=8.1592, total=16.2830
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6416 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3288, fwd=7.8824 cmp_logits=0.0679, sampling=8.0395, total=16.3195
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6807 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3293, fwd=7.6885 cmp_logits=0.0763, sampling=8.1043, total=16.1998
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5586 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3312, fwd=7.7450 cmp_logits=0.0668, sampling=8.0614, total=16.2053
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5558 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3297, fwd=7.8321 cmp_logits=0.0668, sampling=7.9880, total=16.2172
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5734 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3314, fwd=7.7932 cmp_logits=0.0668, sampling=8.0302, total=16.2222
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5789 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3285, fwd=7.8208 cmp_logits=0.0665, sampling=8.0526, total=16.2690
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6187 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3438, fwd=7.8266 cmp_logits=0.0668, sampling=7.9908, total=16.2287
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5963 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3295, fwd=7.8654 cmp_logits=0.0670, sampling=7.9238, total=16.1867
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5439 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3276, fwd=7.8037 cmp_logits=0.0668, sampling=7.9975, total=16.1965
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5510 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3357, fwd=7.8440 cmp_logits=0.0699, sampling=7.9927, total=16.2435
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6028 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3283, fwd=7.8018 cmp_logits=0.0670, sampling=8.0726, total=16.2711
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6285 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3347, fwd=7.8058 cmp_logits=0.0670, sampling=8.0235, total=16.2320
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5851 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3314, fwd=7.8402 cmp_logits=0.0663, sampling=7.9911, total=16.2299
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5830 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3262, fwd=7.6833 cmp_logits=0.0668, sampling=8.1365, total=16.2137
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5994 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3135, fwd=7.7617 cmp_logits=0.0670, sampling=7.4868, total=15.6298
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9740 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3059, fwd=7.7844 cmp_logits=0.0675, sampling=7.1752, total=15.3341
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6279 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3068, fwd=7.7844 cmp_logits=0.0677, sampling=7.1552, total=15.3148
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6002 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2966, fwd=7.7634 cmp_logits=0.0670, sampling=7.1840, total=15.3122
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5969 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2956, fwd=7.7355 cmp_logits=0.0665, sampling=7.2091, total=15.3077
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5926 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2966, fwd=7.7796 cmp_logits=0.0665, sampling=7.1828, total=15.3267
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6374 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2789, fwd=7.8442 cmp_logits=0.0658, sampling=7.0851, total=15.2748
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5189 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2804, fwd=7.6957 cmp_logits=0.0658, sampling=7.1831, total=15.2259
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4676 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2778, fwd=7.7038 cmp_logits=0.0663, sampling=7.1955, total=15.2442
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4898 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2775, fwd=7.7064 cmp_logits=0.0668, sampling=7.1759, total=15.2273
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4703 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2789, fwd=7.6833 cmp_logits=0.0660, sampling=7.1936, total=15.2228
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4674 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2837, fwd=7.7960 cmp_logits=0.0670, sampling=7.1537, total=15.3010
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5480 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2792, fwd=7.7124 cmp_logits=0.0658, sampling=7.1447, total=15.2030
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4452 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2792, fwd=7.7562 cmp_logits=0.0663, sampling=7.1285, total=15.2309
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4819 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2809, fwd=7.6599 cmp_logits=0.0656, sampling=7.2422, total=15.2495
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4912 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2792, fwd=7.8123 cmp_logits=0.0668, sampling=7.0720, total=15.2311
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4746 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2818, fwd=7.7119 cmp_logits=0.0660, sampling=7.2169, total=15.2774
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5210 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2768, fwd=7.6859 cmp_logits=0.0656, sampling=7.2062, total=15.2352
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4793 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:24:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2930, fwd=7.6935 cmp_logits=0.0665, sampling=7.1969, total=15.2509
INFO 10-04 03:24:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:24:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5241 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:24:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:24:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2587, fwd=7.9298 cmp_logits=0.0670, sampling=7.1306, total=15.3873
INFO 10-04 03:24:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:24:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5864 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:24:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:24:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2635, fwd=7.9055 cmp_logits=0.0663, sampling=7.1580, total=15.3942
INFO 10-04 03:24:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:24:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5919 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:24:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:24:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2558, fwd=8.0321 cmp_logits=0.0718, sampling=7.0620, total=15.4226
INFO 10-04 03:24:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:24:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6193 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:24:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:24:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2565, fwd=7.8416 cmp_logits=0.0663, sampling=7.2243, total=15.3894
INFO 10-04 03:24:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:24:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5857 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:24:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:24:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2575, fwd=7.8878 cmp_logits=0.0660, sampling=7.1762, total=15.3883
INFO 10-04 03:24:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:24:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5861 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:24:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:24:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2568, fwd=7.8344 cmp_logits=0.0675, sampling=7.2112, total=15.3706
INFO 10-04 03:24:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:24:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5730 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:24:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:24:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2580, fwd=7.8585 cmp_logits=0.0672, sampling=7.1959, total=15.3804
INFO 10-04 03:24:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:24:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5749 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:24:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:24:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2644, fwd=7.9157 cmp_logits=0.0670, sampling=7.1735, total=15.4219
INFO 10-04 03:24:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:24:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6183 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:24:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:24:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2575, fwd=7.8619 cmp_logits=0.0665, sampling=7.2243, total=15.4111
INFO 10-04 03:24:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:24:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6059 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:24:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:24:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2546, fwd=7.9427 cmp_logits=0.0663, sampling=7.1247, total=15.3892
INFO 10-04 03:24:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:24:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5954 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:24:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 03:24:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:24:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2556, fwd=7.8926 cmp_logits=0.0668, sampling=7.1664, total=15.3818
INFO 10-04 03:24:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:24:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6066 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012239.7486994, last_token_time=1728012240.9685683, first_scheduled_time=1728012239.7700057, first_token_time=1728012239.872137, time_in_queue=0.021306276321411133, finished_time=1728012240.9685252, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012239.750512, last_token_time=1728012241.3688116, first_scheduled_time=1728012239.7700057, first_token_time=1728012239.9651635, time_in_queue=0.019493818283081055, finished_time=1728012241.3687847, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012239.752441, last_token_time=1728012241.1633153, first_scheduled_time=1728012239.87257, first_token_time=1728012240.0632875, time_in_queue=0.12012910842895508, finished_time=1728012241.1632893, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012239.7537236, last_token_time=1728012241.6873934, first_scheduled_time=1728012239.96553, first_token_time=1728012240.1413982, time_in_queue=0.2118062973022461, finished_time=1728012241.6873705, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012239.75634, last_token_time=1728012241.7035692, first_scheduled_time=1728012240.0636818, first_token_time=1728012240.219336, time_in_queue=0.3073418140411377, finished_time=1728012241.7035508, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012239.7586877, last_token_time=1728012241.0939643, first_scheduled_time=1728012240.1418135, first_token_time=1728012240.29374, time_in_queue=0.38312578201293945, finished_time=1728012241.0939445, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012239.7596993, last_token_time=1728012241.0939643, first_scheduled_time=1728012240.2198262, first_token_time=1728012240.29374, time_in_queue=0.4601268768310547, finished_time=1728012241.093949, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012239.7607253, last_token_time=1728012241.782411, first_scheduled_time=1728012240.2198262, first_token_time=1728012240.4587586, time_in_queue=0.45910096168518066, finished_time=1728012241.782397, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012239.7632658, last_token_time=1728012242.1587634, first_scheduled_time=1728012240.3712418, first_token_time=1728012240.546428, time_in_queue=0.607975959777832, finished_time=1728012242.1587613, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012239.7668314, last_token_time=1728012241.9856844, first_scheduled_time=1728012240.4593203, first_token_time=1728012240.6769457, time_in_queue=0.6924889087677002, finished_time=1728012241.9856827, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 11197
End-to-End latency: 2.41 seconds
Throughput: 4.15 requests/s, 4645.54 tokens/s
Per_token_time: 0.215 ms

Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=8192, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=2048, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Selected required requests 10
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Chunked prefill is enabled. max_num_batched_tokens=2048, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'type', 'original_max_position_embeddings', 'finetuned'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 03:21:55 config.py:654] [SchedulerConfig] max_num_batched_tokens: 2048 chunked_prefill_enabled: True
INFO 10-04 03:21:55 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 03:21:56 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 03:22:01 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 03:22:04 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 03:22:04 model_runner.py:183] Loaded model: 
INFO 10-04 03:22:04 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 03:22:04 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 03:22:04 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:22:04 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 03:22:04 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 03:22:04 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 03:22:04 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:22:04 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:22:04 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 03:22:04 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 03:22:04 model_runner.py:183]         )
INFO 10-04 03:22:04 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 03:22:04 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:22:04 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:22:04 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 03:22:04 model_runner.py:183]         )
INFO 10-04 03:22:04 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:22:04 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:22:04 model_runner.py:183]       )
INFO 10-04 03:22:04 model_runner.py:183]     )
INFO 10-04 03:22:04 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:22:04 model_runner.py:183]   )
INFO 10-04 03:22:04 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:22:04 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 03:22:04 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 03:22:04 model_runner.py:183] )
INFO 10-04 03:22:05 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 03:22:05 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 8.7676, fwd=378.6023 cmp_logits=0.2382, sampling=157.1879, total=544.7979
INFO 10-04 03:22:05 worker.py:164] Peak: 13.332 GB, Initial: 38.980 GB, Free: 25.648 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.092 GB
INFO 10-04 03:22:05 gpu_executor.py:117] # GPU blocks: 3083, # CPU blocks: 8192
INFO 10-04 03:22:36 worker.py:189] _init_cache_engine took 24.0859 GB
INFO 10-04 03:22:36 scheduler.py:307] Scheduler initialized with prompt limit: 8192
INFO 10-04 03:22:36 Start warmup...
INFO 10-04 03:22:36 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:36 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:22:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8969, fwd=1952.9350 cmp_logits=71.5077, sampling=0.9325, total=2026.2752
INFO 10-04 03:22:38 metrics.py:335] Avg prompt throughput: 503.6 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 2033.3
INFO 10-04 03:22:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 2026.7909 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3064, fwd=45.1696 cmp_logits=0.1202, sampling=6.6149, total=52.2130
INFO 10-04 03:22:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 53.0
INFO 10-04 03:22:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 52.6466 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3140, fwd=7.9889 cmp_logits=0.0696, sampling=7.1218, total=15.4958
INFO 10-04 03:22:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 03:22:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7809 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2763, fwd=7.8771 cmp_logits=0.0699, sampling=7.2045, total=15.4293
INFO 10-04 03:22:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:22:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6550 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2718, fwd=7.8855 cmp_logits=0.0696, sampling=6.3024, total=14.5307
INFO 10-04 03:22:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.9
INFO 10-04 03:22:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.7445 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2675, fwd=7.8607 cmp_logits=0.0696, sampling=6.0556, total=14.2548
INFO 10-04 03:22:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 03:22:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4715 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2730, fwd=7.9074 cmp_logits=0.0739, sampling=6.0015, total=14.2572
INFO 10-04 03:22:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:22:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4765 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2644, fwd=7.8230 cmp_logits=0.0687, sampling=6.0825, total=14.2393
INFO 10-04 03:22:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 03:22:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4770 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2635, fwd=7.8785 cmp_logits=0.0670, sampling=6.0511, total=14.2610
INFO 10-04 03:22:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 03:22:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4563 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2601, fwd=7.8282 cmp_logits=0.0665, sampling=6.0723, total=14.2281
INFO 10-04 03:22:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 03:22:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4398 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:43 Start benchmarking...
INFO 10-04 03:22:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 03:22:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.0989, fwd=1599.2556 cmp_logits=0.1554, sampling=140.7082, total=1741.2195
INFO 10-04 03:22:45 metrics.py:335] Avg prompt throughput: 1155.1 tokens/s, Avg generation throughput: 1.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%, Interval(ms): 1773.1
INFO 10-04 03:22:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1741.8168 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:22:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 03:22:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1368, fwd=14.8580 cmp_logits=0.0873, sampling=126.3721, total=142.4553
INFO 10-04 03:22:45 metrics.py:335] Avg prompt throughput: 14291.0 tokens/s, Avg generation throughput: 27.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 9.8%, CPU KV cache usage: 0.0%, Interval(ms): 143.2
INFO 10-04 03:22:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 142.9150 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 03:22:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1132, fwd=14.4496 cmp_logits=0.0834, sampling=119.9543, total=135.6015
INFO 10-04 03:22:45 metrics.py:335] Avg prompt throughput: 14999.2 tokens/s, Avg generation throughput: 51.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 14.9%, CPU KV cache usage: 0.0%, Interval(ms): 136.3
INFO 10-04 03:22:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 136.1156 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:22:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 03:22:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1003, fwd=14.4322 cmp_logits=0.0827, sampling=120.8053, total=136.4214
INFO 10-04 03:22:45 metrics.py:335] Avg prompt throughput: 14887.5 tokens/s, Avg generation throughput: 58.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 18.6%, CPU KV cache usage: 0.0%, Interval(ms): 137.1
INFO 10-04 03:22:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 136.9221 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:22:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:45 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1184, fwd=14.4880 cmp_logits=0.0846, sampling=129.9639, total=145.6559
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 13937.5 tokens/s, Avg generation throughput: 61.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.8%, CPU KV cache usage: 0.0%, Interval(ms): 146.4
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 146.1971 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([448]), positions.shape=torch.Size([448]) hidden_states.shape=torch.Size([448, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7265, fwd=14.6394 cmp_logits=0.0837, sampling=37.3225, total=52.7730
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 8208.6 tokens/s, Avg generation throughput: 187.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 53.5
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.3035 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4232, fwd=7.8118 cmp_logits=0.0677, sampling=9.1922, total=17.4959
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0073 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4189, fwd=7.7605 cmp_logits=0.0696, sampling=9.2134, total=17.4632
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9708 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4170, fwd=7.7152 cmp_logits=0.0682, sampling=9.2671, total=17.4687
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9768 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4222, fwd=7.7786 cmp_logits=0.0677, sampling=9.2380, total=17.5076
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0175 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4168, fwd=7.7505 cmp_logits=0.0677, sampling=9.2444, total=17.4806
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9906 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4160, fwd=7.7600 cmp_logits=0.0687, sampling=9.3131, total=17.5588
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0616 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4244, fwd=7.7205 cmp_logits=0.0741, sampling=9.2788, total=17.4985
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0039 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4127, fwd=7.7574 cmp_logits=0.0675, sampling=9.2266, total=17.4654
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9691 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4184, fwd=7.7045 cmp_logits=0.0668, sampling=9.2928, total=17.4832
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9973 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4165, fwd=7.6966 cmp_logits=0.0677, sampling=9.3215, total=17.5030
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0531 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4208, fwd=7.7233 cmp_logits=0.0677, sampling=9.2971, total=17.5104
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0140 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4175, fwd=7.7393 cmp_logits=0.0687, sampling=9.3036, total=17.5297
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0259 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4103, fwd=7.6869 cmp_logits=0.0677, sampling=9.3205, total=17.4863
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9923 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4177, fwd=7.7341 cmp_logits=0.0677, sampling=9.2826, total=17.5030
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0039 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4146, fwd=7.7155 cmp_logits=0.0675, sampling=9.3281, total=17.5266
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0571 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4182, fwd=7.7212 cmp_logits=0.0668, sampling=9.3243, total=17.5314
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0328 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4179, fwd=7.7035 cmp_logits=0.0679, sampling=9.3493, total=17.5400
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0409 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4251, fwd=7.7112 cmp_logits=0.0703, sampling=9.3315, total=17.5393
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0452 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4141, fwd=7.7121 cmp_logits=0.0682, sampling=9.3296, total=17.5250
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0247 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4156, fwd=7.6845 cmp_logits=0.0672, sampling=9.3989, total=17.5674
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 546.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1069 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4203, fwd=7.6931 cmp_logits=0.0679, sampling=9.3834, total=17.5657
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0850 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4060, fwd=7.7965 cmp_logits=0.0675, sampling=8.9836, total=17.2546
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7288 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3998, fwd=7.7007 cmp_logits=0.0670, sampling=9.0516, total=17.2200
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 503.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7062 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4013, fwd=7.7107 cmp_logits=0.0682, sampling=9.0783, total=17.2596
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7405 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4032, fwd=7.7307 cmp_logits=0.0670, sampling=9.0411, total=17.2429
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7884 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3748, fwd=7.7803 cmp_logits=0.0675, sampling=8.5435, total=16.7670
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1821 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3691, fwd=7.7019 cmp_logits=0.0668, sampling=8.5890, total=16.7277
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 404.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1442 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3717, fwd=7.7164 cmp_logits=0.0663, sampling=8.5623, total=16.7177
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 404.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1397 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3700, fwd=7.6959 cmp_logits=0.0668, sampling=8.6255, total=16.7589
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1711 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3715, fwd=7.7116 cmp_logits=0.0670, sampling=8.5750, total=16.7263
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1840 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3741, fwd=7.7350 cmp_logits=0.0668, sampling=8.6610, total=16.8378
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2703 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3555, fwd=7.7362 cmp_logits=0.0794, sampling=8.3985, total=16.5706
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9504 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3567, fwd=7.7050 cmp_logits=0.0668, sampling=8.4424, total=16.5720
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9570 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3550, fwd=7.7152 cmp_logits=0.0675, sampling=8.4615, total=16.5999
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9806 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3521, fwd=7.7324 cmp_logits=0.0675, sampling=8.4486, total=16.6016
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9897 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3524, fwd=7.7622 cmp_logits=0.0668, sampling=8.3897, total=16.5722
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9566 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3567, fwd=7.7276 cmp_logits=0.0663, sampling=8.4352, total=16.5868
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9675 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3562, fwd=7.7002 cmp_logits=0.0677, sampling=8.4410, total=16.5660
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9539 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3569, fwd=7.7136 cmp_logits=0.0665, sampling=8.4321, total=16.5701
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9597 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3521, fwd=7.7398 cmp_logits=0.0670, sampling=8.4541, total=16.6140
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0000 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3588, fwd=7.7364 cmp_logits=0.0665, sampling=8.4248, total=16.5877
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9773 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3605, fwd=7.7326 cmp_logits=0.0701, sampling=8.4326, total=16.5968
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9830 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3533, fwd=7.7834 cmp_logits=0.0672, sampling=8.3549, total=16.5598
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9644 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3421, fwd=7.7980 cmp_logits=0.0799, sampling=8.0755, total=16.2964
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6507 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3414, fwd=7.7481 cmp_logits=0.0663, sampling=8.1542, total=16.3109
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6674 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3498, fwd=7.7047 cmp_logits=0.0663, sampling=8.1837, total=16.3054
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6624 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3421, fwd=7.7536 cmp_logits=0.0668, sampling=8.0571, total=16.2206
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5741 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3431, fwd=7.7074 cmp_logits=0.0668, sampling=8.1205, total=16.2387
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5896 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3402, fwd=7.6947 cmp_logits=0.0656, sampling=8.1251, total=16.2265
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5782 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3393, fwd=7.7860 cmp_logits=0.0663, sampling=8.0502, total=16.2427
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5949 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:46 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3362, fwd=7.6725 cmp_logits=0.0668, sampling=8.1580, total=16.2344
INFO 10-04 03:22:46 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:22:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5927 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:46 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3383, fwd=7.7233 cmp_logits=0.0668, sampling=8.0917, total=16.2208
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5718 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3438, fwd=7.7350 cmp_logits=0.0672, sampling=8.0764, total=16.2234
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5722 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3376, fwd=7.7260 cmp_logits=0.0663, sampling=8.0616, total=16.1927
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5453 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3371, fwd=7.7600 cmp_logits=0.0660, sampling=8.0819, total=16.2461
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5954 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3386, fwd=7.7636 cmp_logits=0.0665, sampling=8.0416, total=16.2113
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5656 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3474, fwd=7.7384 cmp_logits=0.0672, sampling=8.0671, total=16.2210
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5806 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3426, fwd=7.7245 cmp_logits=0.0660, sampling=8.1072, total=16.2413
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5937 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3386, fwd=7.7646 cmp_logits=0.0663, sampling=8.0600, total=16.2303
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.6%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5806 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3390, fwd=7.7393 cmp_logits=0.0660, sampling=8.1177, total=16.2630
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.6%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6137 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3362, fwd=7.7424 cmp_logits=0.0658, sampling=8.0805, total=16.2258
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.5 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5970 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3314, fwd=7.7193 cmp_logits=0.0665, sampling=7.5407, total=15.6586
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.0029 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3178, fwd=7.7488 cmp_logits=0.0870, sampling=7.2203, total=15.3749
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6615 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3145, fwd=7.7240 cmp_logits=0.0653, sampling=7.2181, total=15.3229
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6069 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3068, fwd=7.7605 cmp_logits=0.0663, sampling=7.2284, total=15.3630
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6684 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2875, fwd=7.7682 cmp_logits=0.0656, sampling=7.1428, total=15.2650
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5087 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2880, fwd=7.7198 cmp_logits=0.0656, sampling=7.1754, total=15.2497
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4912 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2897, fwd=7.7400 cmp_logits=0.0653, sampling=7.1619, total=15.2581
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5017 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2873, fwd=7.6897 cmp_logits=0.0648, sampling=7.2055, total=15.2483
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4905 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2861, fwd=7.7460 cmp_logits=0.0663, sampling=7.1931, total=15.2924
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5325 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2851, fwd=7.7229 cmp_logits=0.0665, sampling=7.1692, total=15.2447
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4862 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2863, fwd=7.6914 cmp_logits=0.0663, sampling=7.1859, total=15.2309
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4698 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2873, fwd=7.7143 cmp_logits=0.0658, sampling=7.1855, total=15.2538
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4939 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2859, fwd=7.7097 cmp_logits=0.0656, sampling=7.1683, total=15.2304
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4703 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2851, fwd=7.7732 cmp_logits=0.0663, sampling=7.1590, total=15.2843
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5323 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2863, fwd=7.7064 cmp_logits=0.0670, sampling=7.1883, total=15.2488
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4896 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2878, fwd=7.6387 cmp_logits=0.0660, sampling=7.2565, total=15.2500
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5132 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2646, fwd=7.9055 cmp_logits=0.0668, sampling=7.1599, total=15.3973
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5950 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2642, fwd=7.8924 cmp_logits=0.0660, sampling=7.1785, total=15.4023
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5966 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2615, fwd=7.9010 cmp_logits=0.0663, sampling=7.1886, total=15.4181
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6124 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2637, fwd=7.8311 cmp_logits=0.0670, sampling=7.2167, total=15.3797
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5754 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2646, fwd=7.8607 cmp_logits=0.0677, sampling=7.2088, total=15.4028
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6074 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2620, fwd=7.8690 cmp_logits=0.0663, sampling=7.1816, total=15.3799
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5752 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2694, fwd=7.8726 cmp_logits=0.0670, sampling=7.1921, total=15.4021
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5969 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2663, fwd=7.9303 cmp_logits=0.0665, sampling=7.1828, total=15.4469
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6419 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2632, fwd=7.8650 cmp_logits=0.0663, sampling=7.2162, total=15.4116
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6071 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2651, fwd=7.8924 cmp_logits=0.0679, sampling=7.1597, total=15.3863
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5880 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2649, fwd=7.9012 cmp_logits=0.0670, sampling=7.1459, total=15.3797
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5740 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:22:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 03:22:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:22:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2654, fwd=7.9043 cmp_logits=0.0670, sampling=7.1549, total=15.3930
INFO 10-04 03:22:47 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:22:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6190 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012163.7110298, last_token_time=1728012166.4725761, first_scheduled_time=1728012163.7324438, first_token_time=1728012165.4738326, time_in_queue=0.02141404151916504, finished_time=1728012166.472535, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012163.712898, last_token_time=1728012166.8539517, first_scheduled_time=1728012163.7324438, first_token_time=1728012165.4738326, time_in_queue=0.019545793533325195, finished_time=1728012166.8539255, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012163.7148092, last_token_time=1728012166.64838, first_scheduled_time=1728012163.7324438, first_token_time=1728012165.6170068, time_in_queue=0.01763463020324707, finished_time=1728012166.6483572, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012163.7160754, last_token_time=1728012167.1554837, first_scheduled_time=1728012165.4744196, first_token_time=1728012165.6170068, time_in_queue=1.7583441734313965, finished_time=1728012167.1554625, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012163.7187133, last_token_time=1728012167.171678, first_scheduled_time=1728012165.4744196, first_token_time=1728012165.7532084, time_in_queue=1.7557063102722168, finished_time=1728012167.1716595, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012163.7210999, last_token_time=1728012166.5442579, first_scheduled_time=1728012165.617474, first_token_time=1728012165.7532084, time_in_queue=1.896374225616455, finished_time=1728012166.5442393, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012163.7221396, last_token_time=1728012166.5442579, first_scheduled_time=1728012165.617474, first_token_time=1728012165.7532084, time_in_queue=1.8953344821929932, finished_time=1728012166.5442436, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012163.7231655, last_token_time=1728012167.2190998, first_scheduled_time=1728012165.617474, first_token_time=1728012165.890301, time_in_queue=1.8943085670471191, finished_time=1728012167.219086, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012163.7256937, last_token_time=1728012167.595644, first_scheduled_time=1728012165.753746, first_token_time=1728012166.036638, time_in_queue=2.02805233001709, finished_time=1728012167.5956416, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012163.7292302, last_token_time=1728012167.4068065, first_scheduled_time=1728012165.8908408, first_token_time=1728012166.0900984, time_in_queue=2.1616106033325195, finished_time=1728012167.4068046, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 11197
End-to-End latency: 3.88 seconds
Throughput: 2.57 requests/s, 2882.24 tokens/s
Per_token_time: 0.347 ms

Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=128, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=8192, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=4096, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Selected required requests 20
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Chunked prefill is enabled. max_num_batched_tokens=4096, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings', 'type', 'finetuned'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 03:48:41 config.py:654] [SchedulerConfig] max_num_batched_tokens: 4096 chunked_prefill_enabled: True
INFO 10-04 03:48:41 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 03:48:42 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 03:48:46 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 03:48:50 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 03:48:50 model_runner.py:183] Loaded model: 
INFO 10-04 03:48:50 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 03:48:50 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 03:48:50 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:48:50 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 03:48:50 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 03:48:50 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 03:48:50 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:48:50 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:48:50 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 03:48:50 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 03:48:50 model_runner.py:183]         )
INFO 10-04 03:48:50 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 03:48:50 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:48:50 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:48:50 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 03:48:50 model_runner.py:183]         )
INFO 10-04 03:48:50 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:48:50 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:48:50 model_runner.py:183]       )
INFO 10-04 03:48:50 model_runner.py:183]     )
INFO 10-04 03:48:50 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:48:50 model_runner.py:183]   )
INFO 10-04 03:48:50 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:48:50 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 03:48:50 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 03:48:50 model_runner.py:183] )
INFO 10-04 03:48:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 03:48:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 12.8548, fwd=340.0347 cmp_logits=0.2422, sampling=256.9337, total=610.0674
INFO 10-04 03:48:51 worker.py:164] Peak: 13.385 GB, Initial: 38.980 GB, Free: 25.595 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.039 GB
INFO 10-04 03:48:51 gpu_executor.py:117] # GPU blocks: 3077, # CPU blocks: 8192
INFO 10-04 03:49:22 worker.py:189] _init_cache_engine took 24.0625 GB
INFO 10-04 03:49:22 scheduler.py:307] Scheduler initialized with prompt limit: 8192
INFO 10-04 03:49:22 Start warmup...
INFO 10-04 03:49:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:49:25 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9379, fwd=3507.9091 cmp_logits=71.1753, sampling=0.9844, total=3581.0094
INFO 10-04 03:49:25 metrics.py:335] Avg prompt throughput: 285.4 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 3588.5
INFO 10-04 03:49:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 3581.5766 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:25 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3183, fwd=42.4762 cmp_logits=0.1223, sampling=6.0608, total=48.9798
INFO 10-04 03:49:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 49.8
INFO 10-04 03:49:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.4349 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3238, fwd=8.2235 cmp_logits=0.0806, sampling=6.5818, total=15.2113
INFO 10-04 03:49:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:49:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5358 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2890, fwd=8.1098 cmp_logits=0.0737, sampling=6.6621, total=15.1362
INFO 10-04 03:49:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:49:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4047 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2797, fwd=8.1420 cmp_logits=0.0715, sampling=6.7456, total=15.2404
INFO 10-04 03:49:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:49:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4781 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2787, fwd=8.0223 cmp_logits=0.0713, sampling=6.7379, total=15.1117
INFO 10-04 03:49:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-04 03:49:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3451 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2782, fwd=8.1239 cmp_logits=0.0751, sampling=6.6435, total=15.1219
INFO 10-04 03:49:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-04 03:49:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3482 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2706, fwd=8.0683 cmp_logits=0.0706, sampling=6.6929, total=15.1033
INFO 10-04 03:49:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-04 03:49:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3544 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2694, fwd=8.0905 cmp_logits=0.0684, sampling=6.6912, total=15.1207
INFO 10-04 03:49:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-04 03:49:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3224 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:26 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2668, fwd=8.1017 cmp_logits=0.0682, sampling=6.6946, total=15.1322
INFO 10-04 03:49:26 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-04 03:49:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3549 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:31 Start benchmarking...
INFO 10-04 03:49:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 03:49:33 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.8365, fwd=1597.1155 cmp_logits=0.1633, sampling=280.5562, total=1879.6730
INFO 10-04 03:49:33 metrics.py:335] Avg prompt throughput: 2132.8 tokens/s, Avg generation throughput: 3.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 10.7%, CPU KV cache usage: 0.0%, Interval(ms): 1920.4
INFO 10-04 03:49:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1880.5737 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:49:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:33 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 03:49:33 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.7879, fwd=14.8008 cmp_logits=0.0999, sampling=244.2811, total=260.9711
INFO 10-04 03:49:33 metrics.py:335] Avg prompt throughput: 15613.0 tokens/s, Avg generation throughput: 34.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 261.9
INFO 10-04 03:49:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 261.6382 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:49:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:33 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 03:49:33 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.8194, fwd=14.5094 cmp_logits=0.0918, sampling=250.0091, total=266.4306
INFO 10-04 03:49:33 metrics.py:335] Avg prompt throughput: 15277.1 tokens/s, Avg generation throughput: 59.8 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 25.4%, CPU KV cache usage: 0.0%, Interval(ms): 267.5
INFO 10-04 03:49:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 267.3194 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-04 03:49:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:33 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2846]), positions.shape=torch.Size([2846]) hidden_states.shape=torch.Size([2846, 4096]) residual=None
INFO 10-04 03:49:33 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.5228, fwd=14.5435 cmp_logits=0.0944, sampling=183.3918, total=199.5535
INFO 10-04 03:49:33 metrics.py:335] Avg prompt throughput: 14101.9 tokens/s, Avg generation throughput: 99.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.1%, CPU KV cache usage: 0.0%, Interval(ms): 200.7
INFO 10-04 03:49:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 200.4533 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 03:49:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:33 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 03:49:33 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5851, fwd=8.0249 cmp_logits=0.0718, sampling=11.4691, total=20.1521
INFO 10-04 03:49:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 941.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-04 03:49:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0187 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 03:49:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:33 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 03:49:33 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5789, fwd=7.9305 cmp_logits=0.0699, sampling=11.6138, total=20.1943
INFO 10-04 03:49:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-04 03:49:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9846 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 03:49:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:33 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 03:49:33 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5648, fwd=7.8690 cmp_logits=0.0699, sampling=11.7545, total=20.2594
INFO 10-04 03:49:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 940.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-04 03:49:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0559 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 03:49:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:33 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 03:49:33 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5698, fwd=7.9353 cmp_logits=0.0713, sampling=11.6134, total=20.1907
INFO 10-04 03:49:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-04 03:49:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9920 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 03:49:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:33 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 03:49:33 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5810, fwd=7.9477 cmp_logits=0.0708, sampling=11.5590, total=20.1595
INFO 10-04 03:49:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.7 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-04 03:49:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9806 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 03:49:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:33 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-04 03:49:33 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5596, fwd=7.9994 cmp_logits=0.0818, sampling=11.4086, total=20.0498
INFO 10-04 03:49:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 899.8 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-04 03:49:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8368 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-04 03:49:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:33 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([18]), positions.shape=torch.Size([18]) hidden_states.shape=torch.Size([18, 4096]) residual=None
INFO 10-04 03:49:33 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5426, fwd=7.9899 cmp_logits=0.0823, sampling=11.4188, total=20.0346
INFO 10-04 03:49:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 856.2 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-04 03:49:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7875 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-04 03:49:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:33 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:49:33 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5119, fwd=7.9923 cmp_logits=0.0708, sampling=10.7756, total=19.3517
INFO 10-04 03:49:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 788.4 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.3
INFO 10-04 03:49:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0419 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:49:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:33 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:49:33 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5066, fwd=7.9389 cmp_logits=0.0708, sampling=10.8378, total=19.3551
INFO 10-04 03:49:33 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 790.4 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 03:49:33 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0441 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:49:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:33 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5121, fwd=7.9198 cmp_logits=0.0694, sampling=10.8876, total=19.3899
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 788.7 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.3
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0856 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5231, fwd=7.9482 cmp_logits=0.0696, sampling=10.8395, total=19.3813
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 787.4 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.3
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0877 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5114, fwd=7.9157 cmp_logits=0.0699, sampling=10.8867, total=19.3849
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 789.3 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.3
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0756 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5045, fwd=7.8592 cmp_logits=0.0699, sampling=10.9222, total=19.3570
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 789.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.3
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0622 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4966, fwd=7.9374 cmp_logits=0.0823, sampling=10.8066, total=19.3243
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 741.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0005 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4804, fwd=7.9453 cmp_logits=0.0823, sampling=10.3180, total=18.8270
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 710.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.7%, CPU KV cache usage: 0.0%, Interval(ms): 19.7
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.4743 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4835, fwd=7.9000 cmp_logits=0.0696, sampling=10.3645, total=18.8184
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 710.5 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.7
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.4783 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4652, fwd=7.9761 cmp_logits=0.0815, sampling=10.1724, total=18.6963
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 665.5 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3064 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4759, fwd=7.9067 cmp_logits=0.0691, sampling=10.3006, total=18.7535
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 664.8 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3646 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4654, fwd=7.8905 cmp_logits=0.0696, sampling=10.3245, total=18.7514
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 665.1 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3570 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4685, fwd=7.8728 cmp_logits=0.0689, sampling=10.3507, total=18.7619
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 664.5 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3748 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4730, fwd=7.8650 cmp_logits=0.0689, sampling=10.3936, total=18.8019
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 661.9 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.4159 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4663, fwd=7.8807 cmp_logits=0.0679, sampling=10.3490, total=18.7650
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 664.9 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3672 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4706, fwd=7.8850 cmp_logits=0.0684, sampling=10.3290, total=18.7540
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 664.3 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3815 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4523, fwd=7.9710 cmp_logits=0.0825, sampling=9.8665, total=18.3735
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 626.0 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9531 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4342, fwd=7.9775 cmp_logits=0.0823, sampling=9.7096, total=18.2045
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 580.0 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.0
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7469 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4385, fwd=7.9000 cmp_logits=0.0689, sampling=9.7899, total=18.1980
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 580.1 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.0
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7464 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4351, fwd=7.8361 cmp_logits=0.0687, sampling=9.8631, total=18.2042
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 581.0 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7519 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4389, fwd=7.9131 cmp_logits=0.0696, sampling=9.7830, total=18.2054
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 580.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7635 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4287, fwd=7.9637 cmp_logits=0.0832, sampling=9.5363, total=18.0130
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 533.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.8
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5425 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4189, fwd=7.8776 cmp_logits=0.0682, sampling=9.6116, total=17.9775
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 535.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4934 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4234, fwd=7.9274 cmp_logits=0.0679, sampling=9.5687, total=17.9884
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 534.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5173 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4272, fwd=7.9014 cmp_logits=0.0677, sampling=9.6259, total=18.0230
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 533.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5544 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4065, fwd=7.9195 cmp_logits=0.0682, sampling=9.2356, total=17.6308
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 491.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1148 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4082, fwd=7.8709 cmp_logits=0.0672, sampling=9.2149, total=17.5619
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0411 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4067, fwd=7.8638 cmp_logits=0.0682, sampling=9.2447, total=17.5841
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0628 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4110, fwd=7.8895 cmp_logits=0.0679, sampling=9.2421, total=17.6117
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 491.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0948 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4060, fwd=7.8871 cmp_logits=0.0672, sampling=9.2144, total=17.5757
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0578 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4067, fwd=7.8924 cmp_logits=0.0682, sampling=9.2330, total=17.6013
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 492.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0957 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4036, fwd=7.9041 cmp_logits=0.0687, sampling=9.1958, total=17.5736
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0531 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4091, fwd=7.8912 cmp_logits=0.0679, sampling=9.2266, total=17.5958
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0843 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4060, fwd=7.9041 cmp_logits=0.0675, sampling=9.2120, total=17.5905
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 492.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0769 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4056, fwd=7.8952 cmp_logits=0.0679, sampling=9.2058, total=17.5753
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0702 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3929, fwd=7.9167 cmp_logits=0.0818, sampling=8.8735, total=17.2660
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 446.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.6%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7157 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3891, fwd=7.9124 cmp_logits=0.0684, sampling=8.8942, total=17.2651
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 447.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7135 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3893, fwd=7.9031 cmp_logits=0.0677, sampling=8.9202, total=17.2813
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 446.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7331 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3932, fwd=7.8666 cmp_logits=0.0672, sampling=8.9443, total=17.2722
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 445.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7462 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3753, fwd=7.9155 cmp_logits=0.0689, sampling=8.5294, total=16.8900
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3180 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3736, fwd=7.8716 cmp_logits=0.0677, sampling=8.5554, total=16.8693
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2875 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3762, fwd=7.9060 cmp_logits=0.0668, sampling=8.5034, total=16.8536
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2865 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3791, fwd=7.8924 cmp_logits=0.0684, sampling=8.5754, total=16.9163
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3335 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3769, fwd=7.9126 cmp_logits=0.0675, sampling=8.5554, total=16.9134
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3390 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3791, fwd=7.8866 cmp_logits=0.0672, sampling=8.5669, total=16.9010
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3221 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3748, fwd=7.8914 cmp_logits=0.0675, sampling=8.5578, total=16.8924
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3297 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3600, fwd=7.9269 cmp_logits=0.0813, sampling=8.1613, total=16.5305
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.9%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9380 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3452, fwd=7.9463 cmp_logits=0.0799, sampling=7.7891, total=16.1614
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5353 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3283, fwd=7.9055 cmp_logits=0.0823, sampling=7.3104, total=15.6274
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.3 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9509 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3269, fwd=7.8762 cmp_logits=0.0668, sampling=7.3364, total=15.6071
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9256 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3252, fwd=7.8583 cmp_logits=0.0665, sampling=7.3414, total=15.5923
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9330 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3116, fwd=7.9732 cmp_logits=0.0806, sampling=7.0283, total=15.3949
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6801 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3088, fwd=7.8511 cmp_logits=0.0660, sampling=7.1459, total=15.3728
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6548 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3090, fwd=7.9081 cmp_logits=0.0663, sampling=7.0958, total=15.3801
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6653 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3083, fwd=7.8712 cmp_logits=0.0663, sampling=7.1375, total=15.3842
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6684 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3090, fwd=7.8263 cmp_logits=0.0660, sampling=7.1781, total=15.3804
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6653 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:49:34 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3085, fwd=7.8704 cmp_logits=0.0663, sampling=7.1349, total=15.3811
INFO 10-04 03:49:34 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:49:34 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6670 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:49:34 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:34 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3085, fwd=7.8039 cmp_logits=0.0660, sampling=7.2188, total=15.3983
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6834 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3128, fwd=7.9410 cmp_logits=0.0677, sampling=7.0889, total=15.4114
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7080 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3088, fwd=7.8304 cmp_logits=0.0658, sampling=7.1864, total=15.3921
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6760 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3109, fwd=7.8561 cmp_logits=0.0663, sampling=7.1566, total=15.3911
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6727 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3090, fwd=7.8921 cmp_logits=0.0675, sampling=7.1206, total=15.3902
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6808 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3207, fwd=7.9036 cmp_logits=0.0679, sampling=7.1282, total=15.4214
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7406 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3200, fwd=7.8650 cmp_logits=0.0739, sampling=7.1502, total=15.4102
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7177 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2918, fwd=7.8979 cmp_logits=0.0789, sampling=6.9757, total=15.2452
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4922 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2873, fwd=7.8182 cmp_logits=0.0665, sampling=7.0505, total=15.2235
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4698 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2918, fwd=7.8442 cmp_logits=0.0658, sampling=7.0136, total=15.2164
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4889 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2673, fwd=8.0328 cmp_logits=0.0682, sampling=6.9916, total=15.3608
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5940 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2656, fwd=8.0161 cmp_logits=0.0672, sampling=7.0095, total=15.3592
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5566 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2661, fwd=8.0526 cmp_logits=0.0675, sampling=7.0078, total=15.3949
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5916 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2663, fwd=8.0366 cmp_logits=0.0677, sampling=6.9895, total=15.3608
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5578 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2646, fwd=8.0729 cmp_logits=0.0668, sampling=6.9499, total=15.3553
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5540 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2658, fwd=8.0469 cmp_logits=0.0665, sampling=6.9826, total=15.3625
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5933 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2677, fwd=8.1325 cmp_logits=0.0672, sampling=6.9349, total=15.4033
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6047 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2651, fwd=8.0717 cmp_logits=0.0675, sampling=7.0319, total=15.4371
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6362 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2658, fwd=8.0752 cmp_logits=0.0675, sampling=7.0362, total=15.4457
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6438 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2670, fwd=8.0521 cmp_logits=0.0670, sampling=7.0665, total=15.4533
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6519 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2809, fwd=8.0214 cmp_logits=0.0672, sampling=7.0994, total=15.4700
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7032 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:49:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 03:49:35 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:49:35 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2747, fwd=8.0523 cmp_logits=0.0679, sampling=7.0572, total=15.4531
INFO 10-04 03:49:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:49:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6767 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.159469, last_token_time=1728013773.9060287, first_scheduled_time=1728013771.1895876, first_token_time=1728013773.0694785, time_in_queue=0.030118703842163086, finished_time=1728013773.9059489, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.1604085, last_token_time=1728013774.4362893, first_scheduled_time=1728013771.1895876, first_token_time=1728013773.0694785, time_in_queue=0.029179096221923828, finished_time=1728013774.4362452, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.1619947, last_token_time=1728013774.1294756, first_scheduled_time=1728013771.1895876, first_token_time=1728013773.0694785, time_in_queue=0.027592897415161133, finished_time=1728013774.129423, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.1627622, last_token_time=1728013774.8300965, first_scheduled_time=1728013771.1895876, first_token_time=1728013773.0694785, time_in_queue=0.026825428009033203, finished_time=1728013774.8300712, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.1649582, last_token_time=1728013774.8129563, first_scheduled_time=1728013771.1895876, first_token_time=1728013773.0694785, time_in_queue=0.02462935447692871, finished_time=1728013774.8129334, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.1670554, last_token_time=1728013773.9481695, first_scheduled_time=1728013771.1895876, first_token_time=1728013773.0694785, time_in_queue=0.022532224655151367, finished_time=1728013773.9481146, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.167506, last_token_time=1728013773.9481695, first_scheduled_time=1728013771.1895876, first_token_time=1728013773.0694785, time_in_queue=0.022081613540649414, finished_time=1728013773.9481187, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.1680925, last_token_time=1728013774.895165, first_scheduled_time=1728013771.1895876, first_token_time=1728013773.331426, time_in_queue=0.02149510383605957, finished_time=1728013774.8951476, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.1703458, last_token_time=1728013775.3370464, first_scheduled_time=1728013773.0702903, first_token_time=1728013773.331426, time_in_queue=1.899944543838501, finished_time=1728013775.3370438, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.1738214, last_token_time=1728013775.1010644, first_scheduled_time=1728013773.0702903, first_token_time=1728013773.59877, time_in_queue=1.8964688777923584, finished_time=1728013775.101058, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.1765418, last_token_time=1728013774.0698514, first_scheduled_time=1728013773.3321538, first_token_time=1728013773.59877, time_in_queue=2.155611991882324, finished_time=1728013774.0698183, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.1771, last_token_time=1728013774.2664373, first_scheduled_time=1728013773.3321538, first_token_time=1728013773.59877, time_in_queue=2.1550538539886475, finished_time=1728013774.2664108, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.1780393, last_token_time=1728013774.618863, first_scheduled_time=1728013773.3321538, first_token_time=1728013773.59877, time_in_queue=2.1541144847869873, finished_time=1728013774.6188498, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.1795995, last_token_time=1728013774.690516, first_scheduled_time=1728013773.3321538, first_token_time=1728013773.59877, time_in_queue=2.1525542736053467, finished_time=1728013774.6905053, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.1812935, last_token_time=1728013774.0900764, first_scheduled_time=1728013773.3321538, first_token_time=1728013773.59877, time_in_queue=2.15086030960083, finished_time=1728013774.0900583, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.1818728, last_token_time=1728013774.3614078, first_scheduled_time=1728013773.3321538, first_token_time=1728013773.59877, time_in_queue=2.1502809524536133, finished_time=1728013774.3613973, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.182927, last_token_time=1728013774.285606, first_scheduled_time=1728013773.3321538, first_token_time=1728013773.7994242, time_in_queue=2.1492269039154053, finished_time=1728013774.285596, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.1838098, last_token_time=1728013773.9271457, first_scheduled_time=1728013773.5996826, first_token_time=1728013773.7994242, time_in_queue=2.415872812271118, finished_time=1728013773.9271357, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.184081, last_token_time=1728013774.8468297, first_scheduled_time=1728013773.5996826, first_token_time=1728013773.7994242, time_in_queue=2.4156014919281006, finished_time=1728013774.8468237, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728013771.186265, last_token_time=1728013775.1479867, first_scheduled_time=1728013773.5996826, first_token_time=1728013773.7994242, time_in_queue=2.4134175777435303, finished_time=1728013775.147985, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 15868
End-to-End latency: 4.18 seconds
Throughput: 4.79 requests/s, 3798.18 tokens/s
Per_token_time: 0.263 ms

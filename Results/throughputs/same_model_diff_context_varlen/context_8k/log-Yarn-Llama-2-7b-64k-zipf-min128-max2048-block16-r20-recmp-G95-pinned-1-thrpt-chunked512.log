Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=128, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=8192, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=512, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Selected required requests 20
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Chunked prefill is enabled. max_num_batched_tokens=512, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings', 'finetuned', 'type'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 03:52:38 config.py:654] [SchedulerConfig] max_num_batched_tokens: 512 chunked_prefill_enabled: True
INFO 10-04 03:52:38 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 03:52:38 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 03:52:43 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 03:52:47 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 03:52:47 model_runner.py:183] Loaded model: 
INFO 10-04 03:52:47 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 03:52:47 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 03:52:47 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:52:47 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 03:52:47 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 03:52:47 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 03:52:47 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:52:47 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:52:47 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 03:52:47 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 03:52:47 model_runner.py:183]         )
INFO 10-04 03:52:47 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 03:52:47 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:52:47 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:52:47 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 03:52:47 model_runner.py:183]         )
INFO 10-04 03:52:47 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:52:47 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:52:47 model_runner.py:183]       )
INFO 10-04 03:52:47 model_runner.py:183]     )
INFO 10-04 03:52:47 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:52:47 model_runner.py:183]   )
INFO 10-04 03:52:47 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:52:47 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 03:52:47 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 03:52:47 model_runner.py:183] )
INFO 10-04 03:52:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:52:47 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 7.7879, fwd=341.7981 cmp_logits=0.2332, sampling=79.7260, total=429.5475
INFO 10-04 03:52:47 worker.py:164] Peak: 13.264 GB, Initial: 38.980 GB, Free: 25.716 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.160 GB
INFO 10-04 03:52:47 gpu_executor.py:117] # GPU blocks: 3092, # CPU blocks: 8192
INFO 10-04 03:53:19 worker.py:189] _init_cache_engine took 24.1875 GB
INFO 10-04 03:53:19 scheduler.py:307] Scheduler initialized with prompt limit: 8192
INFO 10-04 03:53:19 Start warmup...
INFO 10-04 03:53:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:20 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7563, fwd=1893.7149 cmp_logits=0.0880, sampling=0.2553, total=1894.8171
INFO 10-04 03:53:20 metrics.py:335] Avg prompt throughput: 269.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 1902.3
INFO 10-04 03:53:20 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1895.2510 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:53:20 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:20 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:21 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 29.6333, fwd=13.6695 cmp_logits=38.2214, sampling=0.8831, total=82.4091
INFO 10-04 03:53:21 metrics.py:335] Avg prompt throughput: 6160.1 tokens/s, Avg generation throughput: 12.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 83.1
INFO 10-04 03:53:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 82.7878 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:53:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:21 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:53:21 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2866, fwd=42.9184 cmp_logits=0.1206, sampling=6.5544, total=49.8819
INFO 10-04 03:53:21 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 50.5
INFO 10-04 03:53:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.2872 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:53:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:21 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:53:21 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3154, fwd=8.0037 cmp_logits=0.0782, sampling=7.0627, total=15.4617
INFO 10-04 03:53:21 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 03:53:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7886 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:53:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:21 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:53:21 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2837, fwd=7.9496 cmp_logits=0.0725, sampling=7.1914, total=15.4986
INFO 10-04 03:53:21 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:53:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7542 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:53:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:21 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:53:21 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2759, fwd=7.8475 cmp_logits=0.0713, sampling=7.1888, total=15.3852
INFO 10-04 03:53:21 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:53:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6200 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:53:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:21 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:53:21 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2749, fwd=7.8542 cmp_logits=0.0737, sampling=7.1797, total=15.3842
INFO 10-04 03:53:21 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:53:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6133 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:53:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:21 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:53:21 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2685, fwd=7.8502 cmp_logits=0.0694, sampling=7.1781, total=15.3668
INFO 10-04 03:53:21 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:53:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6159 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:53:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:21 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:53:21 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2637, fwd=7.8199 cmp_logits=0.0668, sampling=7.2351, total=15.3863
INFO 10-04 03:53:21 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:53:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5983 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:53:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:21 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:53:21 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2627, fwd=7.8435 cmp_logits=0.0658, sampling=7.2298, total=15.4028
INFO 10-04 03:53:21 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:53:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5966 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:53:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:21 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:53:21 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2699, fwd=7.7896 cmp_logits=0.0679, sampling=7.2412, total=15.3694
INFO 10-04 03:53:21 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:53:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5854 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:53:26 Start benchmarking...
INFO 10-04 03:53:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:26 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7229, fwd=1615.1037 cmp_logits=0.1245, sampling=30.9463, total=1646.8983
INFO 10-04 03:53:27 metrics.py:335] Avg prompt throughput: 303.4 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%, Interval(ms): 1687.6
INFO 10-04 03:53:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1647.4180 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:53:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:27 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6614, fwd=14.4393 cmp_logits=0.1028, sampling=34.9696, total=50.1742
INFO 10-04 03:53:27 metrics.py:335] Avg prompt throughput: 10074.8 tokens/s, Avg generation throughput: 39.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 17 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%, Interval(ms): 50.7
INFO 10-04 03:53:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.5135 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:53:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6347, fwd=1626.6568 cmp_logits=0.1376, sampling=32.2146, total=1659.6446
INFO 10-04 03:53:29 metrics.py:335] Avg prompt throughput: 307.2 tokens/s, Avg generation throughput: 1.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%, Interval(ms): 1660.2
INFO 10-04 03:53:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1660.0902 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:53:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6850, fwd=14.4670 cmp_logits=0.0684, sampling=36.8645, total=52.0859
INFO 10-04 03:53:29 metrics.py:335] Avg prompt throughput: 9659.7 tokens/s, Avg generation throughput: 56.9 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%, Interval(ms): 52.7
INFO 10-04 03:53:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 52.4714 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:53:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6647, fwd=14.4753 cmp_logits=0.0837, sampling=31.0931, total=46.3173
INFO 10-04 03:53:29 metrics.py:335] Avg prompt throughput: 10867.0 tokens/s, Avg generation throughput: 85.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 46.8
INFO 10-04 03:53:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.6878 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:53:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6514, fwd=14.4506 cmp_logits=0.0687, sampling=25.6548, total=40.8263
INFO 10-04 03:53:29 metrics.py:335] Avg prompt throughput: 12294.3 tokens/s, Avg generation throughput: 96.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 41.3
INFO 10-04 03:53:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 41.1615 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:53:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6723, fwd=14.4353 cmp_logits=0.0832, sampling=30.6964, total=45.8882
INFO 10-04 03:53:29 metrics.py:335] Avg prompt throughput: 10939.7 tokens/s, Avg generation throughput: 107.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 7.8%, CPU KV cache usage: 0.0%, Interval(ms): 46.4
INFO 10-04 03:53:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.2825 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:53:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7145, fwd=14.4937 cmp_logits=0.0825, sampling=24.4431, total=39.7351
INFO 10-04 03:53:29 metrics.py:335] Avg prompt throughput: 12559.1 tokens/s, Avg generation throughput: 173.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 10.7%, CPU KV cache usage: 0.0%, Interval(ms): 40.4
INFO 10-04 03:53:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 40.2064 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:53:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6986, fwd=14.3893 cmp_logits=0.0775, sampling=26.0179, total=41.1842
INFO 10-04 03:53:29 metrics.py:335] Avg prompt throughput: 12076.5 tokens/s, Avg generation throughput: 167.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 10.3%, CPU KV cache usage: 0.0%, Interval(ms): 41.8
INFO 10-04 03:53:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 41.6486 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:53:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6716, fwd=14.4141 cmp_logits=0.0827, sampling=30.7283, total=45.8975
INFO 10-04 03:53:29 metrics.py:335] Avg prompt throughput: 10884.0 tokens/s, Avg generation throughput: 129.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 10.3%, CPU KV cache usage: 0.0%, Interval(ms): 46.5
INFO 10-04 03:53:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.2902 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:53:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7024, fwd=14.4424 cmp_logits=0.0710, sampling=34.5538, total=49.7708
INFO 10-04 03:53:29 metrics.py:335] Avg prompt throughput: 10039.9 tokens/s, Avg generation throughput: 138.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 50.4
INFO 10-04 03:53:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.2393 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:53:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7069, fwd=14.4935 cmp_logits=0.0689, sampling=29.7256, total=44.9958
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 11074.6 tokens/s, Avg generation throughput: 153.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 45.6
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.4330 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6971, fwd=14.5307 cmp_logits=0.0684, sampling=34.7250, total=50.0216
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 9976.8 tokens/s, Avg generation throughput: 138.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 50.6
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.4475 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7343, fwd=14.3602 cmp_logits=0.0820, sampling=31.9529, total=47.1306
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 10555.4 tokens/s, Avg generation throughput: 167.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 16.9%, CPU KV cache usage: 0.0%, Interval(ms): 47.8
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.6441 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7365, fwd=14.3976 cmp_logits=0.0691, sampling=28.3020, total=43.5061
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 11417.3 tokens/s, Avg generation throughput: 181.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 16.9%, CPU KV cache usage: 0.0%, Interval(ms): 44.1
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.9727 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7284, fwd=14.4050 cmp_logits=0.0701, sampling=32.5828, total=47.7874
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 10408.6 tokens/s, Avg generation throughput: 165.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 16.9%, CPU KV cache usage: 0.0%, Interval(ms): 48.4
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.2526 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7384, fwd=14.4207 cmp_logits=0.0834, sampling=31.5313, total=46.7749
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 10615.9 tokens/s, Avg generation throughput: 189.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 17.6%, CPU KV cache usage: 0.0%, Interval(ms): 47.5
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.2975 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7541, fwd=14.3435 cmp_logits=0.0825, sampling=28.1136, total=43.2947
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 11422.2 tokens/s, Avg generation throughput: 227.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 44.0
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.8609 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7548, fwd=14.4000 cmp_logits=0.0703, sampling=29.8152, total=45.0413
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 10995.5 tokens/s, Avg generation throughput: 196.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 19.4%, CPU KV cache usage: 0.0%, Interval(ms): 45.8
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.5804 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7608, fwd=14.4742 cmp_logits=0.0701, sampling=31.2707, total=46.5767
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 10629.3 tokens/s, Avg generation throughput: 211.3 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 21.4%, CPU KV cache usage: 0.0%, Interval(ms): 47.3
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.1449 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7484, fwd=14.4694 cmp_logits=0.0699, sampling=28.0583, total=43.3471
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 11391.2 tokens/s, Avg generation throughput: 226.9 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 21.4%, CPU KV cache usage: 0.0%, Interval(ms): 44.1
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.8871 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7727, fwd=14.4849 cmp_logits=0.0844, sampling=29.5315, total=44.8744
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 10997.4 tokens/s, Avg generation throughput: 241.0 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 21.2%, CPU KV cache usage: 0.0%, Interval(ms): 45.6
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.4695 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7703, fwd=14.4043 cmp_logits=0.0703, sampling=27.5116, total=42.7575
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 11526.0 tokens/s, Avg generation throughput: 252.6 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 22.4%, CPU KV cache usage: 0.0%, Interval(ms): 43.6
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.3414 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7823, fwd=14.4200 cmp_logits=0.0832, sampling=29.0897, total=44.3761
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 11074.5 tokens/s, Avg generation throughput: 265.3 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 23.5%, CPU KV cache usage: 0.0%, Interval(ms): 45.2
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.0203 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8397, fwd=14.4424 cmp_logits=0.0837, sampling=28.0385, total=43.4055
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 11289.9 tokens/s, Avg generation throughput: 316.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 26.0%, CPU KV cache usage: 0.0%, Interval(ms): 44.3
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.0977 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8171, fwd=14.3225 cmp_logits=0.0715, sampling=29.1653, total=44.3773
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 11013.8 tokens/s, Avg generation throughput: 309.6 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 26.1%, CPU KV cache usage: 0.0%, Interval(ms): 45.2
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.0201 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8414, fwd=14.3735 cmp_logits=0.0834, sampling=33.9234, total=49.2225
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 9934.7 tokens/s, Avg generation throughput: 299.2 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 50.1
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.9332 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8221, fwd=14.4839 cmp_logits=0.0715, sampling=28.8806, total=44.2591
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 11011.4 tokens/s, Avg generation throughput: 332.3 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 45.1
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.9355 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8254, fwd=14.3652 cmp_logits=0.0713, sampling=33.1674, total=48.4297
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 10069.6 tokens/s, Avg generation throughput: 303.9 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 49.4
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.1116 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([492]), positions.shape=torch.Size([492]) hidden_states.shape=torch.Size([492, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8228, fwd=14.4389 cmp_logits=0.0837, sampling=37.9300, total=53.2763
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 8805.1 tokens/s, Avg generation throughput: 295.3 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 54.2
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.9770 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5221, fwd=7.7171 cmp_logits=0.0730, sampling=10.9160, total=19.2292
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 795.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9249 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4940, fwd=7.7555 cmp_logits=0.0684, sampling=10.8995, total=19.2192
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 745.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8889 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4747, fwd=7.8297 cmp_logits=0.0687, sampling=10.8194, total=19.1939
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 698.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8390 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4778, fwd=7.6909 cmp_logits=0.0694, sampling=10.9444, total=19.1832
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 697.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8388 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4740, fwd=7.6814 cmp_logits=0.0682, sampling=10.9477, total=19.1720
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 700.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.0
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8076 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 03:53:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4759, fwd=7.7326 cmp_logits=0.0679, sampling=10.8912, total=19.1684
INFO 10-04 03:53:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 699.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.0
INFO 10-04 03:53:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8109 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:53:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4756, fwd=7.7150 cmp_logits=0.0684, sampling=10.9203, total=19.1803
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 699.3 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.0
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8336 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4604, fwd=7.7617 cmp_logits=0.0796, sampling=10.0696, total=18.3723
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 675.2 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.3%, CPU KV cache usage: 0.0%, Interval(ms): 19.3
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.0365 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4523, fwd=7.6663 cmp_logits=0.0679, sampling=10.1771, total=18.3644
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 626.4 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.3%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9416 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4482, fwd=7.6244 cmp_logits=0.0677, sampling=10.1895, total=18.3306
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 628.7 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.1
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9075 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4461, fwd=7.6885 cmp_logits=0.0675, sampling=10.1540, total=18.3570
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 627.4 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.1
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9443 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4427, fwd=7.6835 cmp_logits=0.0682, sampling=10.1421, total=18.3377
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 628.4 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.5%, CPU KV cache usage: 0.0%, Interval(ms): 19.1
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9140 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4542, fwd=7.7105 cmp_logits=0.0756, sampling=10.1216, total=18.3623
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 626.1 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9860 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4287, fwd=7.7350 cmp_logits=0.0679, sampling=9.7833, total=18.0154
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 585.9 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.8
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5637 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4332, fwd=7.6888 cmp_logits=0.0684, sampling=9.8062, total=17.9975
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 587.3 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5502 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4344, fwd=7.7198 cmp_logits=0.0672, sampling=9.7737, total=17.9963
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 587.4 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5480 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4277, fwd=7.6511 cmp_logits=0.0675, sampling=9.8376, total=17.9846
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 588.0 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5304 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4323, fwd=7.7426 cmp_logits=0.0682, sampling=9.7377, total=17.9813
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 586.7 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.8
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5735 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4299, fwd=7.6549 cmp_logits=0.0682, sampling=9.8896, total=18.0433
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 585.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.8
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.6012 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4156, fwd=7.7391 cmp_logits=0.0670, sampling=9.6135, total=17.8359
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 538.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3570 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4182, fwd=7.6694 cmp_logits=0.0665, sampling=9.6707, total=17.8256
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 539.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3461 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4179, fwd=7.6516 cmp_logits=0.0672, sampling=9.6943, total=17.8320
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 539.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3489 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4144, fwd=7.6787 cmp_logits=0.0679, sampling=9.6555, total=17.8177
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 538.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3856 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4008, fwd=7.7183 cmp_logits=0.0670, sampling=9.3517, total=17.5388
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0242 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4084, fwd=7.7095 cmp_logits=0.0670, sampling=9.3911, total=17.5769
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0538 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3994, fwd=7.6885 cmp_logits=0.0660, sampling=9.4218, total=17.5767
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0609 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3986, fwd=7.6880 cmp_logits=0.0670, sampling=9.4018, total=17.5562
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0349 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3994, fwd=7.6966 cmp_logits=0.0668, sampling=9.4025, total=17.5660
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0748 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3998, fwd=7.7879 cmp_logits=0.0925, sampling=9.3036, total=17.5846
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 492.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1041 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4072, fwd=7.7558 cmp_logits=0.0670, sampling=9.3296, total=17.5610
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0447 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4001, fwd=7.6880 cmp_logits=0.0665, sampling=9.3889, total=17.5445
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0197 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3984, fwd=7.6954 cmp_logits=0.0665, sampling=9.3904, total=17.5514
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0459 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3846, fwd=7.7424 cmp_logits=0.0668, sampling=9.0637, total=17.2584
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 445.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7336 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3622, fwd=7.7932 cmp_logits=0.0665, sampling=8.1894, total=16.4125
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 352.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7997 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3526, fwd=7.6535 cmp_logits=0.0658, sampling=8.2893, total=16.3617
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 354.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7508 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3510, fwd=7.7183 cmp_logits=0.0658, sampling=8.2829, total=16.4189
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 353.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8021 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3541, fwd=7.6888 cmp_logits=0.0658, sampling=8.3399, total=16.4497
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 353.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8333 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3519, fwd=7.6761 cmp_logits=0.0651, sampling=8.3742, total=16.4683
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 352.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8655 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3505, fwd=7.6022 cmp_logits=0.0660, sampling=8.3992, total=16.4187
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 353.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8197 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3386, fwd=7.7834 cmp_logits=0.0651, sampling=7.9689, total=16.1572
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5169 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3350, fwd=7.7915 cmp_logits=0.0665, sampling=7.9720, total=16.1660
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.5 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5429 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3185, fwd=7.7267 cmp_logits=0.0648, sampling=7.6988, total=15.8100
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.1 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1304 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3181, fwd=7.7474 cmp_logits=0.0656, sampling=7.6857, total=15.8176
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.7 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1350 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3211, fwd=7.6656 cmp_logits=0.0653, sampling=7.7441, total=15.7974
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 246.0 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1138 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3183, fwd=7.6878 cmp_logits=0.0648, sampling=7.7243, total=15.7957
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.9 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1145 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3283, fwd=7.6253 cmp_logits=0.0658, sampling=7.7789, total=15.7995
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.9 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1154 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3176, fwd=7.6690 cmp_logits=0.0653, sampling=7.7395, total=15.7921
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.9 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1123 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3169, fwd=7.7002 cmp_logits=0.0651, sampling=7.7395, total=15.8226
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.5 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1409 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3181, fwd=7.6652 cmp_logits=0.0665, sampling=7.7579, total=15.8088
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1226 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3211, fwd=7.6358 cmp_logits=0.0656, sampling=7.7858, total=15.8088
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1266 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3171, fwd=7.6754 cmp_logits=0.0660, sampling=7.7403, total=15.7998
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.9 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1183 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3192, fwd=7.6461 cmp_logits=0.0663, sampling=7.9925, total=16.0248
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 242.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.5
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.3605 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3040, fwd=7.7655 cmp_logits=0.0656, sampling=7.2591, total=15.3954
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6817 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3045, fwd=7.6468 cmp_logits=0.0639, sampling=7.3357, total=15.3518
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6372 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3047, fwd=7.6876 cmp_logits=0.0648, sampling=7.2978, total=15.3558
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6398 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3011, fwd=7.6361 cmp_logits=0.0658, sampling=7.3419, total=15.3458
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6298 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3026, fwd=7.6962 cmp_logits=0.0651, sampling=7.2908, total=15.3556
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6400 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3047, fwd=7.6687 cmp_logits=0.0639, sampling=7.3631, total=15.4014
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6815 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3021, fwd=7.6976 cmp_logits=0.0663, sampling=7.2951, total=15.3620
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6686 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2837, fwd=7.7307 cmp_logits=0.0651, sampling=7.1023, total=15.1825
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4355 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2840, fwd=7.6754 cmp_logits=0.0653, sampling=7.1390, total=15.1646
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4080 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2978, fwd=7.6535 cmp_logits=0.0646, sampling=7.1740, total=15.1908
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4357 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2835, fwd=7.7069 cmp_logits=0.0653, sampling=7.1609, total=15.2175
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4631 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:53:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2830, fwd=7.6554 cmp_logits=0.0646, sampling=7.1564, total=15.1603
INFO 10-04 03:53:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-04 03:53:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4040 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:53:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:53:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2851, fwd=7.6640 cmp_logits=0.0653, sampling=7.1621, total=15.1775
INFO 10-04 03:53:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:53:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4214 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:53:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:53:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2835, fwd=7.6346 cmp_logits=0.0651, sampling=7.1890, total=15.1730
INFO 10-04 03:53:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:53:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4200 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:53:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:53:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2840, fwd=7.6494 cmp_logits=0.0656, sampling=7.1766, total=15.1763
INFO 10-04 03:53:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:53:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4240 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:53:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:53:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2840, fwd=7.6597 cmp_logits=0.0651, sampling=7.2498, total=15.2593
INFO 10-04 03:53:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:53:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5025 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:53:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:53:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2830, fwd=7.6811 cmp_logits=0.0660, sampling=7.1907, total=15.2221
INFO 10-04 03:53:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:53:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4676 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:53:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:53:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2816, fwd=7.6272 cmp_logits=0.0648, sampling=7.2470, total=15.2214
INFO 10-04 03:53:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:53:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4650 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:53:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:53:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2818, fwd=7.6904 cmp_logits=0.0651, sampling=7.1812, total=15.2194
INFO 10-04 03:53:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:53:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4662 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:53:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:53:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2904, fwd=7.6051 cmp_logits=0.0648, sampling=7.2789, total=15.2400
INFO 10-04 03:53:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:53:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5144 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:53:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:53:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2611, fwd=7.9024 cmp_logits=0.0660, sampling=6.7525, total=14.9829
INFO 10-04 03:53:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.4
INFO 10-04 03:53:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.1815 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:53:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 03:53:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:53:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2577, fwd=7.7758 cmp_logits=0.0658, sampling=6.8402, total=14.9405
INFO 10-04 03:53:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.3
INFO 10-04 03:53:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.1572 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.226073, last_token_time=1728014009.8840294, first_scheduled_time=1728014006.256414, first_token_time=1728014007.9034674, time_in_queue=0.030340909957885742, finished_time=1728014009.884002, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2270715, last_token_time=1728014011.009399, first_scheduled_time=1728014006.256414, first_token_time=1728014007.954212, time_in_queue=0.0293424129486084, finished_time=1728014011.0093431, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2289543, last_token_time=1728014010.491932, first_scheduled_time=1728014007.9039266, first_token_time=1728014009.6143746, time_in_queue=1.6749722957611084, finished_time=1728014010.4918976, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2297204, last_token_time=1728014011.474954, first_scheduled_time=1728014007.954576, first_token_time=1728014009.7139423, time_in_queue=1.724855661392212, finished_time=1728014011.4749165, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2319024, last_token_time=1728014011.4928944, first_scheduled_time=1728014009.6675136, first_token_time=1728014009.8016677, time_in_queue=3.4356112480163574, finished_time=1728014011.4928622, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2341564, last_token_time=1728014010.3090568, first_scheduled_time=1728014009.7556663, first_token_time=1728014009.8420017, time_in_queue=3.521509885787964, finished_time=1728014010.309039, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2346141, last_token_time=1728014010.3090568, first_scheduled_time=1728014009.8021448, first_token_time=1728014009.8420017, time_in_queue=3.567530632019043, finished_time=1728014010.309043, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2350636, last_token_time=1728014011.62818, first_scheduled_time=1728014009.8021448, first_token_time=1728014009.9807107, time_in_queue=3.5670812129974365, finished_time=1728014011.6281579, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2373025, last_token_time=1728014012.1208217, first_scheduled_time=1728014009.9308152, first_token_time=1728014010.1247416, time_in_queue=3.6935126781463623, finished_time=1728014012.1208138, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2407904, last_token_time=1728014011.9181085, first_scheduled_time=1728014010.0774784, first_token_time=1728014010.2647614, time_in_queue=3.8366880416870117, finished_time=1728014011.9181015, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2435696, last_token_time=1728014010.909254, first_scheduled_time=1728014010.2178504, first_token_time=1728014010.3087704, time_in_queue=3.974280834197998, finished_time=1728014010.9092205, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2441294, last_token_time=1728014011.1242867, first_scheduled_time=1728014010.2653356, first_token_time=1728014010.3546379, time_in_queue=4.0212061405181885, finished_time=1728014011.1242611, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2450101, last_token_time=1728014011.4928944, first_scheduled_time=1728014010.3094573, first_token_time=1728014010.4019296, time_in_queue=4.0644471645355225, finished_time=1728014011.4928806, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2465773, last_token_time=1728014011.5947752, first_scheduled_time=1728014010.355209, first_token_time=1728014010.4916236, time_in_queue=4.10863184928894, finished_time=1728014011.594765, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.248268, last_token_time=1728014011.0286539, first_scheduled_time=1728014010.446603, first_token_time=1728014010.53519, time_in_queue=4.19833517074585, finished_time=1728014011.0286357, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.248836, last_token_time=1728014011.310915, first_scheduled_time=1728014010.4922872, first_token_time=1728014010.5803816, time_in_queue=4.243451118469238, finished_time=1728014011.310905, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2498884, last_token_time=1728014011.236751, first_scheduled_time=1728014010.5358534, first_token_time=1728014010.624649, time_in_queue=4.2859649658203125, finished_time=1728014011.2367413, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2508135, last_token_time=1728014010.889145, first_scheduled_time=1728014010.5810843, first_token_time=1728014010.624649, time_in_queue=4.330270767211914, finished_time=1728014010.8891351, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2510855, last_token_time=1728014011.8074653, first_scheduled_time=1728014010.5810843, first_token_time=1728014010.7199728, time_in_queue=4.329998731613159, finished_time=1728014011.8074589, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728014006.2532105, last_token_time=1728014012.1514819, first_scheduled_time=1728014010.6705859, first_token_time=1728014010.8686228, time_in_queue=4.417375326156616, finished_time=1728014012.1514792, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 15868
End-to-End latency: 5.93 seconds
Throughput: 3.38 requests/s, 2677.87 tokens/s
Per_token_time: 0.373 ms

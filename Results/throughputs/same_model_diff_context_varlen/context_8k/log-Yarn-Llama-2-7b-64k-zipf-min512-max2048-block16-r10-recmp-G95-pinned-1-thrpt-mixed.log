Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=8192, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=8192, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=True)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Selected required requests 10
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Mixed batch is enabled. max_num_batched_tokens=8192, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'finetuned', 'type', 'original_max_position_embeddings'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 03:19:22 config.py:654] [SchedulerConfig] max_num_batched_tokens: 8192 chunked_prefill_enabled: False
INFO 10-04 03:19:22 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=True)
INFO 10-04 03:19:23 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 03:19:28 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 03:19:31 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 03:19:31 model_runner.py:183] Loaded model: 
INFO 10-04 03:19:31 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 03:19:31 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 03:19:31 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:19:31 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 03:19:31 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 03:19:31 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 03:19:31 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:19:31 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:19:31 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 03:19:31 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 03:19:31 model_runner.py:183]         )
INFO 10-04 03:19:31 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 03:19:31 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:19:31 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:19:31 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 03:19:31 model_runner.py:183]         )
INFO 10-04 03:19:31 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:19:31 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:19:31 model_runner.py:183]       )
INFO 10-04 03:19:31 model_runner.py:183]     )
INFO 10-04 03:19:31 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:19:31 model_runner.py:183]   )
INFO 10-04 03:19:31 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:19:31 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 03:19:31 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 03:19:31 model_runner.py:183] )
INFO 10-04 03:19:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8192]), positions.shape=torch.Size([8192]) hidden_states.shape=torch.Size([8192, 4096]) residual=None
INFO 10-04 03:19:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 4.1158, fwd=339.2892 cmp_logits=0.2382, sampling=457.0799, total=800.7250
INFO 10-04 03:19:32 worker.py:164] Peak: 13.771 GB, Initial: 38.980 GB, Free: 25.208 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 23.653 GB
INFO 10-04 03:19:32 gpu_executor.py:117] # GPU blocks: 3027, # CPU blocks: 8192
INFO 10-04 03:20:04 worker.py:189] _init_cache_engine took 23.6484 GB
INFO 10-04 03:20:04 scheduler.py:307] Scheduler initialized with prompt limit: 8192
INFO 10-04 03:20:04 Start warmup...
INFO 10-04 03:20:04 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:04 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:20:04 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8976, fwd=11.9474 cmp_logits=72.5040, sampling=0.9818, total=86.3335
INFO 10-04 03:20:04 metrics.py:335] Avg prompt throughput: 10976.7 tokens/s, Avg generation throughput: 10.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 93.3
INFO 10-04 03:20:04 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.7922 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:04 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:04 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:04 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2933, fwd=44.9593 cmp_logits=0.1183, sampling=6.3546, total=51.7275
INFO 10-04 03:20:04 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 52.7
INFO 10-04 03:20:04 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 52.1746 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:04 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:04 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:04 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3192, fwd=8.3463 cmp_logits=0.0801, sampling=6.5274, total=15.2750
INFO 10-04 03:20:04 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:20:04 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6026 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:04 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:04 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:04 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2854, fwd=8.0497 cmp_logits=0.0703, sampling=6.7966, total=15.2037
INFO 10-04 03:20:04 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:20:04 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4638 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:04 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:04 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:04 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2737, fwd=7.9052 cmp_logits=0.0699, sampling=6.9363, total=15.1870
INFO 10-04 03:20:04 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:20:04 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4223 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:04 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:04 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:04 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2706, fwd=8.0945 cmp_logits=0.0730, sampling=6.7458, total=15.1856
INFO 10-04 03:20:04 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:20:04 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4192 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:04 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:04 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:04 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3066, fwd=8.7323 cmp_logits=0.0744, sampling=6.1224, total=15.2376
INFO 10-04 03:20:04 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:20:04 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4858 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:04 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:04 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:04 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2682, fwd=8.4515 cmp_logits=0.0682, sampling=6.3813, total=15.1701
INFO 10-04 03:20:04 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 03:20:04 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4223 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:04 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:04 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:04 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2751, fwd=8.3988 cmp_logits=0.0691, sampling=6.4430, total=15.1870
INFO 10-04 03:20:04 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-04 03:20:04 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3930 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:04 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:04 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:04 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2744, fwd=7.9794 cmp_logits=0.0694, sampling=6.8486, total=15.1730
INFO 10-04 03:20:04 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-04 03:20:04 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3956 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:09 Start benchmarking...
INFO 10-04 03:20:09 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:09 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7293]), positions.shape=torch.Size([7293]) hidden_states.shape=torch.Size([7293, 4096]) residual=None
INFO 10-04 03:20:09 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 2.5623, fwd=10.7996 cmp_logits=0.1130, sampling=459.7569, total=473.2330
INFO 10-04 03:20:09 metrics.py:335] Avg prompt throughput: 14437.3 tokens/s, Avg generation throughput: 15.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 15.2%, CPU KV cache usage: 0.0%, Interval(ms): 505.1
INFO 10-04 03:20:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 474.0763 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:20:09 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:09 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3373]), positions.shape=torch.Size([3373]) hidden_states.shape=torch.Size([3373, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.3909, fwd=10.1836 cmp_logits=0.0818, sampling=193.0015, total=204.6585
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 16375.2 tokens/s, Avg generation throughput: 48.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 205.5
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 205.2314 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4179, fwd=7.7322 cmp_logits=0.0689, sampling=9.3975, total=17.6175
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0881 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4189, fwd=7.6115 cmp_logits=0.0668, sampling=9.4709, total=17.5688
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0335 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4082, fwd=7.6578 cmp_logits=0.0682, sampling=9.4635, total=17.5986
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0707 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4041, fwd=7.6745 cmp_logits=0.0672, sampling=9.4240, total=17.5707
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0697 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4089, fwd=7.6549 cmp_logits=0.0687, sampling=9.4523, total=17.5855
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0600 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4103, fwd=7.7257 cmp_logits=0.0684, sampling=9.3305, total=17.5362
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0068 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4032, fwd=7.6225 cmp_logits=0.0656, sampling=9.4345, total=17.5266
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9977 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4103, fwd=7.7031 cmp_logits=0.0677, sampling=9.3405, total=17.5223
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9944 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4048, fwd=7.6475 cmp_logits=0.0670, sampling=9.4385, total=17.5588
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0364 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4032, fwd=7.6060 cmp_logits=0.0663, sampling=9.4938, total=17.5705
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0452 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4151, fwd=7.5953 cmp_logits=0.0672, sampling=9.4652, total=17.5438
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0140 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4041, fwd=7.6721 cmp_logits=0.0670, sampling=9.4213, total=17.5657
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0411 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4079, fwd=7.6690 cmp_logits=0.0675, sampling=9.3884, total=17.5340
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0151 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4058, fwd=7.5834 cmp_logits=0.0663, sampling=9.4779, total=17.5345
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0070 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4063, fwd=7.6809 cmp_logits=0.0672, sampling=9.3830, total=17.5383
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0156 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4051, fwd=7.6592 cmp_logits=0.0670, sampling=9.4240, total=17.5560
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0271 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4089, fwd=7.5998 cmp_logits=0.0665, sampling=9.5270, total=17.6034
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0750 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4070, fwd=7.6022 cmp_logits=0.0670, sampling=9.4817, total=17.5586
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0359 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4063, fwd=7.6463 cmp_logits=0.0679, sampling=9.4297, total=17.5509
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0237 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4086, fwd=7.6463 cmp_logits=0.0677, sampling=9.4836, total=17.6072
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 546.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0817 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4044, fwd=7.6289 cmp_logits=0.0677, sampling=9.4690, total=17.5710
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0430 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4127, fwd=7.6091 cmp_logits=0.0699, sampling=9.4969, total=17.5898
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0619 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4120, fwd=7.6160 cmp_logits=0.0670, sampling=9.4697, total=17.5657
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0564 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4051, fwd=7.6013 cmp_logits=0.0675, sampling=9.5046, total=17.5796
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0874 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4110, fwd=7.6013 cmp_logits=0.0670, sampling=9.5074, total=17.5874
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0795 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3896, fwd=7.6363 cmp_logits=0.0789, sampling=9.1965, total=17.3023
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7481 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3963, fwd=7.6113 cmp_logits=0.0660, sampling=9.2392, total=17.3140
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 500.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7972 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3645, fwd=7.6680 cmp_logits=0.0777, sampling=8.7128, total=16.8242
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2195 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3605, fwd=7.6241 cmp_logits=0.0660, sampling=8.7118, total=16.7634
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1893 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3726, fwd=7.6578 cmp_logits=0.0670, sampling=8.6758, total=16.7747
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1683 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3598, fwd=7.6892 cmp_logits=0.0660, sampling=8.6668, total=16.7832
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1740 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3631, fwd=7.6699 cmp_logits=0.0660, sampling=8.7681, total=16.8684
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2589 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3695, fwd=7.6561 cmp_logits=0.0665, sampling=8.7767, total=16.8698
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2634 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3593, fwd=7.6010 cmp_logits=0.0658, sampling=8.9023, total=16.9296
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3647 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3483, fwd=7.7040 cmp_logits=0.0775, sampling=8.5230, total=16.6535
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0150 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3557, fwd=7.6382 cmp_logits=0.0663, sampling=8.6021, total=16.6631
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0264 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3457, fwd=7.6420 cmp_logits=0.0670, sampling=8.5905, total=16.6461
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0066 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3479, fwd=7.6149 cmp_logits=0.0663, sampling=8.6193, total=16.6492
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0093 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3488, fwd=7.6733 cmp_logits=0.0665, sampling=8.5623, total=16.6521
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0476 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3462, fwd=7.7274 cmp_logits=0.0677, sampling=8.4970, total=16.6397
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0031 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3474, fwd=7.7047 cmp_logits=0.0663, sampling=8.5173, total=16.6368
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9978 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3445, fwd=7.6413 cmp_logits=0.0665, sampling=8.5735, total=16.6268
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9866 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3464, fwd=7.6497 cmp_logits=0.0665, sampling=8.5833, total=16.6469
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0162 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3488, fwd=7.6277 cmp_logits=0.0663, sampling=8.5845, total=16.6283
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0267 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3488, fwd=7.7126 cmp_logits=0.0660, sampling=8.5313, total=16.6597
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0262 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3505, fwd=7.6666 cmp_logits=0.0653, sampling=8.5852, total=16.6686
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0257 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3455, fwd=7.6344 cmp_logits=0.0660, sampling=8.6031, total=16.6500
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0312 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3350, fwd=7.6962 cmp_logits=0.0780, sampling=8.1632, total=16.2733
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6106 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3350, fwd=7.6795 cmp_logits=0.0660, sampling=8.1646, total=16.2463
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6142 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3362, fwd=7.6251 cmp_logits=0.0660, sampling=8.2483, total=16.2766
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6178 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3359, fwd=7.6635 cmp_logits=0.0658, sampling=8.1911, total=16.2573
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5944 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3378, fwd=7.6723 cmp_logits=0.0675, sampling=8.1859, total=16.2647
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6037 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3350, fwd=7.6556 cmp_logits=0.0663, sampling=8.2157, total=16.2735
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6101 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:10 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3395, fwd=7.6470 cmp_logits=0.0665, sampling=8.1851, total=16.2389
INFO 10-04 03:20:10 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:20:10 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6042 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:10 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:10 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3345, fwd=7.6759 cmp_logits=0.0653, sampling=8.1851, total=16.2616
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5982 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3347, fwd=7.6632 cmp_logits=0.0658, sampling=8.2054, total=16.2702
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6099 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3312, fwd=7.6940 cmp_logits=0.0648, sampling=8.1539, total=16.2454
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5780 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3307, fwd=7.6497 cmp_logits=0.0663, sampling=8.2035, total=16.2511
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5825 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3307, fwd=7.6699 cmp_logits=0.0663, sampling=8.1909, total=16.2585
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6295 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3369, fwd=7.7159 cmp_logits=0.0660, sampling=8.1618, total=16.2814
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6166 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3378, fwd=7.6427 cmp_logits=0.0660, sampling=8.2092, total=16.2568
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5906 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3352, fwd=7.6506 cmp_logits=0.0665, sampling=8.2219, total=16.2752
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6097 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3304, fwd=7.6396 cmp_logits=0.0658, sampling=8.2324, total=16.2692
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6028 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3345, fwd=7.6470 cmp_logits=0.0658, sampling=8.2111, total=16.2594
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.2%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6690 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2987, fwd=7.6823 cmp_logits=0.0787, sampling=7.3013, total=15.3620
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6329 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2978, fwd=7.6180 cmp_logits=0.0656, sampling=7.3776, total=15.3596
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6462 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2832, fwd=7.7453 cmp_logits=0.0761, sampling=7.2148, total=15.3203
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5544 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2804, fwd=7.5893 cmp_logits=0.0653, sampling=7.3507, total=15.2867
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5177 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2785, fwd=7.6272 cmp_logits=0.0663, sampling=7.3171, total=15.2900
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5544 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2828, fwd=7.6411 cmp_logits=0.0648, sampling=7.3180, total=15.3077
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5385 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2792, fwd=7.5941 cmp_logits=0.0653, sampling=7.3593, total=15.2986
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5282 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2825, fwd=7.6318 cmp_logits=0.0660, sampling=7.3185, total=15.2998
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5392 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2792, fwd=7.5829 cmp_logits=0.0663, sampling=7.3645, total=15.2938
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5296 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2789, fwd=7.5715 cmp_logits=0.0648, sampling=7.3612, total=15.2776
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5439 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2813, fwd=7.5679 cmp_logits=0.0656, sampling=7.3740, total=15.2895
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5249 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2813, fwd=7.6032 cmp_logits=0.0656, sampling=7.3550, total=15.3062
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5404 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2811, fwd=7.6029 cmp_logits=0.0651, sampling=7.3524, total=15.3027
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5573 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2587, fwd=7.7622 cmp_logits=0.0658, sampling=7.3769, total=15.4643
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6567 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2589, fwd=7.7538 cmp_logits=0.0658, sampling=7.3509, total=15.4307
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6589 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2608, fwd=7.7746 cmp_logits=0.0660, sampling=7.3419, total=15.4443
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6353 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2596, fwd=7.7767 cmp_logits=0.0663, sampling=7.3376, total=15.4414
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6310 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2601, fwd=7.8228 cmp_logits=0.0658, sampling=7.3128, total=15.4622
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6543 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2599, fwd=7.7555 cmp_logits=0.0668, sampling=7.3419, total=15.4250
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6150 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2596, fwd=7.7648 cmp_logits=0.0658, sampling=7.3521, total=15.4433
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6662 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2620, fwd=7.7691 cmp_logits=0.0653, sampling=7.3550, total=15.4524
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6422 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2618, fwd=7.7658 cmp_logits=0.0656, sampling=7.3802, total=15.4743
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6643 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2596, fwd=7.7929 cmp_logits=0.0658, sampling=7.3268, total=15.4462
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6360 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2604, fwd=7.7693 cmp_logits=0.0660, sampling=7.3550, total=15.4524
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6412 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2584, fwd=7.7639 cmp_logits=0.0670, sampling=7.3667, total=15.4567
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6794 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:20:11 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=8192, max_num_seqs=256
INFO 10-04 03:20:11 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:20:11 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2601, fwd=7.8576 cmp_logits=0.0665, sampling=7.2684, total=15.4533
INFO 10-04 03:20:11 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:20:11 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6820 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012009.3366385, last_token_time=1728012010.4927108, first_scheduled_time=1728012009.3579943, first_token_time=1728012009.831403, time_in_queue=0.021355867385864258, finished_time=1728012010.4926717, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012009.3384337, last_token_time=1728012010.8737538, first_scheduled_time=1728012009.3579943, first_token_time=1728012009.831403, time_in_queue=0.019560575485229492, finished_time=1728012010.873727, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012009.3403254, last_token_time=1728012010.650451, first_scheduled_time=1728012009.3579943, first_token_time=1728012009.831403, time_in_queue=0.017668962478637695, finished_time=1728012010.650428, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012009.3415842, last_token_time=1728012011.1587574, first_scheduled_time=1728012009.3579943, first_token_time=1728012009.831403, time_in_queue=0.016410112380981445, finished_time=1728012011.1587362, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012009.3440738, last_token_time=1728012011.1587574, first_scheduled_time=1728012009.3579943, first_token_time=1728012009.831403, time_in_queue=0.01392054557800293, finished_time=1728012011.1587424, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012009.346549, last_token_time=1728012010.528631, first_scheduled_time=1728012009.3579943, first_token_time=1728012009.831403, time_in_queue=0.011445283889770508, finished_time=1728012010.5286112, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012009.3475807, last_token_time=1728012010.528631, first_scheduled_time=1728012009.3579943, first_token_time=1728012009.831403, time_in_queue=0.010413646697998047, finished_time=1728012010.5286155, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012009.3486338, last_token_time=1728012011.190401, first_scheduled_time=1728012009.3579943, first_token_time=1728012009.831403, time_in_queue=0.009360551834106445, finished_time=1728012011.1903884, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012009.3511555, last_token_time=1728012011.5681548, first_scheduled_time=1728012009.8321176, first_token_time=1728012010.036918, time_in_queue=0.48096203804016113, finished_time=1728012011.568152, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012009.354666, last_token_time=1728012011.3629072, first_scheduled_time=1728012009.8321176, first_token_time=1728012010.036918, time_in_queue=0.4774515628814697, finished_time=1728012011.3629053, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 11197
End-to-End latency: 2.23 seconds
Throughput: 4.48 requests/s, 5017.20 tokens/s
Per_token_time: 0.199 ms

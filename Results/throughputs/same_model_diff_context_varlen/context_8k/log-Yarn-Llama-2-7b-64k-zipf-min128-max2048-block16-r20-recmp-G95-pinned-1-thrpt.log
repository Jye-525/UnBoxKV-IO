Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=128, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=8192, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=8192, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Selected required requests 20
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Running vLLM with default batching strategy. max_num_batched_tokens=8192, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'finetuned', 'type', 'original_max_position_embeddings'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 03:32:08 config.py:654] [SchedulerConfig] max_num_batched_tokens: 8192 chunked_prefill_enabled: False
INFO 10-04 03:32:08 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 03:32:09 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 03:32:13 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 03:32:17 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 03:32:17 model_runner.py:183] Loaded model: 
INFO 10-04 03:32:17 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 03:32:17 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 03:32:17 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:32:17 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 03:32:17 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 03:32:17 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 03:32:17 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:32:17 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:32:17 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 03:32:17 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 03:32:17 model_runner.py:183]         )
INFO 10-04 03:32:17 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 03:32:17 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 03:32:17 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 03:32:17 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 03:32:17 model_runner.py:183]         )
INFO 10-04 03:32:17 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:32:17 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:32:17 model_runner.py:183]       )
INFO 10-04 03:32:17 model_runner.py:183]     )
INFO 10-04 03:32:17 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 03:32:17 model_runner.py:183]   )
INFO 10-04 03:32:17 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 03:32:17 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 03:32:17 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 03:32:17 model_runner.py:183] )
INFO 10-04 03:32:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8192]), positions.shape=torch.Size([8192]) hidden_states.shape=torch.Size([8192, 4096]) residual=None
INFO 10-04 03:32:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 24.0333, fwd=355.6643 cmp_logits=0.2449, sampling=455.6420, total=835.5865
INFO 10-04 03:32:18 worker.py:164] Peak: 13.771 GB, Initial: 38.980 GB, Free: 25.208 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 23.653 GB
INFO 10-04 03:32:18 gpu_executor.py:117] # GPU blocks: 3027, # CPU blocks: 8192
INFO 10-04 03:32:49 worker.py:189] _init_cache_engine took 23.6484 GB
INFO 10-04 03:32:49 scheduler.py:307] Scheduler initialized with prompt limit: 8192
INFO 10-04 03:32:49 Start warmup...
INFO 10-04 03:32:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 03:32:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8581, fwd=11.5874 cmp_logits=72.8838, sampling=0.9475, total=86.2789
INFO 10-04 03:32:49 metrics.py:335] Avg prompt throughput: 11014.1 tokens/s, Avg generation throughput: 10.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 93.0
INFO 10-04 03:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.6845 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2918, fwd=41.9726 cmp_logits=0.1149, sampling=6.6507, total=49.0320
INFO 10-04 03:32:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 49.8
INFO 10-04 03:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.4287 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3047, fwd=8.1398 cmp_logits=0.0706, sampling=7.0381, total=15.5547
INFO 10-04 03:32:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 03:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8179 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2768, fwd=8.1217 cmp_logits=0.0703, sampling=7.0429, total=15.5129
INFO 10-04 03:32:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7182 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2701, fwd=8.2145 cmp_logits=0.0730, sampling=6.9716, total=15.5303
INFO 10-04 03:32:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7404 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2792, fwd=8.2784 cmp_logits=0.0732, sampling=6.8862, total=15.5184
INFO 10-04 03:32:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7225 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2832, fwd=8.3401 cmp_logits=0.0772, sampling=6.8617, total=15.5635
INFO 10-04 03:32:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7802 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2742, fwd=8.2614 cmp_logits=0.0708, sampling=5.8239, total=14.4310
INFO 10-04 03:32:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.8
INFO 10-04 03:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.6580 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2887, fwd=8.2331 cmp_logits=0.0699, sampling=5.7733, total=14.3661
INFO 10-04 03:32:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5562 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2778, fwd=8.1406 cmp_logits=0.0699, sampling=5.8439, total=14.3330
INFO 10-04 03:32:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 03:32:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5435 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:54 Start benchmarking...
INFO 10-04 03:32:54 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6970]), positions.shape=torch.Size([6970]) hidden_states.shape=torch.Size([6970, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 2.5327, fwd=10.7980 cmp_logits=0.1109, sampling=430.7895, total=444.2322
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 14370.3 tokens/s, Avg generation throughput: 18.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 14.5%, CPU KV cache usage: 0.0%, Interval(ms): 485.0
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 444.9594 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8132]), positions.shape=torch.Size([8132]) hidden_states.shape=torch.Size([8132, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 2.5599, fwd=9.7878 cmp_logits=0.0854, sampling=456.9058, total=469.3401
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 17288.9 tokens/s, Avg generation throughput: 23.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.4%, CPU KV cache usage: 0.0%, Interval(ms): 470.4
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 470.0882 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6080, fwd=8.0297 cmp_logits=0.0832, sampling=11.6601, total=20.3819
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 937.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1265 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5522, fwd=7.7760 cmp_logits=0.0701, sampling=11.8189, total=20.2181
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 944.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9601 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5515, fwd=7.7510 cmp_logits=0.0699, sampling=11.8163, total=20.1895
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9115 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5579, fwd=7.7705 cmp_logits=0.0691, sampling=11.8053, total=20.2038
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 946.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9258 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5491, fwd=7.7412 cmp_logits=0.0701, sampling=11.8518, total=20.2127
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 943.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9894 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5593, fwd=7.8011 cmp_logits=0.0703, sampling=11.7714, total=20.2031
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 945.4 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.2
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9446 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5500, fwd=7.8502 cmp_logits=0.0813, sampling=11.6706, total=20.1528
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 900.9 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8576 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5429, fwd=7.7329 cmp_logits=0.0703, sampling=11.8020, total=20.1490
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 901.8 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8642 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([18]), positions.shape=torch.Size([18]) hidden_states.shape=torch.Size([18, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5207, fwd=7.8378 cmp_logits=0.0813, sampling=11.5798, total=20.0205
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.2 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6981 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([18]), positions.shape=torch.Size([18]) hidden_states.shape=torch.Size([18, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5293, fwd=7.7553 cmp_logits=0.0701, sampling=11.7207, total=20.0763
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 858.4 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7686 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5133, fwd=7.8485 cmp_logits=0.0818, sampling=10.9181, total=19.3627
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 789.4 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.3
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9916 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5019, fwd=7.7791 cmp_logits=0.0694, sampling=10.9761, total=19.3279
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 793.5 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9652 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4973, fwd=7.7608 cmp_logits=0.0691, sampling=11.0075, total=19.3355
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 792.6 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9842 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5004, fwd=7.8056 cmp_logits=0.0708, sampling=10.9828, total=19.3605
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 791.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0098 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4866, fwd=7.8275 cmp_logits=0.0806, sampling=10.8805, total=19.2761
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 745.5 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9006 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4692, fwd=7.8523 cmp_logits=0.0811, sampling=10.4389, total=18.8427
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 712.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 19.7
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.4461 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4737, fwd=7.7345 cmp_logits=0.0682, sampling=10.5927, total=18.8704
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 711.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 19.7
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.4585 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4716, fwd=7.7319 cmp_logits=0.0696, sampling=10.5960, total=18.8701
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 712.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.3%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.4540 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4725, fwd=7.7426 cmp_logits=0.0787, sampling=10.5515, total=18.8463
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 713.1 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.4435 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4599, fwd=7.7679 cmp_logits=0.0806, sampling=10.4792, total=18.7888
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 664.7 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.5%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3431 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4573, fwd=7.7348 cmp_logits=0.0694, sampling=10.5166, total=18.7790
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 665.9 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.5%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3393 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:32:55 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4566, fwd=7.7493 cmp_logits=0.0682, sampling=10.4911, total=18.7662
INFO 10-04 03:32:55 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 665.2 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.5%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:32:55 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3596 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:32:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4535, fwd=7.7543 cmp_logits=0.0687, sampling=10.4935, total=18.7707
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 666.4 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.5%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3217 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4578, fwd=7.7622 cmp_logits=0.0684, sampling=10.4885, total=18.7781
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 665.1 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3634 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4256, fwd=7.8099 cmp_logits=0.0689, sampling=9.9072, total=18.2123
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 580.8 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7099 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4323, fwd=7.7033 cmp_logits=0.0691, sampling=10.0045, total=18.2097
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 582.4 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7097 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4249, fwd=7.7448 cmp_logits=0.0684, sampling=9.9692, total=18.2080
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 582.7 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7025 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4263, fwd=7.7929 cmp_logits=0.0682, sampling=9.9680, total=18.2562
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 581.1 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7559 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4306, fwd=7.8037 cmp_logits=0.0679, sampling=9.9089, total=18.2118
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 581.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7349 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4206, fwd=7.8199 cmp_logits=0.0815, sampling=9.6800, total=18.0030
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 535.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4712 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4151, fwd=7.7269 cmp_logits=0.0679, sampling=9.7959, total=18.0066
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 536.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4691 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4110, fwd=7.7260 cmp_logits=0.0682, sampling=9.7847, total=17.9906
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 536.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4677 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4077, fwd=7.7937 cmp_logits=0.0687, sampling=9.7468, total=18.0178
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 535.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4910 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4158, fwd=7.7355 cmp_logits=0.0679, sampling=9.7957, total=18.0159
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 535.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4827 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4096, fwd=7.7453 cmp_logits=0.0687, sampling=9.7976, total=18.0221
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 535.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5072 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3965, fwd=7.7858 cmp_logits=0.0687, sampling=9.3961, total=17.6475
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 491.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0919 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4067, fwd=7.7388 cmp_logits=0.0682, sampling=9.4018, total=17.6167
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0595 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4003, fwd=7.7322 cmp_logits=0.0675, sampling=9.4035, total=17.6044
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0519 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4022, fwd=7.7078 cmp_logits=0.0682, sampling=9.4516, total=17.6306
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0769 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3989, fwd=7.7264 cmp_logits=0.0687, sampling=9.4192, total=17.6139
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 492.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0752 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4051, fwd=7.7560 cmp_logits=0.0677, sampling=9.8069, total=18.0364
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 482.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4860 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4056, fwd=7.8104 cmp_logits=0.0672, sampling=9.3608, total=17.6446
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 492.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0898 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4115, fwd=7.7443 cmp_logits=0.0679, sampling=9.4001, total=17.6251
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0833 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3812, fwd=7.8051 cmp_logits=0.0796, sampling=8.9798, total=17.2467
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 447.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6575 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3884, fwd=7.7477 cmp_logits=0.0772, sampling=9.0122, total=17.2265
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 448.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6764 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3798, fwd=7.7970 cmp_logits=0.0668, sampling=8.9815, total=17.2260
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 449.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6435 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3805, fwd=7.7186 cmp_logits=0.0682, sampling=9.0580, total=17.2260
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 448.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6597 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3693, fwd=7.9610 cmp_logits=0.0799, sampling=8.4462, total=16.8574
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2482 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3645, fwd=7.7863 cmp_logits=0.0672, sampling=8.6677, total=16.8867
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2763 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3681, fwd=7.7910 cmp_logits=0.0679, sampling=8.6837, total=16.9117
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2951 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3705, fwd=7.8156 cmp_logits=0.0677, sampling=8.6482, total=16.9032
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3080 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3650, fwd=7.7991 cmp_logits=0.0684, sampling=8.6763, total=16.9101
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3013 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3674, fwd=7.7448 cmp_logits=0.0675, sampling=8.7218, total=16.9022
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2858 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3645, fwd=7.7486 cmp_logits=0.0670, sampling=8.7042, total=16.8855
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2715 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3638, fwd=7.7929 cmp_logits=0.0675, sampling=8.6923, total=16.9172
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3197 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3588, fwd=7.8666 cmp_logits=0.0780, sampling=8.2071, total=16.5117
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 351.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8874 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3374, fwd=7.8087 cmp_logits=0.0792, sampling=7.7479, total=15.9738
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 302.2 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.5
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.3207 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3207, fwd=7.8082 cmp_logits=0.0789, sampling=7.4360, total=15.6443
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.0 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9373 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3195, fwd=7.7436 cmp_logits=0.0672, sampling=7.4902, total=15.6212
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 249.0 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9118 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3207, fwd=7.7825 cmp_logits=0.0679, sampling=7.4642, total=15.6360
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9454 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3035, fwd=7.7908 cmp_logits=0.0780, sampling=7.2744, total=15.4476
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7008 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3030, fwd=7.7784 cmp_logits=0.0672, sampling=7.2796, total=15.4293
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6889 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3030, fwd=7.7231 cmp_logits=0.0663, sampling=7.3533, total=15.4467
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7058 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3068, fwd=7.7779 cmp_logits=0.0672, sampling=7.2937, total=15.4467
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7037 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3026, fwd=7.7596 cmp_logits=0.0660, sampling=7.3185, total=15.4476
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7070 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3014, fwd=7.7417 cmp_logits=0.0668, sampling=7.3206, total=15.4314
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6860 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3011, fwd=7.6756 cmp_logits=0.0663, sampling=7.3967, total=15.4407
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6960 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3030, fwd=7.7031 cmp_logits=0.0663, sampling=7.3717, total=15.4448
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7375 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3033, fwd=7.7209 cmp_logits=0.0672, sampling=7.3431, total=15.4352
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6925 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3028, fwd=7.7393 cmp_logits=0.0665, sampling=7.3156, total=15.4250
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6801 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3018, fwd=7.7412 cmp_logits=0.0665, sampling=7.3209, total=15.4314
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6872 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3061, fwd=7.7949 cmp_logits=0.0658, sampling=7.2591, total=15.4271
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7056 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2847, fwd=7.7984 cmp_logits=0.0780, sampling=7.0894, total=15.2514
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4727 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2842, fwd=7.8099 cmp_logits=0.0660, sampling=7.1101, total=15.2707
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5163 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2637, fwd=7.9184 cmp_logits=0.0670, sampling=7.1473, total=15.3973
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5735 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2627, fwd=7.9010 cmp_logits=0.0672, sampling=7.1309, total=15.3627
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5449 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2620, fwd=7.8347 cmp_logits=0.0672, sampling=7.2069, total=15.3723
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5468 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2620, fwd=7.8678 cmp_logits=0.0670, sampling=7.1764, total=15.3742
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5480 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2623, fwd=7.8855 cmp_logits=0.0675, sampling=7.1886, total=15.4047
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5785 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2632, fwd=7.8356 cmp_logits=0.0672, sampling=7.2048, total=15.3718
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5478 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2587, fwd=7.8902 cmp_logits=0.0679, sampling=7.1487, total=15.3666
INFO 10-04 03:32:56 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:32:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5404 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2635, fwd=7.8924 cmp_logits=0.0670, sampling=7.1418, total=15.3656
INFO 10-04 03:32:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:32:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5418 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2615, fwd=7.8270 cmp_logits=0.0677, sampling=7.2403, total=15.3971
INFO 10-04 03:32:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:32:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5730 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2611, fwd=7.8757 cmp_logits=0.0672, sampling=7.2131, total=15.4181
INFO 10-04 03:32:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:32:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5923 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2611, fwd=7.8640 cmp_logits=0.0672, sampling=7.2334, total=15.4269
INFO 10-04 03:32:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 03:32:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6043 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2632, fwd=7.8506 cmp_logits=0.0668, sampling=7.2451, total=15.4266
INFO 10-04 03:32:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:32:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6047 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2646, fwd=7.8409 cmp_logits=0.0663, sampling=7.2832, total=15.4560
INFO 10-04 03:32:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:32:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6288 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 03:32:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 03:32:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2632, fwd=7.8561 cmp_logits=0.0672, sampling=7.2544, total=15.4414
INFO 10-04 03:32:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 03:32:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6395 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.5870738, last_token_time=1728012775.7016313, first_scheduled_time=1728012774.6172712, first_token_time=1728012775.06168, time_in_queue=0.0301973819732666, finished_time=1728012775.701553, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.5881565, last_token_time=1728012776.2273095, first_scheduled_time=1728012774.6172712, first_token_time=1728012775.06168, time_in_queue=0.029114723205566406, finished_time=1728012776.2272687, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.5899549, last_token_time=1728012775.9230971, first_scheduled_time=1728012774.6172712, first_token_time=1728012775.06168, time_in_queue=0.02731633186340332, finished_time=1728012775.9230444, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.5907068, last_token_time=1728012776.618374, first_scheduled_time=1728012774.6172712, first_token_time=1728012775.06168, time_in_queue=0.026564359664916992, finished_time=1728012776.6183522, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.5929103, last_token_time=1728012776.60183, first_scheduled_time=1728012774.6172712, first_token_time=1728012775.06168, time_in_queue=0.02436089515686035, finished_time=1728012776.6018102, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.5949922, last_token_time=1728012775.743528, first_scheduled_time=1728012774.6172712, first_token_time=1728012775.06168, time_in_queue=0.022279024124145508, finished_time=1728012775.7434747, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.5954335, last_token_time=1728012775.743528, first_scheduled_time=1728012774.6172712, first_token_time=1728012775.06168, time_in_queue=0.021837711334228516, finished_time=1728012775.7434788, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.5958927, last_token_time=1728012776.6666584, first_scheduled_time=1728012774.6172712, first_token_time=1728012775.06168, time_in_queue=0.021378517150878906, finished_time=1728012776.666642, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.598224, last_token_time=1728012777.1082428, first_scheduled_time=1728012774.6172712, first_token_time=1728012775.06168, time_in_queue=0.019047260284423828, finished_time=1728012777.1082401, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.601652, last_token_time=1728012776.8568702, first_scheduled_time=1728012775.062503, first_token_time=1728012775.5320008, time_in_queue=0.46085119247436523, finished_time=1728012776.8568633, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.6043644, last_token_time=1728012775.8243544, first_scheduled_time=1728012775.062503, first_token_time=1728012775.5320008, time_in_queue=0.45813870429992676, finished_time=1728012775.8243196, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.6049247, last_token_time=1728012776.0207775, first_scheduled_time=1728012775.062503, first_token_time=1728012775.5320008, time_in_queue=0.4575784206390381, finished_time=1728012776.0207522, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.6058471, last_token_time=1728012776.3737586, first_scheduled_time=1728012775.062503, first_token_time=1728012775.5320008, time_in_queue=0.45665597915649414, finished_time=1728012776.3737445, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.6074011, last_token_time=1728012776.4451032, first_scheduled_time=1728012775.062503, first_token_time=1728012775.5320008, time_in_queue=0.45510196685791016, finished_time=1728012776.445093, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.609162, last_token_time=1728012775.844476, first_scheduled_time=1728012775.062503, first_token_time=1728012775.5320008, time_in_queue=0.453341007232666, finished_time=1728012775.8444574, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.60974, last_token_time=1728012776.1153257, first_scheduled_time=1728012775.062503, first_token_time=1728012775.5320008, time_in_queue=0.45276308059692383, finished_time=1728012776.115316, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.6107972, last_token_time=1728012776.0207775, first_scheduled_time=1728012775.062503, first_token_time=1728012775.5320008, time_in_queue=0.4517059326171875, finished_time=1728012776.020768, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.6116862, last_token_time=1728012775.659474, first_scheduled_time=1728012775.062503, first_token_time=1728012775.5320008, time_in_queue=0.4508168697357178, finished_time=1728012775.659464, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.6119583, last_token_time=1728012776.5847511, first_scheduled_time=1728012775.062503, first_token_time=1728012775.5320008, time_in_queue=0.4505448341369629, finished_time=1728012776.5847452, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728012774.6140618, last_token_time=1728012776.8881803, first_scheduled_time=1728012775.062503, first_token_time=1728012775.5320008, time_in_queue=0.4484412670135498, finished_time=1728012776.8881783, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 15868
End-to-End latency: 2.52 seconds
Throughput: 7.93 requests/s, 6293.37 tokens/s
Per_token_time: 0.159 ms

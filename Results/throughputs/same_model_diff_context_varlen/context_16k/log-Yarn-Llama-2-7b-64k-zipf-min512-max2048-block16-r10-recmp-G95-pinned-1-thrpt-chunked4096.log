Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=16384, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=4096, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Selected required requests 10
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Chunked prefill is enabled. max_num_batched_tokens=4096, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'finetuned', 'original_max_position_embeddings', 'type'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 05:34:05 config.py:654] [SchedulerConfig] max_num_batched_tokens: 4096 chunked_prefill_enabled: True
INFO 10-04 05:34:05 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 05:34:06 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 05:34:11 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 05:34:15 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 05:34:15 model_runner.py:183] Loaded model: 
INFO 10-04 05:34:15 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 05:34:15 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 05:34:15 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 05:34:15 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 05:34:15 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 05:34:15 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 05:34:15 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 05:34:15 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 05:34:15 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 05:34:15 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 05:34:15 model_runner.py:183]         )
INFO 10-04 05:34:15 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 05:34:15 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 05:34:15 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 05:34:15 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 05:34:15 model_runner.py:183]         )
INFO 10-04 05:34:15 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 05:34:15 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 05:34:15 model_runner.py:183]       )
INFO 10-04 05:34:15 model_runner.py:183]     )
INFO 10-04 05:34:15 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 05:34:15 model_runner.py:183]   )
INFO 10-04 05:34:15 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 05:34:15 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 05:34:15 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 05:34:15 model_runner.py:183] )
INFO 10-04 05:34:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 05:34:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 12.3758, fwd=357.9445 cmp_logits=0.2296, sampling=247.5188, total=618.0708
INFO 10-04 05:34:15 worker.py:164] Peak: 13.385 GB, Initial: 38.980 GB, Free: 25.595 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.039 GB
INFO 10-04 05:34:16 gpu_executor.py:117] # GPU blocks: 3077, # CPU blocks: 8192
INFO 10-04 05:34:47 worker.py:189] _init_cache_engine took 24.0625 GB
INFO 10-04 05:34:47 scheduler.py:307] Scheduler initialized with prompt limit: 16384
INFO 10-04 05:34:47 Start warmup...
INFO 10-04 05:34:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 05:34:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9291, fwd=3595.8722 cmp_logits=71.3077, sampling=0.9604, total=3669.0724
INFO 10-04 05:34:50 metrics.py:335] Avg prompt throughput: 278.5 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 3677.4
INFO 10-04 05:34:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 3669.6122 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3209, fwd=43.6890 cmp_logits=0.1183, sampling=6.5482, total=50.6783
INFO 10-04 05:34:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 51.6
INFO 10-04 05:34:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 51.1255 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3233, fwd=8.1060 cmp_logits=0.0787, sampling=7.0536, total=15.5635
INFO 10-04 05:34:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 61.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 05:34:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8966 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2873, fwd=7.9868 cmp_logits=0.0744, sampling=7.1321, total=15.4822
INFO 10-04 05:34:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 05:34:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7475 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2789, fwd=7.9634 cmp_logits=0.0720, sampling=7.1714, total=15.4874
INFO 10-04 05:34:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 05:34:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7270 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2780, fwd=7.9787 cmp_logits=0.0720, sampling=7.1290, total=15.4598
INFO 10-04 05:34:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:34:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6882 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2794, fwd=7.9436 cmp_logits=0.0746, sampling=7.1750, total=15.4748
INFO 10-04 05:34:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 05:34:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7075 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2689, fwd=7.9503 cmp_logits=0.0703, sampling=7.1719, total=15.4626
INFO 10-04 05:34:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 05:34:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7149 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2728, fwd=7.9966 cmp_logits=0.0677, sampling=7.1440, total=15.4822
INFO 10-04 05:34:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:34:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6891 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2739, fwd=7.9360 cmp_logits=0.0679, sampling=7.1826, total=15.4614
INFO 10-04 05:34:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:34:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6860 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:56 Start benchmarking...
INFO 10-04 05:34:56 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.6623, fwd=1632.0343 cmp_logits=0.1552, sampling=273.1919, total=1907.0446
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 2112.4 tokens/s, Avg generation throughput: 2.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 9.8%, CPU KV cache usage: 0.0%, Interval(ms): 1939.0
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1907.7864 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4096]), positions.shape=torch.Size([4096]) hidden_states.shape=torch.Size([4096, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.7118, fwd=14.3104 cmp_logits=0.0970, sampling=244.0932, total=260.2136
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 15668.8 tokens/s, Avg generation throughput: 30.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 261.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 260.8800 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2478]), positions.shape=torch.Size([2478]) hidden_states.shape=torch.Size([2478, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.2622, fwd=14.2946 cmp_logits=0.0904, sampling=160.6486, total=176.2965
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 13947.9 tokens/s, Avg generation throughput: 56.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.8%, CPU KV cache usage: 0.0%, Interval(ms): 177.1
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 176.8818 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4268, fwd=7.6904 cmp_logits=0.0689, sampling=9.3584, total=17.5457
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0509 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4263, fwd=7.6971 cmp_logits=0.0679, sampling=9.3200, total=17.5123
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0099 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4177, fwd=7.6888 cmp_logits=0.0675, sampling=9.3346, total=17.5097
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0311 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4194, fwd=7.6694 cmp_logits=0.0679, sampling=9.3229, total=17.4806
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9820 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4158, fwd=7.7055 cmp_logits=0.0694, sampling=9.3265, total=17.5178
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0204 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4203, fwd=7.6895 cmp_logits=0.0668, sampling=9.3400, total=17.5176
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0142 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4184, fwd=7.6890 cmp_logits=0.0679, sampling=9.3272, total=17.5037
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9987 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4172, fwd=7.7074 cmp_logits=0.0675, sampling=9.3291, total=17.5221
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0173 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4256, fwd=7.7121 cmp_logits=0.0677, sampling=9.3093, total=17.5154
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0252 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4234, fwd=7.6857 cmp_logits=0.0675, sampling=9.3338, total=17.5114
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0230 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4210, fwd=7.6988 cmp_logits=0.0679, sampling=9.4380, total=17.6268
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 546.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1403 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4170, fwd=7.7024 cmp_logits=0.0668, sampling=9.3331, total=17.5204
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0380 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4222, fwd=7.7140 cmp_logits=0.0675, sampling=9.3484, total=17.5531
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0533 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4139, fwd=7.7085 cmp_logits=0.0672, sampling=9.3358, total=17.5266
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0287 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4151, fwd=7.8273 cmp_logits=0.0670, sampling=9.2053, total=17.5157
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0221 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4168, fwd=7.6919 cmp_logits=0.0675, sampling=9.3565, total=17.5338
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0418 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4213, fwd=7.6694 cmp_logits=0.0677, sampling=9.3853, total=17.5447
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0473 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4263, fwd=7.7016 cmp_logits=0.0677, sampling=9.3455, total=17.5424
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0733 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4196, fwd=7.6911 cmp_logits=0.0672, sampling=9.3803, total=17.5593
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0593 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4206, fwd=7.7007 cmp_logits=0.0694, sampling=9.3913, total=17.5829
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0905 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4163, fwd=7.6787 cmp_logits=0.0665, sampling=9.4056, total=17.5676
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0709 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4170, fwd=7.6873 cmp_logits=0.0670, sampling=9.3808, total=17.5531
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0655 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4225, fwd=7.6921 cmp_logits=0.0675, sampling=9.3837, total=17.5669
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 546.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1129 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4134, fwd=7.6833 cmp_logits=0.0677, sampling=9.4006, total=17.5660
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0883 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4075, fwd=7.7763 cmp_logits=0.0803, sampling=9.0275, total=17.2925
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 500.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7743 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:34:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4058, fwd=7.7164 cmp_logits=0.0675, sampling=9.0761, total=17.2668
INFO 10-04 05:34:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 05:34:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7462 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:34:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4072, fwd=7.6942 cmp_logits=0.0675, sampling=9.1157, total=17.2856
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7872 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3772, fwd=7.7350 cmp_logits=0.0789, sampling=8.6193, total=16.8114
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2698 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3691, fwd=7.7157 cmp_logits=0.0672, sampling=8.6324, total=16.7854
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2012 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3698, fwd=7.6563 cmp_logits=0.0665, sampling=8.6899, total=16.7832
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1988 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3746, fwd=7.7000 cmp_logits=0.0660, sampling=8.6448, total=16.7866
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2048 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3717, fwd=7.6952 cmp_logits=0.0663, sampling=8.7492, total=16.8831
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3118 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3722, fwd=7.7162 cmp_logits=0.0663, sampling=8.7197, total=16.8753
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3471 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3562, fwd=7.7317 cmp_logits=0.0796, sampling=8.4465, total=16.6152
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9961 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3564, fwd=7.7269 cmp_logits=0.0658, sampling=8.4689, total=16.6190
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9990 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3581, fwd=7.6964 cmp_logits=0.0670, sampling=8.4825, total=16.6047
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9909 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3619, fwd=7.7186 cmp_logits=0.0660, sampling=8.4975, total=16.6450
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0362 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3555, fwd=7.6859 cmp_logits=0.0663, sampling=8.5034, total=16.6121
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0307 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3674, fwd=7.6489 cmp_logits=0.0656, sampling=8.5382, total=16.6209
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0050 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3581, fwd=7.6604 cmp_logits=0.0660, sampling=8.5006, total=16.5861
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9740 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3560, fwd=7.7159 cmp_logits=0.0663, sampling=8.4643, total=16.6037
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9792 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3579, fwd=7.6888 cmp_logits=0.0665, sampling=8.5130, total=16.6271
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0147 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3583, fwd=7.7307 cmp_logits=0.0670, sampling=8.4677, total=16.6249
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0398 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3581, fwd=7.6611 cmp_logits=0.0665, sampling=8.5111, total=16.5977
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9871 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3526, fwd=7.6756 cmp_logits=0.0660, sampling=8.5018, total=16.5968
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9842 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3676, fwd=7.7114 cmp_logits=0.0734, sampling=8.4786, total=16.6318
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0274 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3357, fwd=7.8218 cmp_logits=0.0784, sampling=8.0869, total=16.3238
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6764 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3390, fwd=7.7114 cmp_logits=0.0663, sampling=8.1699, total=16.2876
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6361 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3433, fwd=7.7045 cmp_logits=0.0660, sampling=8.1353, total=16.2501
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5987 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3428, fwd=7.6978 cmp_logits=0.0660, sampling=8.1522, total=16.2601
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6106 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3436, fwd=7.6838 cmp_logits=0.0670, sampling=8.1556, total=16.2511
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6070 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3400, fwd=7.7369 cmp_logits=0.0656, sampling=8.1160, total=16.2594
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6144 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3421, fwd=7.7488 cmp_logits=0.0670, sampling=8.1110, total=16.2702
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6223 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3388, fwd=7.6816 cmp_logits=0.0660, sampling=8.1642, total=16.2516
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6030 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3448, fwd=7.6854 cmp_logits=0.0658, sampling=8.1644, total=16.2613
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6070 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3386, fwd=7.6733 cmp_logits=0.0672, sampling=8.1956, total=16.2754
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6304 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3428, fwd=7.6725 cmp_logits=0.0668, sampling=8.1584, total=16.2416
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5946 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3405, fwd=7.7176 cmp_logits=0.0653, sampling=8.2178, total=16.3422
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6919 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3383, fwd=7.7026 cmp_logits=0.0668, sampling=8.1217, total=16.2308
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5861 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3438, fwd=7.6778 cmp_logits=0.0660, sampling=8.1601, total=16.2485
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.6%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6044 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3483, fwd=7.6728 cmp_logits=0.0737, sampling=8.1792, total=16.2749
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.6%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6221 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3371, fwd=7.6709 cmp_logits=0.0679, sampling=8.1608, total=16.2377
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.6%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5915 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3386, fwd=7.7095 cmp_logits=0.0656, sampling=8.1658, total=16.2807
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6576 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3204, fwd=7.7407 cmp_logits=0.0663, sampling=7.6039, total=15.7323
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.0689 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3045, fwd=7.7593 cmp_logits=0.0796, sampling=7.2339, total=15.3782
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6622 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3023, fwd=7.6730 cmp_logits=0.0648, sampling=7.2937, total=15.3348
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6369 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2856, fwd=7.8504 cmp_logits=0.0784, sampling=7.0746, total=15.2900
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5354 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2861, fwd=7.7019 cmp_logits=0.0646, sampling=7.2703, total=15.3236
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5683 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2851, fwd=7.7305 cmp_logits=0.0658, sampling=7.2076, total=15.2898
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5325 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2828, fwd=7.6525 cmp_logits=0.0653, sampling=7.2651, total=15.2667
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5065 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2825, fwd=7.6976 cmp_logits=0.0663, sampling=7.2491, total=15.2967
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5389 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2830, fwd=7.7176 cmp_logits=0.0668, sampling=7.1976, total=15.2659
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5101 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2840, fwd=7.7572 cmp_logits=0.0656, sampling=7.1959, total=15.3036
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5447 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2935, fwd=7.6988 cmp_logits=0.0656, sampling=7.2567, total=15.3155
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5578 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2875, fwd=7.6842 cmp_logits=0.0653, sampling=7.2577, total=15.2957
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5368 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2847, fwd=7.6783 cmp_logits=0.0656, sampling=7.2460, total=15.2755
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5232 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2823, fwd=7.7217 cmp_logits=0.0644, sampling=7.2021, total=15.2712
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5370 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2637, fwd=7.8700 cmp_logits=0.0675, sampling=7.2367, total=15.4388
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6372 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2608, fwd=7.8664 cmp_logits=0.0668, sampling=7.2105, total=15.4057
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6045 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2639, fwd=7.7999 cmp_logits=0.0670, sampling=7.2696, total=15.4011
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5988 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2601, fwd=7.8795 cmp_logits=0.0677, sampling=7.1843, total=15.3923
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5897 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2613, fwd=7.8738 cmp_logits=0.0656, sampling=7.2088, total=15.4104
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6064 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2615, fwd=7.8883 cmp_logits=0.0660, sampling=7.2303, total=15.4469
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6450 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2604, fwd=7.8490 cmp_logits=0.0670, sampling=7.2236, total=15.4011
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5997 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2613, fwd=7.8187 cmp_logits=0.0668, sampling=7.2541, total=15.4016
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6004 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2604, fwd=7.8330 cmp_logits=0.0663, sampling=7.2441, total=15.4049
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6035 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:34:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2601, fwd=7.9010 cmp_logits=0.0670, sampling=7.1962, total=15.4250
INFO 10-04 05:34:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:34:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6224 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:34:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:34:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:35:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2627, fwd=7.8804 cmp_logits=0.0668, sampling=7.2253, total=15.4359
INFO 10-04 05:35:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:35:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6353 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:35:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:35:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:35:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2718, fwd=7.8084 cmp_logits=0.0741, sampling=7.2534, total=15.4088
INFO 10-04 05:35:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:35:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6057 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:35:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=4096, max_num_seqs=256
INFO 10-04 05:35:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:35:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2632, fwd=7.8597 cmp_logits=0.0670, sampling=7.2150, total=15.4059
INFO 10-04 05:35:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:35:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6286 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020096.1474006, last_token_time=1728020098.951982, first_scheduled_time=1728020096.1688418, first_token_time=1728020098.076074, time_in_queue=0.021441221237182617, finished_time=1728020098.9519405, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020096.1491504, last_token_time=1728020099.333599, first_scheduled_time=1728020096.1688418, first_token_time=1728020098.076074, time_in_queue=0.01969146728515625, finished_time=1728020099.3335729, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020096.151074, last_token_time=1728020099.1104298, first_scheduled_time=1728020096.1688418, first_token_time=1728020098.076074, time_in_queue=0.017767906188964844, finished_time=1728020099.1104064, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020096.152347, last_token_time=1728020099.618826, first_scheduled_time=1728020096.1688418, first_token_time=1728020098.076074, time_in_queue=0.0164947509765625, finished_time=1728020099.6188042, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020096.1548555, last_token_time=1728020099.6350896, first_scheduled_time=1728020096.1688418, first_token_time=1728020098.33718, time_in_queue=0.013986349105834961, finished_time=1728020099.6350727, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020096.1573734, last_token_time=1728020099.0058455, first_scheduled_time=1728020098.0768037, first_token_time=1728020098.33718, time_in_queue=1.9194302558898926, finished_time=1728020099.0058274, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020096.1584206, last_token_time=1728020099.0058455, first_scheduled_time=1728020098.0768037, first_token_time=1728020098.33718, time_in_queue=1.9183831214904785, finished_time=1728020099.0058317, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020096.1594586, last_token_time=1728020099.6667206, first_scheduled_time=1728020098.0768037, first_token_time=1728020098.33718, time_in_queue=1.9173450469970703, finished_time=1728020099.666708, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020096.1620102, last_token_time=1728020100.0439982, first_scheduled_time=1728020098.0768037, first_token_time=1728020098.5142603, time_in_queue=1.9147934913635254, finished_time=1728020100.0439954, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020096.165565, last_token_time=1728020099.8392138, first_scheduled_time=1728020098.3378155, first_token_time=1728020098.5142603, time_in_queue=2.172250509262085, finished_time=1728020099.839212, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 11197
End-to-End latency: 3.90 seconds
Throughput: 2.57 requests/s, 2873.38 tokens/s
Per_token_time: 0.348 ms

Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=128, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=16384, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=512, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Selected required requests 20
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Chunked prefill is enabled. max_num_batched_tokens=512, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'type', 'original_max_position_embeddings', 'finetuned'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 04:20:09 config.py:654] [SchedulerConfig] max_num_batched_tokens: 512 chunked_prefill_enabled: True
INFO 10-04 04:20:09 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 04:20:10 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 04:20:15 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 04:20:18 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 04:20:18 model_runner.py:183] Loaded model: 
INFO 10-04 04:20:18 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 04:20:18 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 04:20:18 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 04:20:18 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 04:20:18 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 04:20:18 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 04:20:18 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 04:20:18 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 04:20:18 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 04:20:18 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 04:20:18 model_runner.py:183]         )
INFO 10-04 04:20:18 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 04:20:18 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 04:20:18 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 04:20:18 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 04:20:18 model_runner.py:183]         )
INFO 10-04 04:20:18 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 04:20:18 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 04:20:18 model_runner.py:183]       )
INFO 10-04 04:20:18 model_runner.py:183]     )
INFO 10-04 04:20:18 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 04:20:18 model_runner.py:183]   )
INFO 10-04 04:20:18 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 04:20:18 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 04:20:18 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 04:20:18 model_runner.py:183] )
INFO 10-04 04:20:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:20:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 28.5108, fwd=360.7671 cmp_logits=0.2539, sampling=79.6905, total=469.2242
INFO 10-04 04:20:19 worker.py:164] Peak: 13.264 GB, Initial: 38.980 GB, Free: 25.716 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.160 GB
INFO 10-04 04:20:19 gpu_executor.py:117] # GPU blocks: 3092, # CPU blocks: 8192
INFO 10-04 04:20:50 worker.py:189] _init_cache_engine took 24.1875 GB
INFO 10-04 04:20:50 scheduler.py:307] Scheduler initialized with prompt limit: 16384
INFO 10-04 04:20:50 Start warmup...
INFO 10-04 04:20:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:20:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:20:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7272, fwd=1873.2073 cmp_logits=0.0870, sampling=0.2480, total=1874.2712
INFO 10-04 04:20:52 metrics.py:335] Avg prompt throughput: 272.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 1881.4
INFO 10-04 04:20:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1874.7077 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:20:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:20:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:20:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 29.6578, fwd=13.8330 cmp_logits=38.2297, sampling=0.9034, total=82.6256
INFO 10-04 04:20:52 metrics.py:335] Avg prompt throughput: 6140.8 tokens/s, Avg generation throughput: 12.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 83.4
INFO 10-04 04:20:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 83.0359 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:20:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:20:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:20:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2978, fwd=44.3010 cmp_logits=0.1175, sampling=6.5711, total=51.2888
INFO 10-04 04:20:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 52.0
INFO 10-04 04:20:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 51.7106 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:20:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:20:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:20:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3138, fwd=8.2192 cmp_logits=0.0808, sampling=6.9320, total=15.5475
INFO 10-04 04:20:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 61.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 04:20:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8706 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:20:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:20:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:20:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2863, fwd=8.1966 cmp_logits=0.0744, sampling=7.0460, total=15.6054
INFO 10-04 04:20:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.0
INFO 10-04 04:20:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8644 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:20:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:20:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:20:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2794, fwd=8.0917 cmp_logits=0.0756, sampling=7.0338, total=15.4815
INFO 10-04 04:20:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 04:20:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7192 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:20:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:20:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:20:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2785, fwd=8.1019 cmp_logits=0.0765, sampling=7.0112, total=15.4696
INFO 10-04 04:20:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 04:20:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7056 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:20:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:20:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:20:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2708, fwd=8.0726 cmp_logits=0.0710, sampling=6.8994, total=15.3146
INFO 10-04 04:20:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:20:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5652 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:20:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:20:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:20:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2651, fwd=8.0540 cmp_logits=0.0691, sampling=5.9044, total=14.2934
INFO 10-04 04:20:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 04:20:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5082 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:20:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:20:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:20:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2639, fwd=8.0750 cmp_logits=0.0699, sampling=5.8987, total=14.3085
INFO 10-04 04:20:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 04:20:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5078 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:20:52 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:20:52 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:20:52 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2642, fwd=8.0879 cmp_logits=0.0701, sampling=5.8320, total=14.2553
INFO 10-04 04:20:52 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 04:20:52 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4732 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:20:57 Start benchmarking...
INFO 10-04 04:20:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:20:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:20:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7155, fwd=1584.3778 cmp_logits=0.1221, sampling=30.8006, total=1616.0176
INFO 10-04 04:20:59 metrics.py:335] Avg prompt throughput: 309.1 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 18 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%, Interval(ms): 1656.5
INFO 10-04 04:20:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1616.5543 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:20:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:20:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:20:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6688, fwd=14.6389 cmp_logits=0.1137, sampling=34.8029, total=50.2257
INFO 10-04 04:20:59 metrics.py:335] Avg prompt throughput: 10057.0 tokens/s, Avg generation throughput: 39.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 17 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%, Interval(ms): 50.8
INFO 10-04 04:20:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.5939 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:20:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:20:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6518, fwd=1572.4781 cmp_logits=0.1414, sampling=25.9986, total=1599.2713
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 318.8 tokens/s, Avg generation throughput: 1.9 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%, Interval(ms): 1599.9
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1599.7367 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6974, fwd=14.7119 cmp_logits=0.0784, sampling=29.8812, total=45.3699
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 11062.7 tokens/s, Avg generation throughput: 65.2 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%, Interval(ms): 46.0
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.7816 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6716, fwd=14.6358 cmp_logits=0.0944, sampling=30.4875, total=45.8901
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 10957.1 tokens/s, Avg generation throughput: 86.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 46.5
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.2852 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6611, fwd=14.7269 cmp_logits=0.0727, sampling=28.7063, total=44.1680
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 11367.8 tokens/s, Avg generation throughput: 89.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 44.7
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.5187 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6759, fwd=14.7445 cmp_logits=0.0887, sampling=31.7771, total=47.2872
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 10617.3 tokens/s, Avg generation throughput: 104.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 14 reqs, GPU KV cache usage: 7.8%, CPU KV cache usage: 0.0%, Interval(ms): 47.8
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.6875 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7191, fwd=14.7219 cmp_logits=0.0868, sampling=24.0998, total=39.6285
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 12586.9 tokens/s, Avg generation throughput: 173.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 10.7%, CPU KV cache usage: 0.0%, Interval(ms): 40.3
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 40.1120 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7031, fwd=14.7614 cmp_logits=0.0725, sampling=25.5396, total=41.0774
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 12101.6 tokens/s, Avg generation throughput: 167.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 10.3%, CPU KV cache usage: 0.0%, Interval(ms): 41.7
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 41.5585 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6940, fwd=14.7262 cmp_logits=0.0865, sampling=30.6783, total=46.1860
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 10812.8 tokens/s, Avg generation throughput: 128.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 10.3%, CPU KV cache usage: 0.0%, Interval(ms): 46.8
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.5927 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7198, fwd=14.7736 cmp_logits=0.0730, sampling=34.2944, total=49.8619
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 10020.9 tokens/s, Avg generation throughput: 138.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 50.5
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.3302 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7081, fwd=14.7367 cmp_logits=0.0720, sampling=29.4027, total=44.9207
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 11092.3 tokens/s, Avg generation throughput: 153.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 45.5
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.3582 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6943, fwd=14.8623 cmp_logits=0.0722, sampling=34.2622, total=49.8922
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 10000.4 tokens/s, Avg generation throughput: 138.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 50.5
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.3273 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7381, fwd=14.7216 cmp_logits=0.0875, sampling=31.5967, total=47.1447
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 10549.6 tokens/s, Avg generation throughput: 167.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 16.9%, CPU KV cache usage: 0.0%, Interval(ms): 47.9
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.6635 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7288, fwd=14.7469 cmp_logits=0.0722, sampling=28.0561, total=43.6049
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 11388.8 tokens/s, Avg generation throughput: 180.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 16.9%, CPU KV cache usage: 0.0%, Interval(ms): 44.3
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.0776 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7312, fwd=14.7505 cmp_logits=0.0715, sampling=32.2871, total=47.8408
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 10393.7 tokens/s, Avg generation throughput: 165.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 16.9%, CPU KV cache usage: 0.0%, Interval(ms): 48.5
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.3153 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7496, fwd=14.7130 cmp_logits=0.0865, sampling=31.2271, total=46.7770
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 10614.9 tokens/s, Avg generation throughput: 189.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 17.6%, CPU KV cache usage: 0.0%, Interval(ms): 47.5
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.3073 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7603, fwd=14.7319 cmp_logits=0.0885, sampling=27.7441, total=43.3257
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 11410.8 tokens/s, Avg generation throughput: 226.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%, Interval(ms): 44.1
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.9034 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7510, fwd=14.7433 cmp_logits=0.0720, sampling=29.4514, total=45.0187
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 10997.2 tokens/s, Avg generation throughput: 196.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 19.4%, CPU KV cache usage: 0.0%, Interval(ms): 45.8
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.5644 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7708, fwd=14.7219 cmp_logits=0.0722, sampling=31.0006, total=46.5662
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 10631.3 tokens/s, Avg generation throughput: 211.4 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 21.4%, CPU KV cache usage: 0.0%, Interval(ms): 47.3
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.1346 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7567, fwd=14.7059 cmp_logits=0.0715, sampling=27.8285, total=43.3638
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 11386.0 tokens/s, Avg generation throughput: 226.8 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 21.4%, CPU KV cache usage: 0.0%, Interval(ms): 44.1
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.9045 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7777, fwd=14.9014 cmp_logits=0.0880, sampling=29.1257, total=44.8937
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 10991.0 tokens/s, Avg generation throughput: 240.8 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 21.2%, CPU KV cache usage: 0.0%, Interval(ms): 45.7
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.4903 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7746, fwd=14.7595 cmp_logits=0.0725, sampling=27.1740, total=42.7816
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 11513.0 tokens/s, Avg generation throughput: 252.3 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 22.4%, CPU KV cache usage: 0.0%, Interval(ms): 43.6
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.3815 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7951, fwd=14.7245 cmp_logits=0.0873, sampling=28.8043, total=44.4119
INFO 10-04 04:21:01 metrics.py:335] Avg prompt throughput: 11064.4 tokens/s, Avg generation throughput: 265.0 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 23.5%, CPU KV cache usage: 0.0%, Interval(ms): 45.3
INFO 10-04 04:21:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.0571 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 04:21:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8328, fwd=14.7474 cmp_logits=0.0863, sampling=27.7486, total=43.4160
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 11283.3 tokens/s, Avg generation throughput: 315.9 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 26.0%, CPU KV cache usage: 0.0%, Interval(ms): 44.3
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.1222 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8092, fwd=14.7572 cmp_logits=0.0727, sampling=28.7139, total=44.3537
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 11013.4 tokens/s, Avg generation throughput: 309.6 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 26.1%, CPU KV cache usage: 0.0%, Interval(ms): 45.2
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.0168 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8478, fwd=14.6797 cmp_logits=0.0873, sampling=33.6394, total=49.2547
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 9925.3 tokens/s, Avg generation throughput: 299.0 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 50.2
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.9725 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8337, fwd=14.7827 cmp_logits=0.0730, sampling=28.5738, total=44.2643
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 11009.3 tokens/s, Avg generation throughput: 332.3 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 45.1
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.9409 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8426, fwd=14.6437 cmp_logits=0.0732, sampling=32.8584, total=48.4188
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 10073.9 tokens/s, Avg generation throughput: 304.0 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 49.3
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.0992 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([492]), positions.shape=torch.Size([492]) hidden_states.shape=torch.Size([492, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8452, fwd=14.7963 cmp_logits=0.0865, sampling=37.5478, total=53.2768
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 8804.0 tokens/s, Avg generation throughput: 295.3 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 54.2
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.9811 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5076, fwd=7.9818 cmp_logits=0.0722, sampling=10.6695, total=19.2323
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 794.4 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9382 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4969, fwd=7.9799 cmp_logits=0.0706, sampling=10.6645, total=19.2125
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 745.2 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9037 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4826, fwd=7.9789 cmp_logits=0.0706, sampling=10.6568, total=19.1898
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 697.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8410 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4900, fwd=7.9544 cmp_logits=0.0708, sampling=10.6742, total=19.1903
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 697.3 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8421 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4835, fwd=7.9057 cmp_logits=0.0699, sampling=10.7262, total=19.1865
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 699.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.0
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8228 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4792, fwd=7.8886 cmp_logits=0.0701, sampling=10.7346, total=19.1736
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 699.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.0
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8176 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4747, fwd=7.8959 cmp_logits=0.0701, sampling=10.7684, total=19.2099
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 698.2 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8619 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4737, fwd=7.9553 cmp_logits=0.0837, sampling=9.8982, total=18.4116
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 673.8 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.3%, CPU KV cache usage: 0.0%, Interval(ms): 19.3
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.0709 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4506, fwd=7.9048 cmp_logits=0.0696, sampling=9.9518, total=18.3778
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 625.3 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.3%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9729 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4487, fwd=7.9393 cmp_logits=0.0691, sampling=9.8908, total=18.3487
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 627.3 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.1
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9428 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4618, fwd=7.9355 cmp_logits=0.0725, sampling=9.8822, total=18.3530
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 627.0 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.1
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9478 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4475, fwd=7.9012 cmp_logits=0.0699, sampling=9.9626, total=18.3818
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 626.6 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.5%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9650 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4516, fwd=7.9222 cmp_logits=0.0706, sampling=9.9423, total=18.3878
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 625.0 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.0177 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4373, fwd=7.9868 cmp_logits=0.0694, sampling=9.5270, total=18.0213
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 585.1 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.8
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5845 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4444, fwd=7.9377 cmp_logits=0.0694, sampling=9.5303, total=17.9825
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 587.8 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5289 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4399, fwd=7.9234 cmp_logits=0.0701, sampling=9.5723, total=18.0066
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 586.9 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5626 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4404, fwd=7.9427 cmp_logits=0.0694, sampling=9.5561, total=18.0094
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 586.7 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5645 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4373, fwd=7.9119 cmp_logits=0.0699, sampling=9.5828, total=18.0027
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 585.9 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.8
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5935 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4420, fwd=7.9176 cmp_logits=0.0701, sampling=9.6323, total=18.0628
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 584.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.8
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.6291 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4239, fwd=7.9794 cmp_logits=0.0696, sampling=9.3889, total=17.8626
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 537.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3930 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4227, fwd=7.9424 cmp_logits=0.0689, sampling=9.4275, total=17.8626
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 538.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3921 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4222, fwd=7.9107 cmp_logits=0.0696, sampling=9.4361, total=17.8394
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 539.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3649 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4244, fwd=7.9174 cmp_logits=0.0706, sampling=9.4442, total=17.8576
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 537.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4302 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4103, fwd=7.9811 cmp_logits=0.0694, sampling=9.1608, total=17.6225
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 490.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1236 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4103, fwd=7.9339 cmp_logits=0.0694, sampling=9.1629, total=17.5774
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0750 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4086, fwd=7.9434 cmp_logits=0.0694, sampling=9.1374, total=17.5595
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0469 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4096, fwd=7.9236 cmp_logits=0.0684, sampling=9.2003, total=17.6029
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 492.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0919 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4017, fwd=7.9243 cmp_logits=0.0691, sampling=9.1920, total=17.5881
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 492.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1000 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4034, fwd=7.8888 cmp_logits=0.0691, sampling=9.2325, total=17.5946
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0798 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3989, fwd=7.9153 cmp_logits=0.0679, sampling=9.2025, total=17.5853
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0669 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4141, fwd=7.9305 cmp_logits=0.0696, sampling=9.1658, total=17.5812
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0624 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4034, fwd=7.8874 cmp_logits=0.0699, sampling=9.2199, total=17.5812
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 492.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0931 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3979, fwd=8.2009 cmp_logits=0.0715, sampling=8.6241, total=17.2951
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 418.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 19.1
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.8180 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3586, fwd=8.0032 cmp_logits=0.0687, sampling=7.9687, total=16.4003
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 352.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8018 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3583, fwd=7.9024 cmp_logits=0.0687, sampling=8.0452, total=16.3755
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 354.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7665 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3552, fwd=7.9198 cmp_logits=0.0682, sampling=8.0905, total=16.4344
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 353.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8304 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3586, fwd=7.8785 cmp_logits=0.0679, sampling=8.1489, total=16.4547
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 352.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8467 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3583, fwd=7.9458 cmp_logits=0.0687, sampling=8.1384, total=16.5122
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 351.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9146 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 04:21:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3569, fwd=7.9069 cmp_logits=0.0679, sampling=8.0967, total=16.4289
INFO 10-04 04:21:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 353.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 10-04 04:21:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8371 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 04:21:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3438, fwd=7.9753 cmp_logits=0.0689, sampling=7.8006, total=16.1893
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5532 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3421, fwd=7.8812 cmp_logits=0.0679, sampling=7.8688, total=16.1610
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5436 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3271, fwd=7.9384 cmp_logits=0.0694, sampling=7.4933, total=15.8291
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 244.5 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.4
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1574 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3259, fwd=7.9460 cmp_logits=0.0675, sampling=7.5054, total=15.8458
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.1 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1686 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3262, fwd=7.8800 cmp_logits=0.0677, sampling=7.5517, total=15.8265
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1514 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3529, fwd=7.9184 cmp_logits=0.0737, sampling=7.4801, total=15.8260
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1476 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3262, fwd=7.9224 cmp_logits=0.0675, sampling=7.5023, total=15.8188
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.3 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1521 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3316, fwd=7.9150 cmp_logits=0.0687, sampling=7.5145, total=15.8312
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1505 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3264, fwd=7.8881 cmp_logits=0.0679, sampling=7.5779, total=15.8613
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 244.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1858 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3264, fwd=7.9300 cmp_logits=0.0672, sampling=7.4902, total=15.8145
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.6 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1347 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3233, fwd=7.9286 cmp_logits=0.0677, sampling=7.4842, total=15.8048
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.7 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1273 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3245, fwd=7.9019 cmp_logits=0.0677, sampling=7.5181, total=15.8134
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.5 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1402 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3271, fwd=7.9064 cmp_logits=0.0675, sampling=7.7262, total=16.0284
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 242.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.5
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.3693 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3078, fwd=7.9632 cmp_logits=0.0682, sampling=7.0913, total=15.4314
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7218 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3076, fwd=7.8635 cmp_logits=0.0672, sampling=7.1211, total=15.3606
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6498 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3054, fwd=7.8621 cmp_logits=0.0679, sampling=7.1146, total=15.3511
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6379 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3064, fwd=7.9088 cmp_logits=0.0672, sampling=7.0901, total=15.3732
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6615 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3090, fwd=7.9095 cmp_logits=0.0679, sampling=7.0748, total=15.3620
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6503 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3107, fwd=7.9010 cmp_logits=0.0675, sampling=7.1268, total=15.4066
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6987 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3068, fwd=7.8773 cmp_logits=0.0689, sampling=7.1251, total=15.3790
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6918 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2916, fwd=7.9386 cmp_logits=0.0682, sampling=6.9089, total=15.2082
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4591 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2913, fwd=7.8673 cmp_logits=0.0687, sampling=6.9525, total=15.1806
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4285 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2899, fwd=7.8681 cmp_logits=0.0689, sampling=6.9513, total=15.1789
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4233 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2973, fwd=7.8773 cmp_logits=0.0679, sampling=6.9895, total=15.2335
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4791 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2906, fwd=7.8492 cmp_logits=0.0672, sampling=6.9897, total=15.1975
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4450 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2873, fwd=7.8642 cmp_logits=0.0677, sampling=6.9642, total=15.1842
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4312 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2930, fwd=7.8461 cmp_logits=0.0679, sampling=6.9821, total=15.1904
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4366 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2878, fwd=7.8526 cmp_logits=0.0677, sampling=6.9723, total=15.1813
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4324 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2890, fwd=7.9076 cmp_logits=0.0677, sampling=7.0064, total=15.2717
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5199 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2859, fwd=7.8905 cmp_logits=0.0689, sampling=6.9914, total=15.2376
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4853 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2890, fwd=7.8964 cmp_logits=0.0675, sampling=6.9945, total=15.2485
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4960 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2887, fwd=7.8700 cmp_logits=0.0670, sampling=7.0083, total=15.2349
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4810 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2871, fwd=7.8638 cmp_logits=0.0675, sampling=7.0107, total=15.2299
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5046 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2675, fwd=8.0943 cmp_logits=0.0696, sampling=6.5808, total=15.0132
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.4
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.2180 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:21:03 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 04:21:03 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:21:03 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2661, fwd=8.0352 cmp_logits=0.0694, sampling=6.5846, total=14.9562
INFO 10-04 04:21:03 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.3
INFO 10-04 04:21:03 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.1789 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.708981, last_token_time=1728015661.2731578, first_scheduled_time=1728015657.7391183, first_token_time=1728015659.3553002, time_in_queue=0.030137300491333008, finished_time=1728015661.2731292, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.709967, last_token_time=1728015662.3992953, first_scheduled_time=1728015657.7391183, first_token_time=1728015659.4061186, time_in_queue=0.029151439666748047, finished_time=1728015662.3992383, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.7118473, last_token_time=1728015661.8815556, first_scheduled_time=1728015659.3557765, first_token_time=1728015661.005968, time_in_queue=1.6439292430877686, finished_time=1728015661.8815203, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.712605, last_token_time=1728015662.8656487, first_scheduled_time=1728015659.4065385, first_token_time=1728015661.0984583, time_in_queue=1.6939334869384766, finished_time=1728015662.86561, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.7147708, last_token_time=1728015662.884766, first_scheduled_time=1728015661.0524511, first_token_time=1728015661.1909752, time_in_queue=3.3376803398132324, finished_time=1728015662.8847315, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.7170036, last_token_time=1728015661.6986496, first_scheduled_time=1728015661.143569, first_token_time=1728015661.2312107, time_in_queue=3.426565408706665, finished_time=1728015661.69863, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.71746, last_token_time=1728015661.6986496, first_scheduled_time=1728015661.1914563, first_token_time=1728015661.2312107, time_in_queue=3.47399640083313, finished_time=1728015661.6986344, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.7179186, last_token_time=1728015663.020238, first_scheduled_time=1728015661.1914563, first_token_time=1728015661.3702397, time_in_queue=3.4735376834869385, finished_time=1728015663.020216, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.7201374, last_token_time=1728015663.513556, first_scheduled_time=1728015661.3202522, first_token_time=1728015661.514106, time_in_queue=3.6001148223876953, finished_time=1728015663.5135472, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.7235503, last_token_time=1728015663.3106253, first_scheduled_time=1728015661.466826, first_token_time=1728015661.6543083, time_in_queue=3.7432756423950195, finished_time=1728015663.3106184, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.7263038, last_token_time=1728015662.2990735, first_scheduled_time=1728015661.607392, first_token_time=1728015661.698352, time_in_queue=3.8810882568359375, finished_time=1728015662.29904, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.7268639, last_token_time=1728015662.5144005, first_scheduled_time=1728015661.654885, first_token_time=1728015661.7442224, time_in_queue=3.928021192550659, finished_time=1728015662.5143743, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.7277484, last_token_time=1728015662.884766, first_scheduled_time=1728015661.6990626, first_token_time=1728015661.791507, time_in_queue=3.9713141918182373, finished_time=1728015662.8847513, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.729312, last_token_time=1728015662.9867935, first_scheduled_time=1728015661.7447982, first_token_time=1728015661.881246, time_in_queue=4.015486240386963, finished_time=1728015662.9867835, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.7310076, last_token_time=1728015662.4185903, first_scheduled_time=1728015661.8362038, first_token_time=1728015661.9248533, time_in_queue=4.105196237564087, finished_time=1728015662.4185722, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.7315817, last_token_time=1728015662.70132, first_scheduled_time=1728015661.881924, first_token_time=1728015661.9700959, time_in_queue=4.150342226028442, finished_time=1728015662.7013102, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.732632, last_token_time=1728015662.62699, first_scheduled_time=1728015661.925531, first_token_time=1728015662.0143816, time_in_queue=4.192898988723755, finished_time=1728015662.6269798, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.7335663, last_token_time=1728015662.278945, first_scheduled_time=1728015661.9708037, first_token_time=1728015662.0143816, time_in_queue=4.237237453460693, finished_time=1728015662.278935, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.7338474, last_token_time=1728015663.1998348, first_scheduled_time=1728015661.9708037, first_token_time=1728015662.1097612, time_in_queue=4.236956357955933, finished_time=1728015663.1998286, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015657.7359478, last_token_time=1728015663.5442896, first_scheduled_time=1728015662.060338, first_token_time=1728015662.2584074, time_in_queue=4.324390172958374, finished_time=1728015663.544287, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 15868
End-to-End latency: 5.84 seconds
Throughput: 3.43 requests/s, 2719.21 tokens/s
Per_token_time: 0.368 ms

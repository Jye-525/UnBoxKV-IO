Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=128, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=16384, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=16384, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Selected required requests 20
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Running vLLM with default batching strategy. max_num_batched_tokens=16384, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'finetuned', 'original_max_position_embeddings', 'type'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 04:13:40 config.py:654] [SchedulerConfig] max_num_batched_tokens: 16384 chunked_prefill_enabled: False
INFO 10-04 04:13:40 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 04:13:41 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 04:13:46 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 04:13:49 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 04:13:49 model_runner.py:183] Loaded model: 
INFO 10-04 04:13:49 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 04:13:49 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 04:13:49 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 04:13:49 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 04:13:49 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 04:13:49 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 04:13:49 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 04:13:49 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 04:13:49 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 04:13:49 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 04:13:49 model_runner.py:183]         )
INFO 10-04 04:13:49 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 04:13:49 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 04:13:49 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 04:13:49 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 04:13:49 model_runner.py:183]         )
INFO 10-04 04:13:49 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 04:13:49 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 04:13:49 model_runner.py:183]       )
INFO 10-04 04:13:49 model_runner.py:183]     )
INFO 10-04 04:13:49 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 04:13:49 model_runner.py:183]   )
INFO 10-04 04:13:49 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 04:13:49 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 04:13:49 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 04:13:49 model_runner.py:183] )
INFO 10-04 04:13:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16384]), positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 4096]) residual=None
INFO 10-04 04:13:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 5.4102, fwd=363.2696 cmp_logits=0.2434, sampling=855.4010, total=1224.3261
INFO 10-04 04:13:50 worker.py:164] Peak: 14.545 GB, Initial: 38.980 GB, Free: 24.435 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 22.879 GB
INFO 10-04 04:13:51 gpu_executor.py:117] # GPU blocks: 2928, # CPU blocks: 8192
INFO 10-04 04:14:22 worker.py:189] _init_cache_engine took 22.8750 GB
INFO 10-04 04:14:22 scheduler.py:307] Scheduler initialized with prompt limit: 16384
INFO 10-04 04:14:22 Start warmup...
INFO 10-04 04:14:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:14:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9055, fwd=11.5478 cmp_logits=72.9442, sampling=0.9706, total=86.3700
INFO 10-04 04:14:22 metrics.py:335] Avg prompt throughput: 11005.8 tokens/s, Avg generation throughput: 10.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 93.0
INFO 10-04 04:14:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.7836 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3033, fwd=49.8023 cmp_logits=0.1221, sampling=6.1853, total=56.4146
INFO 10-04 04:14:22 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 57.3
INFO 10-04 04:14:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 56.8388 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3111, fwd=8.2109 cmp_logits=0.0763, sampling=6.6895, total=15.2888
INFO 10-04 04:14:22 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 04:14:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5914 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2856, fwd=8.0407 cmp_logits=0.0722, sampling=6.8233, total=15.2237
INFO 10-04 04:14:22 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:14:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4645 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2825, fwd=8.2457 cmp_logits=0.0765, sampling=6.6535, total=15.2597
INFO 10-04 04:14:22 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:14:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4850 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2856, fwd=8.2850 cmp_logits=0.0732, sampling=6.5982, total=15.2438
INFO 10-04 04:14:22 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:14:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4545 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2942, fwd=8.2562 cmp_logits=0.0782, sampling=6.6493, total=15.2791
INFO 10-04 04:14:22 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:14:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4886 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2775, fwd=8.3432 cmp_logits=0.0720, sampling=6.4826, total=15.1765
INFO 10-04 04:14:22 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:14:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3954 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2780, fwd=8.0926 cmp_logits=0.0708, sampling=6.7756, total=15.2183
INFO 10-04 04:14:22 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:14:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4004 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2723, fwd=8.0950 cmp_logits=0.0677, sampling=6.7511, total=15.1875
INFO 10-04 04:14:22 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-04 04:14:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3837 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:27 Start benchmarking...
INFO 10-04 04:14:27 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([15102]), positions.shape=torch.Size([15102]) hidden_states.shape=torch.Size([15102, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 4.8740, fwd=11.1983 cmp_logits=0.1042, sampling=904.9871, total=921.1643
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 15696.9 tokens/s, Avg generation throughput: 20.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.5%, CPU KV cache usage: 0.0%, Interval(ms): 962.1
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 922.4246 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5670, fwd=7.9665 cmp_logits=0.0713, sampling=11.6811, total=20.2868
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 934.8 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.0314 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5512, fwd=7.6241 cmp_logits=0.0710, sampling=11.9321, total=20.1795
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9141 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5493, fwd=7.6230 cmp_logits=0.0682, sampling=11.8647, total=20.1061
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 950.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8206 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5641, fwd=7.7076 cmp_logits=0.0699, sampling=11.8170, total=20.1595
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8826 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5498, fwd=7.6265 cmp_logits=0.0699, sampling=11.9092, total=20.1564
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 948.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.8%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8821 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([20]), positions.shape=torch.Size([20]) hidden_states.shape=torch.Size([20, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5541, fwd=7.7107 cmp_logits=0.0694, sampling=11.7943, total=20.1297
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 947.3 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.9036 resumed_reqs=0, running_reqs=20 raw_running=20
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5376, fwd=7.6971 cmp_logits=0.0832, sampling=11.7445, total=20.0636
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 904.7 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7670 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([19]), positions.shape=torch.Size([19]) hidden_states.shape=torch.Size([19, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5398, fwd=7.6261 cmp_logits=0.0725, sampling=11.8198, total=20.0591
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 905.4 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7822 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([18]), positions.shape=torch.Size([18]) hidden_states.shape=torch.Size([18, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5302, fwd=7.7031 cmp_logits=0.0815, sampling=11.6649, total=19.9807
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 861.1 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6738 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([18]), positions.shape=torch.Size([18]) hidden_states.shape=torch.Size([18, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5240, fwd=7.6301 cmp_logits=0.0701, sampling=11.7416, total=19.9668
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 862.5 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.9
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6664 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4935, fwd=7.7360 cmp_logits=0.0820, sampling=11.0159, total=19.3284
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 792.0 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9614 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5035, fwd=7.6430 cmp_logits=0.0706, sampling=11.1039, total=19.3219
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 792.4 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9664 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5000, fwd=7.6458 cmp_logits=0.0696, sampling=11.0943, total=19.3105
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 793.9 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9559 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5012, fwd=7.6566 cmp_logits=0.0687, sampling=11.1203, total=19.3474
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 792.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9938 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4914, fwd=7.6807 cmp_logits=0.0806, sampling=11.0128, total=19.2666
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 746.2 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8812 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4718, fwd=7.6730 cmp_logits=0.0801, sampling=10.5643, total=18.7902
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 714.5 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.1%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3825 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 04:14:28 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4680, fwd=7.6606 cmp_logits=0.0691, sampling=10.5646, total=18.7633
INFO 10-04 04:14:28 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 716.5 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.2%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 04:14:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3534 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 04:14:28 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4711, fwd=7.6530 cmp_logits=0.0699, sampling=10.5917, total=18.7864
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 714.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.3%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3644 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4652, fwd=7.6110 cmp_logits=0.0689, sampling=10.6332, total=18.7795
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 715.8 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3715 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4549, fwd=7.7040 cmp_logits=0.0808, sampling=10.4740, total=18.7149
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 666.9 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.2797 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4618, fwd=7.7224 cmp_logits=0.0682, sampling=10.4783, total=18.7316
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 667.2 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.2974 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4628, fwd=7.7441 cmp_logits=0.0703, sampling=10.4363, total=18.7144
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 668.0 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.2764 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4578, fwd=7.7224 cmp_logits=0.0694, sampling=10.4883, total=18.7387
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 665.8 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.5%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3393 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4673, fwd=7.6993 cmp_logits=0.0691, sampling=10.4976, total=18.7345
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 666.1 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.3%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3281 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4270, fwd=7.7565 cmp_logits=0.0806, sampling=9.8910, total=18.1563
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 582.4 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.6596 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4239, fwd=7.6976 cmp_logits=0.0687, sampling=9.9719, total=18.1630
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 583.9 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.8
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.6627 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4265, fwd=7.7260 cmp_logits=0.0684, sampling=9.9461, total=18.1680
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 583.5 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.6741 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4365, fwd=7.7376 cmp_logits=0.0694, sampling=9.9449, total=18.1894
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 582.8 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.6951 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4311, fwd=7.7975 cmp_logits=0.0691, sampling=9.9151, total=18.2137
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 581.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7328 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4120, fwd=7.7605 cmp_logits=0.0801, sampling=9.7113, total=17.9648
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 536.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4338 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4137, fwd=7.7798 cmp_logits=0.0682, sampling=9.6810, total=17.9434
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 537.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4214 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4106, fwd=7.7674 cmp_logits=0.0687, sampling=9.7063, total=17.9536
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 537.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4326 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4151, fwd=7.7672 cmp_logits=0.0682, sampling=9.7589, total=18.0104
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 535.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4941 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4265, fwd=7.7031 cmp_logits=0.0689, sampling=9.8014, total=18.0008
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 536.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4734 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4182, fwd=7.7381 cmp_logits=0.0701, sampling=9.7778, total=18.0051
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 534.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4944 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3998, fwd=7.7844 cmp_logits=0.0808, sampling=9.3212, total=17.5874
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0330 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3972, fwd=7.7260 cmp_logits=0.0687, sampling=9.3882, total=17.5810
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0240 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3972, fwd=7.7260 cmp_logits=0.0687, sampling=9.3567, total=17.5493
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 495.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9975 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4075, fwd=7.7031 cmp_logits=0.0703, sampling=9.4223, total=17.6039
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.7%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0478 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4017, fwd=7.7505 cmp_logits=0.0677, sampling=9.3663, total=17.5872
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.7%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0471 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3982, fwd=7.7381 cmp_logits=0.0682, sampling=9.7969, total=18.0025
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 482.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.7%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4453 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3974, fwd=7.7314 cmp_logits=0.0687, sampling=9.3732, total=17.5714
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 495.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.7%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0097 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3994, fwd=7.7159 cmp_logits=0.0687, sampling=9.3825, total=17.5672
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 494.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.7%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0266 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3831, fwd=7.7815 cmp_logits=0.0794, sampling=9.0020, total=17.2470
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 447.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6632 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3836, fwd=7.7863 cmp_logits=0.0679, sampling=9.0075, total=17.2460
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 448.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6694 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3874, fwd=7.6993 cmp_logits=0.0687, sampling=9.0759, total=17.2324
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 448.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6840 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3846, fwd=7.7336 cmp_logits=0.0682, sampling=9.0344, total=17.2219
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 448.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6542 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3674, fwd=7.8180 cmp_logits=0.0806, sampling=8.5731, total=16.8400
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2307 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3633, fwd=7.7660 cmp_logits=0.0677, sampling=8.6346, total=16.8326
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2141 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3669, fwd=7.7519 cmp_logits=0.0682, sampling=8.6551, total=16.8431
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2360 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3669, fwd=7.7178 cmp_logits=0.0684, sampling=8.7001, total=16.8540
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2467 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3674, fwd=7.7786 cmp_logits=0.0679, sampling=8.6823, total=16.8972
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2815 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3705, fwd=7.7651 cmp_logits=0.0691, sampling=8.6594, total=16.8650
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2493 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3655, fwd=7.7658 cmp_logits=0.0677, sampling=8.6606, total=16.8607
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2503 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3679, fwd=7.6859 cmp_logits=0.0682, sampling=8.7471, total=16.8703
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2770 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3524, fwd=7.7851 cmp_logits=0.0799, sampling=8.2319, total=16.4502
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 352.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8266 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3376, fwd=7.7906 cmp_logits=0.0794, sampling=7.7515, total=15.9595
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 303.2 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 16.5
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.2990 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3211, fwd=7.7429 cmp_logits=0.0796, sampling=7.4461, total=15.5907
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.3 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8844 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3221, fwd=7.6978 cmp_logits=0.0672, sampling=7.4947, total=15.5828
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 249.6 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%, Interval(ms): 16.0
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8746 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3202, fwd=7.6785 cmp_logits=0.0691, sampling=7.5002, total=15.5690
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 249.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.0
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8780 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3035, fwd=7.7329 cmp_logits=0.0811, sampling=7.2381, total=15.3565
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6112 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3030, fwd=7.6814 cmp_logits=0.0670, sampling=7.3028, total=15.3551
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6116 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3033, fwd=7.7052 cmp_logits=0.0665, sampling=7.2651, total=15.3413
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5990 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3052, fwd=7.7200 cmp_logits=0.0684, sampling=7.2362, total=15.3306
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5895 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3014, fwd=7.7198 cmp_logits=0.0675, sampling=7.2563, total=15.3456
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6047 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3030, fwd=7.6997 cmp_logits=0.0675, sampling=7.2823, total=15.3534
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6095 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3033, fwd=7.7322 cmp_logits=0.0679, sampling=7.2343, total=15.3387
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5983 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3035, fwd=7.7610 cmp_logits=0.0672, sampling=7.2272, total=15.3599
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.6%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6190 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3033, fwd=7.7477 cmp_logits=0.0682, sampling=7.2563, total=15.3761
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.6%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6672 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3037, fwd=7.7000 cmp_logits=0.0670, sampling=7.2951, total=15.3668
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.6%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6243 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3004, fwd=7.7114 cmp_logits=0.0679, sampling=7.2868, total=15.3675
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.6%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6240 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3030, fwd=7.6807 cmp_logits=0.0670, sampling=7.3097, total=15.3613
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6438 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2851, fwd=7.7436 cmp_logits=0.0784, sampling=7.1108, total=15.2192
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4369 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:14:29 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2844, fwd=7.7484 cmp_logits=0.0672, sampling=7.0930, total=15.1939
INFO 10-04 04:14:29 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:14:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4393 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:14:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2625, fwd=7.9598 cmp_logits=0.0672, sampling=7.0899, total=15.3801
INFO 10-04 04:14:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:14:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5573 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2689, fwd=7.8766 cmp_logits=0.0756, sampling=7.1549, total=15.3768
INFO 10-04 04:14:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:14:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5549 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2632, fwd=7.9184 cmp_logits=0.0682, sampling=7.0846, total=15.3353
INFO 10-04 04:14:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:14:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5120 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2637, fwd=7.8566 cmp_logits=0.0677, sampling=7.2162, total=15.4052
INFO 10-04 04:14:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:14:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5818 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2623, fwd=7.8588 cmp_logits=0.0677, sampling=7.1559, total=15.3456
INFO 10-04 04:14:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:14:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5230 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2618, fwd=7.8650 cmp_logits=0.0679, sampling=7.1568, total=15.3527
INFO 10-04 04:14:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:14:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5270 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2642, fwd=7.9212 cmp_logits=0.0675, sampling=7.1437, total=15.3975
INFO 10-04 04:14:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:14:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5730 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2627, fwd=7.9072 cmp_logits=0.0679, sampling=7.1309, total=15.3697
INFO 10-04 04:14:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:14:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5449 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2615, fwd=7.8766 cmp_logits=0.0679, sampling=7.1390, total=15.3461
INFO 10-04 04:14:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:14:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5251 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2639, fwd=7.8480 cmp_logits=0.0672, sampling=7.2393, total=15.4197
INFO 10-04 04:14:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:14:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5957 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2637, fwd=7.8793 cmp_logits=0.0675, sampling=7.1959, total=15.4068
INFO 10-04 04:14:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:14:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5840 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2608, fwd=8.1718 cmp_logits=0.0677, sampling=6.9425, total=15.4438
INFO 10-04 04:14:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:14:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6217 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2680, fwd=7.8688 cmp_logits=0.0682, sampling=7.1974, total=15.4033
INFO 10-04 04:14:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:14:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5799 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:14:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:14:30 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2632, fwd=7.8628 cmp_logits=0.0679, sampling=7.2129, total=15.4078
INFO 10-04 04:14:30 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:14:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6076 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.6812701, last_token_time=1728015268.8020918, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.029979467391967773, finished_time=1728015268.802016, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.6822145, last_token_time=1728015269.326685, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.029035091400146484, finished_time=1728015269.3266435, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.6837823, last_token_time=1728015269.0229852, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.02746725082397461, finished_time=1728015269.0229354, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.6845384, last_token_time=1728015269.7169454, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.026711225509643555, finished_time=1728015269.7169242, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.6867507, last_token_time=1728015269.700456, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.024498939514160156, finished_time=1728015269.7004368, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.688845, last_token_time=1728015268.8438652, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.02240467071533203, finished_time=1728015268.8438115, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.689286, last_token_time=1728015268.8438652, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.02196359634399414, finished_time=1728015268.8438156, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.6897435, last_token_time=1728015269.7651064, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.021506071090698242, finished_time=1728015269.765089, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.6920643, last_token_time=1728015270.2054253, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.019185304641723633, finished_time=1728015270.2054226, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.6954834, last_token_time=1728015269.9543405, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.015766143798828125, finished_time=1728015269.954333, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.6981797, last_token_time=1728015268.9246013, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.013069868087768555, finished_time=1728015268.9245682, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.6987438, last_token_time=1728015269.1204665, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.012505769729614258, finished_time=1728015269.1204414, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.6996725, last_token_time=1728015269.4727414, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.011577129364013672, finished_time=1728015269.472727, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.7012253, last_token_time=1728015269.5441158, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.010024309158325195, finished_time=1728015269.5441053, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.703004, last_token_time=1728015268.944702, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.008245706558227539, finished_time=1728015268.944685, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.7035813, last_token_time=1728015269.2148297, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.007668256759643555, finished_time=1728015269.2148197, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.7046385, last_token_time=1728015269.1204665, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.0066111087799072266, finished_time=1728015269.120457, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.705528, last_token_time=1728015268.7601032, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.005721569061279297, finished_time=1728015268.7600937, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.705813, last_token_time=1728015269.6834378, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.00543665885925293, finished_time=1728015269.6834319, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015267.7078881, last_token_time=1728015269.9855413, first_scheduled_time=1728015267.7112496, first_token_time=1728015268.6326613, time_in_queue=0.0033614635467529297, finished_time=1728015269.9855397, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 15868
End-to-End latency: 2.52 seconds
Throughput: 7.92 requests/s, 6285.95 tokens/s
Per_token_time: 0.159 ms

Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=20, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=128, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=16384, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=1024, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 20 requests, requiring 20 requests.
Original order of the requests
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Selected required requests 20
User Requests: prompt_len=166, output_len=9, sequence_len=175...
User Requests: prompt_len=719, output_len=36, sequence_len=755...
User Requests: prompt_len=388, output_len=20, sequence_len=408...
User Requests: prompt_len=1152, output_len=58, sequence_len=1210...
User Requests: prompt_len=1136, output_len=57, sequence_len=1193...
User Requests: prompt_len=215, output_len=11, sequence_len=226...
User Requests: prompt_len=218, output_len=11, sequence_len=229...
User Requests: prompt_len=1203, output_len=61, sequence_len=1264...
User Requests: prompt_len=1773, output_len=89, sequence_len=1862...
User Requests: prompt_len=1456, output_len=73, sequence_len=1529...
User Requests: prompt_len=291, output_len=15, sequence_len=306...
User Requests: prompt_len=483, output_len=25, sequence_len=508...
User Requests: prompt_len=864, output_len=44, sequence_len=908...
User Requests: prompt_len=941, output_len=48, sequence_len=989...
User Requests: prompt_len=304, output_len=16, sequence_len=320...
User Requests: prompt_len=591, output_len=30, sequence_len=621...
User Requests: prompt_len=487, output_len=25, sequence_len=512...
User Requests: prompt_len=121, output_len=7, sequence_len=128...
User Requests: prompt_len=1114, output_len=56, sequence_len=1170...
User Requests: prompt_len=1480, output_len=75, sequence_len=1555...
Chunked prefill is enabled. max_num_batched_tokens=1024, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings', 'type', 'finetuned'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 04:18:49 config.py:654] [SchedulerConfig] max_num_batched_tokens: 1024 chunked_prefill_enabled: True
INFO 10-04 04:18:49 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 04:18:50 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 04:18:55 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 04:18:58 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 04:18:58 model_runner.py:183] Loaded model: 
INFO 10-04 04:18:58 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 04:18:58 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 04:18:58 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 04:18:58 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 04:18:58 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 04:18:58 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 04:18:58 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 04:18:58 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 04:18:58 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 04:18:58 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 04:18:58 model_runner.py:183]         )
INFO 10-04 04:18:58 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 04:18:58 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 04:18:58 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 04:18:58 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 04:18:58 model_runner.py:183]         )
INFO 10-04 04:18:58 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 04:18:58 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 04:18:58 model_runner.py:183]       )
INFO 10-04 04:18:58 model_runner.py:183]     )
INFO 10-04 04:18:58 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 04:18:58 model_runner.py:183]   )
INFO 10-04 04:18:58 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 04:18:58 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 04:18:58 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 04:18:58 model_runner.py:183] )
INFO 10-04 04:18:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:18:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 21.4481, fwd=427.6376 cmp_logits=0.2444, sampling=106.7550, total=556.0870
INFO 10-04 04:18:59 worker.py:164] Peak: 13.307 GB, Initial: 38.980 GB, Free: 25.673 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.118 GB
INFO 10-04 04:18:59 gpu_executor.py:117] # GPU blocks: 3087, # CPU blocks: 8192
INFO 10-04 04:19:30 worker.py:189] _init_cache_engine took 24.1250 GB
INFO 10-04 04:19:30 scheduler.py:307] Scheduler initialized with prompt limit: 16384
INFO 10-04 04:19:30 Start warmup...
INFO 10-04 04:19:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:30 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:19:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8910, fwd=1897.8825 cmp_logits=71.4777, sampling=0.9418, total=1971.1950
INFO 10-04 04:19:32 metrics.py:335] Avg prompt throughput: 517.6 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 1978.4
INFO 10-04 04:19:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1971.7071 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:19:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:19:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3018, fwd=49.5923 cmp_logits=0.1178, sampling=6.7377, total=56.7522
INFO 10-04 04:19:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 57.5
INFO 10-04 04:19:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 57.1833 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:19:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:19:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3130, fwd=7.9978 cmp_logits=0.0815, sampling=7.2095, total=15.6038
INFO 10-04 04:19:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 61.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 04:19:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9156 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:19:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:19:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2818, fwd=7.9491 cmp_logits=0.0756, sampling=7.2179, total=15.5258
INFO 10-04 04:19:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.0
INFO 10-04 04:19:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7816 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:19:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:19:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2782, fwd=7.9060 cmp_logits=0.0746, sampling=7.3600, total=15.6205
INFO 10-04 04:19:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.0
INFO 10-04 04:19:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8510 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:19:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:19:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2704, fwd=7.8249 cmp_logits=0.0725, sampling=7.3352, total=15.5044
INFO 10-04 04:19:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 04:19:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7251 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:19:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:19:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2728, fwd=7.8545 cmp_logits=0.0775, sampling=7.3137, total=15.5196
INFO 10-04 04:19:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 04:19:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7409 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:19:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:19:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2680, fwd=7.8118 cmp_logits=0.0727, sampling=7.3385, total=15.4917
INFO 10-04 04:19:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 04:19:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7363 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:19:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:19:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2623, fwd=7.8404 cmp_logits=0.0708, sampling=6.5460, total=14.7202
INFO 10-04 04:19:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.1
INFO 10-04 04:19:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.9167 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:19:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:19:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2596, fwd=7.8428 cmp_logits=0.0706, sampling=6.1519, total=14.3256
INFO 10-04 04:19:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 04:19:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5409 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:19:37 Start benchmarking...
INFO 10-04 04:19:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:37 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:19:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8516, fwd=1596.7460 cmp_logits=0.1478, sampling=71.2240, total=1668.9701
INFO 10-04 04:19:39 metrics.py:335] Avg prompt throughput: 599.1 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 17 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%, Interval(ms): 1709.3
INFO 10-04 04:19:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1669.5318 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:19:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:19:41 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8163, fwd=1592.9425 cmp_logits=0.1421, sampling=69.4973, total=1663.3992
INFO 10-04 04:19:41 metrics.py:335] Avg prompt throughput: 614.2 tokens/s, Avg generation throughput: 1.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%, Interval(ms): 1664.1
INFO 10-04 04:19:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1663.8739 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:19:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:41 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:19:41 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8519, fwd=14.3793 cmp_logits=0.0970, sampling=78.6347, total=93.9641
INFO 10-04 04:19:41 metrics.py:335] Avg prompt throughput: 10791.1 tokens/s, Avg generation throughput: 42.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 15 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%, Interval(ms): 94.6
INFO 10-04 04:19:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 94.3975 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 04:19:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:41 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:19:41 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8733, fwd=14.3056 cmp_logits=0.0932, sampling=61.9354, total=77.2088
INFO 10-04 04:19:41 metrics.py:335] Avg prompt throughput: 13089.2 tokens/s, Avg generation throughput: 89.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 10.7%, CPU KV cache usage: 0.0%, Interval(ms): 77.9
INFO 10-04 04:19:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.7502 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:19:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:41 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:19:41 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8390, fwd=14.3375 cmp_logits=0.0772, sampling=59.2368, total=74.4913
INFO 10-04 04:19:41 metrics.py:335] Avg prompt throughput: 13530.9 tokens/s, Avg generation throughput: 93.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 10.7%, CPU KV cache usage: 0.0%, Interval(ms): 75.2
INFO 10-04 04:19:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 74.9824 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:19:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:41 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:19:41 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8686, fwd=14.3411 cmp_logits=0.0899, sampling=72.7558, total=88.0558
INFO 10-04 04:19:41 metrics.py:335] Avg prompt throughput: 11461.8 tokens/s, Avg generation throughput: 90.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 14.3%, CPU KV cache usage: 0.0%, Interval(ms): 88.7
INFO 10-04 04:19:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 88.5589 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:19:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:41 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:19:41 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8657, fwd=14.2591 cmp_logits=0.0901, sampling=71.8524, total=87.0678
INFO 10-04 04:19:41 metrics.py:335] Avg prompt throughput: 11574.3 tokens/s, Avg generation throughput: 102.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 87.8
INFO 10-04 04:19:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 87.5950 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:19:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:41 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:19:41 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8683, fwd=14.3611 cmp_logits=0.0753, sampling=62.0031, total=77.3087
INFO 10-04 04:19:41 metrics.py:335] Avg prompt throughput: 13014.8 tokens/s, Avg generation throughput: 115.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 78.0
INFO 10-04 04:19:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.8124 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:19:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:41 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:19:41 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9081, fwd=14.4093 cmp_logits=0.0904, sampling=67.0044, total=82.4132
INFO 10-04 04:19:41 metrics.py:335] Avg prompt throughput: 12199.5 tokens/s, Avg generation throughput: 132.2 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 18.5%, CPU KV cache usage: 0.0%, Interval(ms): 83.2
INFO 10-04 04:19:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 83.0257 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 04:19:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:41 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:19:41 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9329, fwd=14.5009 cmp_logits=0.0904, sampling=63.8585, total=79.3834
INFO 10-04 04:19:41 metrics.py:335] Avg prompt throughput: 12626.4 tokens/s, Avg generation throughput: 149.4 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 80.3
INFO 10-04 04:19:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 80.0445 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 04:19:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:41 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:19:41 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9270, fwd=14.3356 cmp_logits=0.0892, sampling=60.8430, total=76.1952
INFO 10-04 04:19:41 metrics.py:335] Avg prompt throughput: 13138.1 tokens/s, Avg generation throughput: 168.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 22.9%, CPU KV cache usage: 0.0%, Interval(ms): 77.0
INFO 10-04 04:19:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.8375 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 04:19:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:41 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9532, fwd=14.3576 cmp_logits=0.0882, sampling=60.4825, total=75.8822
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 13164.0 tokens/s, Avg generation throughput: 195.3 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 25.1%, CPU KV cache usage: 0.0%, Interval(ms): 76.8
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.6056 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.0111, fwd=14.2586 cmp_logits=0.0935, sampling=62.4630, total=77.8272
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 12804.3 tokens/s, Avg generation throughput: 215.7 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 27.7%, CPU KV cache usage: 0.0%, Interval(ms): 78.8
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 78.6014 resumed_reqs=0, running_reqs=18 raw_running=18
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9985, fwd=14.4117 cmp_logits=0.0889, sampling=65.2316, total=80.7316
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 12316.4 tokens/s, Avg generation throughput: 220.2 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.8%, CPU KV cache usage: 0.0%, Interval(ms): 81.8
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 81.5537 resumed_reqs=0, running_reqs=19 raw_running=19
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([898]), positions.shape=torch.Size([898]) hidden_states.shape=torch.Size([898, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9487, fwd=14.5001 cmp_logits=0.0749, sampling=68.2809, total=83.8056
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 10392.1 tokens/s, Avg generation throughput: 200.3 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.9%, CPU KV cache usage: 0.0%, Interval(ms): 84.9
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 84.5659 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5481, fwd=7.7009 cmp_logits=0.0739, sampling=11.8074, total=20.1309
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 807.2 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.1
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.8540 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5114, fwd=7.6826 cmp_logits=0.0732, sampling=11.7927, total=20.0605
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 810.9 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7613 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5128, fwd=7.6790 cmp_logits=0.0727, sampling=11.7993, total=20.0646
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 810.6 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.9%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7779 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([17]), positions.shape=torch.Size([17]) hidden_states.shape=torch.Size([17, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5233, fwd=7.6659 cmp_logits=0.0725, sampling=11.7738, total=20.0362
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 811.0 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7665 resumed_reqs=0, running_reqs=17 raw_running=17
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5081, fwd=7.7946 cmp_logits=0.0854, sampling=10.9453, total=19.3341
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 788.8 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.3
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0257 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16]), positions.shape=torch.Size([16]) hidden_states.shape=torch.Size([16, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.5040, fwd=7.6933 cmp_logits=0.0737, sampling=11.0669, total=19.3391
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 790.8 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0365 resumed_reqs=0, running_reqs=16 raw_running=16
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4861, fwd=7.7252 cmp_logits=0.0730, sampling=10.9606, total=19.2459
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 745.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9056 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([15]), positions.shape=torch.Size([15]) hidden_states.shape=torch.Size([15, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4847, fwd=7.6940 cmp_logits=0.0744, sampling=10.9763, total=19.2299
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 746.2 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.3%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9096 resumed_reqs=0, running_reqs=15 raw_running=15
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4654, fwd=7.6981 cmp_logits=0.0839, sampling=10.9611, total=19.2096
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 697.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8483 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4797, fwd=7.6821 cmp_logits=0.0722, sampling=10.9730, total=19.2082
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 697.4 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8498 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4721, fwd=7.6637 cmp_logits=0.0727, sampling=10.9942, total=19.2037
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 699.2 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.0
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8338 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([14]), positions.shape=torch.Size([14]) hidden_states.shape=torch.Size([14, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4721, fwd=7.7460 cmp_logits=0.0730, sampling=10.9556, total=19.2475
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 697.2 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8951 resumed_reqs=0, running_reqs=14 raw_running=14
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4535, fwd=7.7252 cmp_logits=0.0725, sampling=10.5338, total=18.7860
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 662.9 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3994 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4625, fwd=7.7155 cmp_logits=0.0722, sampling=10.5100, total=18.7614
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 664.6 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3782 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4551, fwd=7.7031 cmp_logits=0.0725, sampling=10.5159, total=18.7483
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 663.6 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3701 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4551, fwd=7.6323 cmp_logits=0.0737, sampling=10.5999, total=18.7621
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 664.7 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3737 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4594, fwd=7.6563 cmp_logits=0.0727, sampling=10.5703, total=18.7597
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 665.0 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3655 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4609, fwd=7.6971 cmp_logits=0.0725, sampling=10.5312, total=18.7624
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 665.0 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3686 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([13]), positions.shape=torch.Size([13]) hidden_states.shape=torch.Size([13, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4559, fwd=7.6876 cmp_logits=0.0727, sampling=10.5925, total=18.8098
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 663.2 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.4216 resumed_reqs=0, running_reqs=13 raw_running=13
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4458, fwd=7.6818 cmp_logits=0.0737, sampling=10.1748, total=18.3771
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 624.7 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.9%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9633 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([12]), positions.shape=torch.Size([12]) hidden_states.shape=torch.Size([12, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4377, fwd=7.6520 cmp_logits=0.0727, sampling=10.2184, total=18.3821
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 626.1 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9881 resumed_reqs=0, running_reqs=12 raw_running=12
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([11]), positions.shape=torch.Size([11]) hidden_states.shape=torch.Size([11, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4332, fwd=7.6907 cmp_logits=0.0730, sampling=9.8605, total=18.0583
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 584.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.8
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.6183 resumed_reqs=0, running_reqs=11 raw_running=11
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4177, fwd=7.6632 cmp_logits=0.0868, sampling=9.7189, total=17.8876
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 537.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4157 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4129, fwd=7.6580 cmp_logits=0.0722, sampling=9.7454, total=17.8893
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 537.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4448 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4137, fwd=7.6764 cmp_logits=0.0713, sampling=9.7475, total=17.9102
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 537.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4298 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4222, fwd=7.6783 cmp_logits=0.0720, sampling=9.7129, total=17.8866
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 537.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4240 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3994, fwd=7.6680 cmp_logits=0.0718, sampling=9.4972, total=17.6377
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 490.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1348 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4058, fwd=7.6804 cmp_logits=0.0720, sampling=9.4388, total=17.5979
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0793 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4017, fwd=7.6711 cmp_logits=0.0713, sampling=9.4600, total=17.6051
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 492.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1236 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3991, fwd=7.6873 cmp_logits=0.0727, sampling=9.4199, total=17.5798
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0616 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4005, fwd=7.7164 cmp_logits=0.0720, sampling=9.4106, total=17.6005
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0814 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3979, fwd=7.6978 cmp_logits=0.0718, sampling=9.4161, total=17.5846
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0743 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3996, fwd=7.6916 cmp_logits=0.0720, sampling=9.4318, total=17.5960
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0871 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4020, fwd=7.6993 cmp_logits=0.0722, sampling=9.4194, total=17.5936
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 492.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1148 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4005, fwd=7.6711 cmp_logits=0.0715, sampling=9.4655, total=17.6094
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 492.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0905 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:19:42 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4044, fwd=7.6339 cmp_logits=0.0715, sampling=9.4962, total=17.6067
INFO 10-04 04:19:42 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 493.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 04:19:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0881 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:19:42 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:42 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4005, fwd=7.6761 cmp_logits=0.0715, sampling=9.4790, total=17.6280
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 491.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1479 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4084, fwd=7.8216 cmp_logits=0.0720, sampling=9.3393, total=17.6423
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 491.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1444 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3836, fwd=7.7116 cmp_logits=0.0727, sampling=9.0744, total=17.2434
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 446.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.6%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7310 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3917, fwd=7.6842 cmp_logits=0.0710, sampling=9.1295, total=17.2775
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 447.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.6%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7326 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3831, fwd=7.6408 cmp_logits=0.0722, sampling=9.1388, total=17.2360
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 448.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6873 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3850, fwd=7.6811 cmp_logits=0.0715, sampling=9.1062, total=17.2448
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 447.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7026 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3834, fwd=7.6842 cmp_logits=0.0710, sampling=9.0938, total=17.2331
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 447.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7093 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3731, fwd=7.7007 cmp_logits=0.0710, sampling=8.7695, total=16.9153
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 398.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.6
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3733 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3669, fwd=7.7105 cmp_logits=0.0706, sampling=8.7340, total=16.8829
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 399.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3426 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3340, fwd=7.7448 cmp_logits=0.0837, sampling=8.0101, total=16.1736
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5269 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3345, fwd=7.7727 cmp_logits=0.0703, sampling=7.9753, total=16.1536
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5086 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3335, fwd=7.6752 cmp_logits=0.0706, sampling=8.0805, total=16.1607
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5150 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3357, fwd=7.6668 cmp_logits=0.0703, sampling=8.0948, total=16.1684
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5541 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3369, fwd=7.6613 cmp_logits=0.0703, sampling=8.0876, total=16.1567
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 300.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5098 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3357, fwd=7.6773 cmp_logits=0.0706, sampling=8.0838, total=16.1684
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.5 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5398 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3202, fwd=7.7014 cmp_logits=0.0703, sampling=7.7500, total=15.8424
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 244.7 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1564 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3169, fwd=7.6942 cmp_logits=0.0706, sampling=7.7479, total=15.8305
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 245.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1488 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3166, fwd=7.6487 cmp_logits=0.0706, sampling=7.8175, total=15.8546
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 244.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.4
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.2230 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3002, fwd=7.7112 cmp_logits=0.0710, sampling=7.3228, total=15.4064
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6884 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3009, fwd=7.7019 cmp_logits=0.0701, sampling=7.3271, total=15.4006
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6856 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3030, fwd=7.6418 cmp_logits=0.0701, sampling=7.3917, total=15.4073
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6941 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3009, fwd=7.6563 cmp_logits=0.0696, sampling=7.3721, total=15.3999
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6841 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3011, fwd=7.6568 cmp_logits=0.0699, sampling=7.3583, total=15.3871
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7094 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3011, fwd=7.6225 cmp_logits=0.0706, sampling=7.4115, total=15.4064
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6906 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3014, fwd=7.6098 cmp_logits=0.0699, sampling=7.4279, total=15.4099
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6937 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3269, fwd=7.6470 cmp_logits=0.0713, sampling=7.3802, total=15.4262
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7130 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2995, fwd=7.6647 cmp_logits=0.0701, sampling=7.3776, total=15.4130
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6951 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3004, fwd=7.6578 cmp_logits=0.0701, sampling=7.3943, total=15.4235
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7397 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3040, fwd=7.6520 cmp_logits=0.0696, sampling=7.3922, total=15.4188
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7027 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2999, fwd=7.6909 cmp_logits=0.0706, sampling=7.3357, total=15.3975
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7044 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2849, fwd=7.7314 cmp_logits=0.0701, sampling=7.1545, total=15.2416
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4877 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2851, fwd=7.6435 cmp_logits=0.0694, sampling=7.2289, total=15.2283
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4736 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2849, fwd=7.6628 cmp_logits=0.0713, sampling=7.2298, total=15.2495
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5008 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2840, fwd=7.6079 cmp_logits=0.0691, sampling=7.2749, total=15.2369
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4827 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2916, fwd=7.6268 cmp_logits=0.0696, sampling=7.2451, total=15.2335
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4784 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2823, fwd=7.6411 cmp_logits=0.0710, sampling=7.2362, total=15.2316
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4793 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2818, fwd=7.6025 cmp_logits=0.0696, sampling=7.2861, total=15.2409
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4836 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2820, fwd=7.6451 cmp_logits=0.0706, sampling=7.2694, total=15.2678
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5311 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2618, fwd=7.7806 cmp_logits=0.0708, sampling=7.2773, total=15.3913
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5911 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2613, fwd=7.7803 cmp_logits=0.0710, sampling=7.3483, total=15.4622
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6605 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2599, fwd=7.8228 cmp_logits=0.0710, sampling=7.2911, total=15.4455
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6443 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2604, fwd=7.7810 cmp_logits=0.0713, sampling=7.3185, total=15.4321
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6329 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2604, fwd=7.8273 cmp_logits=0.0718, sampling=7.3178, total=15.4781
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6758 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 04:19:43 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 04:19:43 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 04:19:43 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2625, fwd=7.8156 cmp_logits=0.0710, sampling=7.3156, total=15.4657
INFO 10-04 04:19:43 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 04:19:43 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6898 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8578432, last_token_time=1728015581.8065722, first_scheduled_time=1728015577.8878036, first_token_time=1728015579.5569336, time_in_queue=0.02996039390563965, finished_time=1728015581.806536, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8588452, last_token_time=1728015582.706484, first_scheduled_time=1728015577.8878036, first_token_time=1728015579.5569336, time_in_queue=0.02895832061767578, finished_time=1728015582.706432, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8604336, last_token_time=1728015582.4106178, first_scheduled_time=1728015577.8878036, first_token_time=1728015581.2209954, time_in_queue=0.027369976043701172, finished_time=1728015582.4105592, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8611932, last_token_time=1728015583.1435366, first_scheduled_time=1728015579.557433, first_token_time=1728015581.3156252, time_in_queue=1.696239709854126, finished_time=1728015583.1435044, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8634014, last_token_time=1728015583.1435366, first_scheduled_time=1728015581.2215316, first_token_time=1728015581.3934596, time_in_queue=3.3581302165985107, finished_time=1728015583.1435108, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8656325, last_token_time=1728015582.2012706, first_scheduled_time=1728015581.3161147, first_token_time=1728015581.3934596, time_in_queue=3.450482130050659, finished_time=1728015582.201222, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8660886, last_token_time=1728015582.2012706, first_scheduled_time=1728015581.3161147, first_token_time=1728015581.3934596, time_in_queue=3.450026035308838, finished_time=1728015582.201226, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8665388, last_token_time=1728015583.2436945, first_scheduled_time=1728015581.3161147, first_token_time=1728015581.5573726, time_in_queue=3.449575901031494, finished_time=1728015583.2436736, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8687687, last_token_time=1728015583.7027209, first_scheduled_time=1728015581.4691827, first_token_time=1728015581.6451344, time_in_queue=3.6004140377044678, finished_time=1728015583.7027185, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8721843, last_token_time=1728015583.4828956, first_scheduled_time=1728015581.557929, first_token_time=1728015581.8062537, time_in_queue=3.6857447624206543, finished_time=1728015583.4828887, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=11, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8749769, last_token_time=1728015582.4508462, first_scheduled_time=1728015581.723694, first_token_time=1728015581.8062537, time_in_queue=3.848717212677002, finished_time=1728015582.4508133, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=12, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8755405, last_token_time=1728015582.6681097, first_scheduled_time=1728015581.723694, first_token_time=1728015581.8865516, time_in_queue=3.848153591156006, finished_time=1728015582.6680844, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=13, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8764257, last_token_time=1728015583.0190396, first_scheduled_time=1728015581.8070152, first_token_time=1728015581.8865516, time_in_queue=3.930589437484741, finished_time=1728015583.0190253, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=14, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8780298, last_token_time=1728015583.1084588, first_scheduled_time=1728015581.8070152, first_token_time=1728015581.9635699, time_in_queue=3.928985357284546, finished_time=1728015583.1084485, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=15, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8797176, last_token_time=1728015582.5310888, first_scheduled_time=1728015581.8872201, first_token_time=1728015582.0403178, time_in_queue=4.007502555847168, finished_time=1728015582.531072, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=16, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.880286, last_token_time=1728015582.7997513, first_scheduled_time=1728015581.9642704, first_token_time=1728015582.0403178, time_in_queue=4.083984375, finished_time=1728015582.7997406, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=17, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8813279, last_token_time=1728015582.7253175, first_scheduled_time=1728015581.9642704, first_token_time=1728015582.1190846, time_in_queue=4.082942485809326, finished_time=1728015582.7253072, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=18, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.882219, last_token_time=1728015582.3701017, first_scheduled_time=1728015582.0410845, first_token_time=1728015582.1190846, time_in_queue=4.158865451812744, finished_time=1728015582.3700924, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=19, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8825395, last_token_time=1728015583.2927148, first_scheduled_time=1728015582.0410845, first_token_time=1728015582.2008088, time_in_queue=4.158545017242432, finished_time=1728015583.2927086, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=20, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728015577.8846357, last_token_time=1728015583.6079767, first_scheduled_time=1728015582.1198986, first_token_time=1728015582.285711, time_in_queue=4.235262870788574, finished_time=1728015583.607975, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 15868
End-to-End latency: 5.85 seconds
Throughput: 3.42 requests/s, 2714.76 tokens/s
Per_token_time: 0.368 ms

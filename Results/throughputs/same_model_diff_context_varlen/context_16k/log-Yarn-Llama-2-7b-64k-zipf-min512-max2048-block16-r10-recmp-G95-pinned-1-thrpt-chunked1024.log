Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=16384, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=1024, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Selected required requests 10
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Chunked prefill is enabled. max_num_batched_tokens=1024, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'finetuned', 'type', 'original_max_position_embeddings'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 05:36:48 config.py:654] [SchedulerConfig] max_num_batched_tokens: 1024 chunked_prefill_enabled: True
INFO 10-04 05:36:48 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 05:36:49 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 05:36:54 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 05:36:58 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 05:36:58 model_runner.py:183] Loaded model: 
INFO 10-04 05:36:58 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 05:36:58 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 05:36:58 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 05:36:58 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 05:36:58 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 05:36:58 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 05:36:58 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 05:36:58 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 05:36:58 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 05:36:58 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 05:36:58 model_runner.py:183]         )
INFO 10-04 05:36:58 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 05:36:58 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 05:36:58 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 05:36:58 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 05:36:58 model_runner.py:183]         )
INFO 10-04 05:36:58 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 05:36:58 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 05:36:58 model_runner.py:183]       )
INFO 10-04 05:36:58 model_runner.py:183]     )
INFO 10-04 05:36:58 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 05:36:58 model_runner.py:183]   )
INFO 10-04 05:36:58 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 05:36:58 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 05:36:58 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 05:36:58 model_runner.py:183] )
INFO 10-04 05:36:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 05:36:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 15.5230, fwd=337.7576 cmp_logits=0.2360, sampling=106.0069, total=459.5256
INFO 10-04 05:36:58 worker.py:164] Peak: 13.307 GB, Initial: 38.980 GB, Free: 25.673 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.118 GB
INFO 10-04 05:36:58 gpu_executor.py:117] # GPU blocks: 3087, # CPU blocks: 8192
INFO 10-04 05:37:29 worker.py:189] _init_cache_engine took 24.1250 GB
INFO 10-04 05:37:29 scheduler.py:307] Scheduler initialized with prompt limit: 16384
INFO 10-04 05:37:29 Start warmup...
INFO 10-04 05:37:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:29 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 05:37:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8976, fwd=1897.9740 cmp_logits=71.3999, sampling=0.9589, total=1971.2331
INFO 10-04 05:37:31 metrics.py:335] Avg prompt throughput: 517.5 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 1978.7
INFO 10-04 05:37:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1971.7376 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3083, fwd=38.3170 cmp_logits=0.1106, sampling=6.3155, total=45.0528
INFO 10-04 05:37:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 45.9
INFO 10-04 05:37:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.4755 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3152, fwd=8.1110 cmp_logits=0.0782, sampling=6.6392, total=15.1448
INFO 10-04 05:37:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4572 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2837, fwd=7.9777 cmp_logits=0.0718, sampling=6.7697, total=15.1043
INFO 10-04 05:37:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-04 05:37:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3632 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2763, fwd=7.9451 cmp_logits=0.0701, sampling=6.8033, total=15.0964
INFO 10-04 05:37:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-04 05:37:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3320 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2759, fwd=7.9749 cmp_logits=0.0699, sampling=6.7532, total=15.0752
INFO 10-04 05:37:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-04 05:37:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3005 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:31 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2785, fwd=7.9596 cmp_logits=0.0730, sampling=6.7475, total=15.0595
INFO 10-04 05:37:31 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-04 05:37:31 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.2900 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:31 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:31 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2682, fwd=7.9556 cmp_logits=0.0691, sampling=6.7472, total=15.0411
INFO 10-04 05:37:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.4
INFO 10-04 05:37:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.2888 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2666, fwd=7.9224 cmp_logits=0.0670, sampling=6.8090, total=15.0654
INFO 10-04 05:37:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.4
INFO 10-04 05:37:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.2671 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:32 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2632, fwd=7.9365 cmp_logits=0.0665, sampling=6.7713, total=15.0380
INFO 10-04 05:37:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.4
INFO 10-04 05:37:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.2559 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:37 Start benchmarking...
INFO 10-04 05:37:37 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:37 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 05:37:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8123, fwd=1641.7489 cmp_logits=0.1185, sampling=70.5101, total=1713.1910
INFO 10-04 05:37:38 metrics.py:335] Avg prompt throughput: 586.9 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%, Interval(ms): 1744.7
INFO 10-04 05:37:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1713.6850 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:37:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 05:37:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7949, fwd=14.5645 cmp_logits=0.1087, sampling=77.3435, total=92.8128
INFO 10-04 05:37:38 metrics.py:335] Avg prompt throughput: 10947.9 tokens/s, Avg generation throughput: 21.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%, Interval(ms): 93.4
INFO 10-04 05:37:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 93.1871 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:37:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 05:37:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7856, fwd=14.4887 cmp_logits=0.0889, sampling=71.6236, total=86.9882
INFO 10-04 05:37:38 metrics.py:335] Avg prompt throughput: 11676.6 tokens/s, Avg generation throughput: 34.3 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 7.2%, CPU KV cache usage: 0.0%, Interval(ms): 87.5
INFO 10-04 05:37:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 87.3556 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 05:37:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7894, fwd=14.5376 cmp_logits=0.0846, sampling=62.2365, total=77.6491
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 13056.5 tokens/s, Avg generation throughput: 51.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 9.8%, CPU KV cache usage: 0.0%, Interval(ms): 78.2
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 78.0373 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8159, fwd=14.4246 cmp_logits=0.0927, sampling=62.0992, total=77.4333
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 13076.1 tokens/s, Avg generation throughput: 64.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 11.0%, CPU KV cache usage: 0.0%, Interval(ms): 78.0
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.8422 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8516, fwd=14.5206 cmp_logits=0.0825, sampling=58.4145, total=73.8702
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 13670.4 tokens/s, Avg generation throughput: 93.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 15.0%, CPU KV cache usage: 0.0%, Interval(ms): 74.5
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 74.3790 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8242, fwd=14.5068 cmp_logits=0.0696, sampling=61.1959, total=76.5975
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 13164.7 tokens/s, Avg generation throughput: 90.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 15.0%, CPU KV cache usage: 0.0%, Interval(ms): 77.3
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.0366 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8502, fwd=14.4978 cmp_logits=0.0837, sampling=71.9445, total=87.3771
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 11548.6 tokens/s, Avg generation throughput: 90.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 88.1
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 87.8921 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8566, fwd=14.6699 cmp_logits=0.0834, sampling=71.2559, total=86.8673
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 11601.5 tokens/s, Avg generation throughput: 102.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.8%, CPU KV cache usage: 0.0%, Interval(ms): 87.6
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 87.4028 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8695, fwd=14.5972 cmp_logits=0.0701, sampling=60.5791, total=76.1161
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 13217.4 tokens/s, Avg generation throughput: 117.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.8%, CPU KV cache usage: 0.0%, Interval(ms): 76.8
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.6170 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([473]), positions.shape=torch.Size([473]) hidden_states.shape=torch.Size([473, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7391, fwd=14.6260 cmp_logits=0.0818, sampling=37.4498, total=52.8975
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 8649.4 tokens/s, Avg generation throughput: 186.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.8%, CPU KV cache usage: 0.0%, Interval(ms): 53.6
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.4692 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4199, fwd=7.8723 cmp_logits=0.0672, sampling=9.1662, total=17.5264
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0383 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4196, fwd=7.7996 cmp_logits=0.0675, sampling=9.2287, total=17.5164
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0278 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4177, fwd=7.7991 cmp_logits=0.0672, sampling=9.2349, total=17.5202
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0254 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4134, fwd=7.7779 cmp_logits=0.0675, sampling=9.2607, total=17.5204
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0256 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4184, fwd=7.8123 cmp_logits=0.0665, sampling=9.2204, total=17.5185
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0571 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4263, fwd=7.8123 cmp_logits=0.0751, sampling=9.2173, total=17.5316
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0457 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4179, fwd=7.8325 cmp_logits=0.0677, sampling=9.2156, total=17.5347
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0483 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4213, fwd=7.8731 cmp_logits=0.0677, sampling=9.1889, total=17.5521
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0643 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4132, fwd=7.7157 cmp_logits=0.0672, sampling=9.3470, total=17.5438
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0521 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4227, fwd=7.7937 cmp_logits=0.0663, sampling=9.2537, total=17.5374
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0712 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4146, fwd=7.7872 cmp_logits=0.0668, sampling=9.2878, total=17.5574
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0676 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4194, fwd=7.7295 cmp_logits=0.0682, sampling=9.3687, total=17.5867
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0950 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4177, fwd=7.7336 cmp_logits=0.0668, sampling=9.3489, total=17.5679
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0771 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4194, fwd=7.8099 cmp_logits=0.0672, sampling=9.2952, total=17.5927
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0910 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4268, fwd=7.7603 cmp_logits=0.0670, sampling=9.3060, total=17.5610
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0929 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4144, fwd=7.7651 cmp_logits=0.0677, sampling=9.3107, total=17.5588
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0840 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4056, fwd=7.9496 cmp_logits=0.0670, sampling=8.9135, total=17.3366
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 499.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.8223 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4048, fwd=7.7970 cmp_logits=0.0670, sampling=9.0015, total=17.2713
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7548 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4027, fwd=7.8099 cmp_logits=0.0672, sampling=8.9779, total=17.2584
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7379 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3984, fwd=7.7939 cmp_logits=0.0665, sampling=9.0330, total=17.2927
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7732 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4034, fwd=7.7856 cmp_logits=0.0677, sampling=9.0129, total=17.2706
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7524 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4075, fwd=7.8008 cmp_logits=0.0668, sampling=9.0210, total=17.2973
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7732 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:37:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4010, fwd=7.7991 cmp_logits=0.0668, sampling=9.0089, total=17.2768
INFO 10-04 05:37:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 05:37:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7925 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:37:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3698, fwd=7.8030 cmp_logits=0.0670, sampling=8.5425, total=16.7835
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2021 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3707, fwd=7.8797 cmp_logits=0.0663, sampling=8.5206, total=16.8378
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2477 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3686, fwd=7.7996 cmp_logits=0.0665, sampling=8.5571, total=16.7930
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2102 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3700, fwd=7.7770 cmp_logits=0.0663, sampling=8.5852, total=16.7997
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2327 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3552, fwd=7.8058 cmp_logits=0.0787, sampling=8.3706, total=16.6109
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9945 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3526, fwd=7.7317 cmp_logits=0.0663, sampling=8.4128, total=16.5641
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9518 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3517, fwd=7.8304 cmp_logits=0.0665, sampling=8.4143, total=16.6638
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0457 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3579, fwd=7.7698 cmp_logits=0.0665, sampling=8.4293, total=16.6242
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0085 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3886, fwd=7.8762 cmp_logits=0.0670, sampling=8.3201, total=16.6526
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0350 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3536, fwd=7.7989 cmp_logits=0.0656, sampling=8.3814, total=16.6004
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9895 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3524, fwd=7.7353 cmp_logits=0.0658, sampling=8.4531, total=16.6078
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9959 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3641, fwd=7.7853 cmp_logits=0.0741, sampling=8.4314, total=16.6562
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0391 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3562, fwd=7.7591 cmp_logits=0.0660, sampling=8.4257, total=16.6080
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9909 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3569, fwd=7.7701 cmp_logits=0.0668, sampling=8.3904, total=16.5854
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9675 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3552, fwd=7.7746 cmp_logits=0.0660, sampling=8.4102, total=16.6068
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0069 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3550, fwd=7.7703 cmp_logits=0.0668, sampling=8.4040, total=16.5968
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0126 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3419, fwd=7.8471 cmp_logits=0.0658, sampling=8.1480, total=16.4037
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 295.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7572 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3405, fwd=7.7260 cmp_logits=0.0651, sampling=8.1847, total=16.3174
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6698 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3407, fwd=7.7507 cmp_logits=0.0663, sampling=8.1599, total=16.3186
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6695 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3421, fwd=7.7364 cmp_logits=0.0672, sampling=8.1766, total=16.3233
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6769 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3412, fwd=7.7584 cmp_logits=0.0660, sampling=8.1532, total=16.3195
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6717 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3431, fwd=7.7567 cmp_logits=0.0660, sampling=8.1851, total=16.3519
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7310 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3359, fwd=7.7188 cmp_logits=0.0663, sampling=8.1055, total=16.2277
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5768 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3397, fwd=7.7503 cmp_logits=0.0656, sampling=8.0714, total=16.2277
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5806 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3390, fwd=7.7350 cmp_logits=0.0660, sampling=8.0850, total=16.2261
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5746 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3440, fwd=7.7796 cmp_logits=0.0665, sampling=8.0581, total=16.2494
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6059 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3376, fwd=7.8344 cmp_logits=0.0679, sampling=8.0514, total=16.2921
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6528 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3443, fwd=7.8921 cmp_logits=0.0663, sampling=7.9463, total=16.2499
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6037 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3390, fwd=7.7255 cmp_logits=0.0668, sampling=8.1015, total=16.2337
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5877 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3412, fwd=7.7257 cmp_logits=0.0663, sampling=8.1270, total=16.2606
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6214 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3381, fwd=7.7150 cmp_logits=0.0656, sampling=8.1191, total=16.2389
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6116 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3402, fwd=7.8418 cmp_logits=0.0663, sampling=8.0698, total=16.3190
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6721 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3402, fwd=7.8056 cmp_logits=0.0653, sampling=8.0416, total=16.2537
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6025 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3393, fwd=7.8034 cmp_logits=0.0670, sampling=8.0345, total=16.2451
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5985 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3483, fwd=7.7357 cmp_logits=0.0746, sampling=8.1227, total=16.2818
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6564 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3223, fwd=7.8027 cmp_logits=0.0663, sampling=7.4723, total=15.6643
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 246.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.0089 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3068, fwd=7.7949 cmp_logits=0.0660, sampling=7.2272, total=15.3959
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6822 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3033, fwd=7.7150 cmp_logits=0.0653, sampling=7.2677, total=15.3522
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6345 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2997, fwd=7.7128 cmp_logits=0.0660, sampling=7.2837, total=15.3632
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6515 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3099, fwd=7.8659 cmp_logits=0.0660, sampling=7.1595, total=15.4018
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6841 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3247, fwd=7.7660 cmp_logits=0.0656, sampling=7.2188, total=15.3763
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6868 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2937, fwd=7.8511 cmp_logits=0.0651, sampling=7.1309, total=15.3418
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 126.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5938 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2966, fwd=7.7291 cmp_logits=0.0653, sampling=7.2129, total=15.3048
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5497 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2875, fwd=7.7243 cmp_logits=0.0656, sampling=7.1895, total=15.2676
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5163 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2868, fwd=7.7496 cmp_logits=0.0651, sampling=7.1778, total=15.2805
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5249 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2851, fwd=7.7083 cmp_logits=0.0658, sampling=7.2279, total=15.2881
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5323 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2856, fwd=7.7422 cmp_logits=0.0648, sampling=7.2172, total=15.3108
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5551 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2854, fwd=7.7057 cmp_logits=0.0653, sampling=7.2160, total=15.2731
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5180 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2873, fwd=7.7555 cmp_logits=0.0660, sampling=7.1790, total=15.2888
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5342 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2837, fwd=7.6976 cmp_logits=0.0651, sampling=7.2424, total=15.2895
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5349 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2849, fwd=7.7236 cmp_logits=0.0653, sampling=7.1976, total=15.2719
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5146 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2882, fwd=7.7569 cmp_logits=0.0660, sampling=7.1850, total=15.2967
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5401 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2890, fwd=7.7209 cmp_logits=0.0660, sampling=7.1926, total=15.2695
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5156 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2882, fwd=7.7422 cmp_logits=0.0660, sampling=7.1785, total=15.2760
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5449 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2737, fwd=7.8962 cmp_logits=0.0668, sampling=7.2031, total=15.4405
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6391 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2620, fwd=7.8716 cmp_logits=0.0665, sampling=7.2205, total=15.4219
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6178 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2635, fwd=7.8833 cmp_logits=0.0660, sampling=7.2279, total=15.4414
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6403 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2646, fwd=7.8471 cmp_logits=0.0658, sampling=7.2384, total=15.4169
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6133 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2694, fwd=7.8526 cmp_logits=0.0665, sampling=7.2417, total=15.4309
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6286 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2608, fwd=7.8499 cmp_logits=0.0670, sampling=7.2219, total=15.4004
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5983 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:40 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2639, fwd=7.8492 cmp_logits=0.0663, sampling=7.2250, total=15.4054
INFO 10-04 05:37:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6031 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:40 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:41 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2627, fwd=7.8933 cmp_logits=0.0660, sampling=7.2191, total=15.4424
INFO 10-04 05:37:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:37:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6410 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:41 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:41 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2637, fwd=7.8790 cmp_logits=0.0663, sampling=7.2308, total=15.4405
INFO 10-04 05:37:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:37:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6367 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:41 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:41 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2627, fwd=7.8564 cmp_logits=0.0658, sampling=7.2262, total=15.4119
INFO 10-04 05:37:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:37:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6088 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:37:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 05:37:41 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:37:41 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2639, fwd=7.8552 cmp_logits=0.0665, sampling=7.2260, total=15.4123
INFO 10-04 05:37:41 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:37:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6362 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020257.0394485, last_token_time=1728020259.8608906, first_scheduled_time=1728020257.060621, first_token_time=1728020258.7739635, time_in_queue=0.021172523498535156, finished_time=1728020259.8608494, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020257.0411787, last_token_time=1728020260.262128, first_scheduled_time=1728020257.060621, first_token_time=1728020258.8674054, time_in_queue=0.019442319869995117, finished_time=1728020260.2621021, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020257.0430925, last_token_time=1728020260.056135, first_scheduled_time=1728020258.7744722, first_token_time=1728020258.9549224, time_in_queue=1.731379747390747, finished_time=1728020260.0561113, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020257.0443666, last_token_time=1728020260.5812669, first_scheduled_time=1728020258.8678207, first_token_time=1728020259.0331004, time_in_queue=1.8234541416168213, finished_time=1728020260.581245, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020257.0470748, last_token_time=1728020260.597469, first_scheduled_time=1728020258.955334, first_token_time=1728020259.1110811, time_in_queue=1.9082591533660889, finished_time=1728020260.597451, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020257.04944, last_token_time=1728020259.9865334, first_scheduled_time=1728020259.033527, first_token_time=1728020259.185562, time_in_queue=1.9840869903564453, finished_time=1728020259.986515, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020257.0504663, last_token_time=1728020259.9865334, first_scheduled_time=1728020259.1115632, first_token_time=1728020259.185562, time_in_queue=2.0610969066619873, finished_time=1728020259.9865196, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020257.0514922, last_token_time=1728020260.676587, first_scheduled_time=1728020259.1115632, first_token_time=1728020259.350873, time_in_queue=2.0600709915161133, finished_time=1728020260.676574, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020257.0539978, last_token_time=1728020261.0538425, first_scheduled_time=1728020259.2633626, first_token_time=1728020259.438425, time_in_queue=2.209364891052246, finished_time=1728020261.0538402, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020257.057482, last_token_time=1728020260.880448, first_scheduled_time=1728020259.35142, first_token_time=1728020259.5688436, time_in_queue=2.293937921524048, finished_time=1728020260.8804462, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 11197
End-to-End latency: 4.01 seconds
Throughput: 2.49 requests/s, 2789.07 tokens/s
Per_token_time: 0.359 ms

Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=16384, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=512, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Selected required requests 10
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Chunked prefill is enabled. max_num_batched_tokens=512, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'type', 'finetuned', 'original_max_position_embeddings'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 05:38:06 config.py:654] [SchedulerConfig] max_num_batched_tokens: 512 chunked_prefill_enabled: True
INFO 10-04 05:38:06 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 05:38:07 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 05:38:12 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 05:38:15 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 05:38:15 model_runner.py:183] Loaded model: 
INFO 10-04 05:38:15 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 05:38:15 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 05:38:15 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 05:38:15 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 05:38:15 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 05:38:15 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 05:38:15 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 05:38:15 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 05:38:15 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 05:38:15 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 05:38:15 model_runner.py:183]         )
INFO 10-04 05:38:15 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 05:38:15 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 05:38:15 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 05:38:15 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 05:38:15 model_runner.py:183]         )
INFO 10-04 05:38:15 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 05:38:15 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 05:38:15 model_runner.py:183]       )
INFO 10-04 05:38:15 model_runner.py:183]     )
INFO 10-04 05:38:15 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 05:38:15 model_runner.py:183]   )
INFO 10-04 05:38:15 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 05:38:15 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 05:38:15 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 05:38:15 model_runner.py:183] )
INFO 10-04 05:38:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 4.5414, fwd=350.9760 cmp_logits=0.2398, sampling=79.8182, total=435.5774
INFO 10-04 05:38:16 worker.py:164] Peak: 13.264 GB, Initial: 38.980 GB, Free: 25.716 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.160 GB
INFO 10-04 05:38:16 gpu_executor.py:117] # GPU blocks: 3092, # CPU blocks: 8192
INFO 10-04 05:38:47 worker.py:189] _init_cache_engine took 24.1875 GB
INFO 10-04 05:38:47 scheduler.py:307] Scheduler initialized with prompt limit: 16384
INFO 10-04 05:38:47 Start warmup...
INFO 10-04 05:38:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:47 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7074, fwd=1941.8662 cmp_logits=0.0796, sampling=0.2429, total=1942.8983
INFO 10-04 05:38:49 metrics.py:335] Avg prompt throughput: 262.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 1949.9
INFO 10-04 05:38:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1943.3026 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 30.0512, fwd=13.5629 cmp_logits=38.5532, sampling=0.8764, total=83.0460
INFO 10-04 05:38:49 metrics.py:335] Avg prompt throughput: 6109.3 tokens/s, Avg generation throughput: 11.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 83.8
INFO 10-04 05:38:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 83.4398 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2995, fwd=42.7661 cmp_logits=0.1142, sampling=6.8383, total=50.0197
INFO 10-04 05:38:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 50.6
INFO 10-04 05:38:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.4255 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3161, fwd=7.9319 cmp_logits=0.0761, sampling=7.2265, total=15.5523
INFO 10-04 05:38:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 61.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 05:38:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8725 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2844, fwd=7.8309 cmp_logits=0.0699, sampling=6.5563, total=14.7426
INFO 10-04 05:38:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.2
INFO 10-04 05:38:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.9932 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2744, fwd=7.8256 cmp_logits=0.0696, sampling=6.1150, total=14.2858
INFO 10-04 05:38:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 05:38:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5152 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2768, fwd=7.8132 cmp_logits=0.0727, sampling=6.1057, total=14.2698
INFO 10-04 05:38:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 05:38:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4985 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2646, fwd=7.8404 cmp_logits=0.0670, sampling=6.1040, total=14.2770
INFO 10-04 05:38:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 05:38:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5214 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2601, fwd=7.7841 cmp_logits=0.0651, sampling=6.1626, total=14.2729
INFO 10-04 05:38:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 05:38:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4823 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:49 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2635, fwd=7.8037 cmp_logits=0.0656, sampling=6.1202, total=14.2539
INFO 10-04 05:38:49 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 05:38:49 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4467 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:49 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:49 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2646, fwd=7.7531 cmp_logits=0.0668, sampling=6.1829, total=14.2684
INFO 10-04 05:38:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 05:38:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5133 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:55 Start benchmarking...
INFO 10-04 05:38:55 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:55 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6659, fwd=1649.9755 cmp_logits=0.0718, sampling=0.2477, total=1650.9619
INFO 10-04 05:38:56 metrics.py:335] Avg prompt throughput: 304.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%, Interval(ms): 1683.0
INFO 10-04 05:38:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1651.3975 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:56 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 30.2536, fwd=13.6623 cmp_logits=0.0756, sampling=37.1728, total=81.1656
INFO 10-04 05:38:56 metrics.py:335] Avg prompt throughput: 6266.7 tokens/s, Avg generation throughput: 12.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%, Interval(ms): 81.7
INFO 10-04 05:38:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 81.5012 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:56 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6175, fwd=14.2422 cmp_logits=0.1028, sampling=39.5272, total=54.4903
INFO 10-04 05:38:56 metrics.py:335] Avg prompt throughput: 9301.2 tokens/s, Avg generation throughput: 36.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%, Interval(ms): 54.9
INFO 10-04 05:38:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 54.7907 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:38:56 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6180, fwd=14.3173 cmp_logits=0.0684, sampling=35.1598, total=50.1642
INFO 10-04 05:38:56 metrics.py:335] Avg prompt throughput: 10081.0 tokens/s, Avg generation throughput: 39.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%, Interval(ms): 50.6
INFO 10-04 05:38:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.4444 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:38:56 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6418, fwd=14.4131 cmp_logits=0.0820, sampling=38.4512, total=53.5891
INFO 10-04 05:38:56 metrics.py:335] Avg prompt throughput: 9425.5 tokens/s, Avg generation throughput: 55.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 54.1
INFO 10-04 05:38:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.9463 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 05:38:56 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:56 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6385, fwd=14.4405 cmp_logits=0.0682, sampling=38.6207, total=53.7686
INFO 10-04 05:38:56 metrics.py:335] Avg prompt throughput: 9377.5 tokens/s, Avg generation throughput: 55.3 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 54.3
INFO 10-04 05:38:56 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 54.0776 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 05:38:56 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6564, fwd=14.3828 cmp_logits=0.0818, sampling=39.7105, total=54.8325
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 9192.1 tokens/s, Avg generation throughput: 72.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 9.8%, CPU KV cache usage: 0.0%, Interval(ms): 55.4
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 55.2254 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6418, fwd=14.4482 cmp_logits=0.0677, sampling=25.7072, total=40.8654
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 12279.9 tokens/s, Avg generation throughput: 96.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 41.4
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 41.2121 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6549, fwd=14.4293 cmp_logits=0.0684, sampling=30.7183, total=45.8720
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 10955.7 tokens/s, Avg generation throughput: 86.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 46.4
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.2127 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6673, fwd=14.3435 cmp_logits=0.0813, sampling=30.3979, total=45.4907
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 11033.5 tokens/s, Avg generation throughput: 108.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 11.0%, CPU KV cache usage: 0.0%, Interval(ms): 46.0
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.8894 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6759, fwd=14.4982 cmp_logits=0.0815, sampling=26.7589, total=42.0153
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 11887.7 tokens/s, Avg generation throughput: 140.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%, Interval(ms): 42.6
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 42.4528 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7031, fwd=14.3833 cmp_logits=0.0823, sampling=26.6044, total=41.7743
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 11931.0 tokens/s, Avg generation throughput: 165.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 14.9%, CPU KV cache usage: 0.0%, Interval(ms): 42.4
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 42.2459 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6859, fwd=14.3876 cmp_logits=0.0687, sampling=27.7059, total=42.8493
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 11619.7 tokens/s, Avg generation throughput: 161.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 15.0%, CPU KV cache usage: 0.0%, Interval(ms): 43.5
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.2925 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6869, fwd=14.3731 cmp_logits=0.0694, sampling=32.7427, total=47.8730
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 10417.5 tokens/s, Avg generation throughput: 144.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 15.0%, CPU KV cache usage: 0.0%, Interval(ms): 48.5
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.3091 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7207, fwd=14.3805 cmp_logits=0.0834, sampling=31.7211, total=46.9065
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 10613.0 tokens/s, Avg generation throughput: 168.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 47.6
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.4176 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7071, fwd=14.4632 cmp_logits=0.0687, sampling=29.3930, total=44.6329
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 11122.5 tokens/s, Avg generation throughput: 176.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 45.3
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.1045 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7095, fwd=14.4446 cmp_logits=0.0691, sampling=34.4107, total=49.6347
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 10026.7 tokens/s, Avg generation throughput: 159.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%, Interval(ms): 50.3
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.0968 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7672, fwd=14.5097 cmp_logits=0.0820, sampling=39.4108, total=54.7707
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 9084.2 tokens/s, Avg generation throughput: 162.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.8%, CPU KV cache usage: 0.0%, Interval(ms): 55.5
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 55.3133 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7386, fwd=14.4150 cmp_logits=0.0699, sampling=27.8556, total=43.0801
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 11494.6 tokens/s, Avg generation throughput: 205.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.8%, CPU KV cache usage: 0.0%, Interval(ms): 43.8
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.5851 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7522, fwd=14.3323 cmp_logits=0.0687, sampling=32.8619, total=48.0156
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 10322.3 tokens/s, Avg generation throughput: 184.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 48.7
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.5568 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7393, fwd=14.3242 cmp_logits=0.0694, sampling=37.9281, total=53.0617
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 9358.0 tokens/s, Avg generation throughput: 167.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 53.8
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.5736 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([21]), positions.shape=torch.Size([21]) hidden_states.shape=torch.Size([21, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.6042, fwd=14.2677 cmp_logits=0.0813, sampling=11.7412, total=26.6953
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 437.5 tokens/s, Avg generation throughput: 364.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.9%, CPU KV cache usage: 0.0%, Interval(ms): 27.4
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.2508 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4187, fwd=7.7126 cmp_logits=0.0677, sampling=9.3682, total=17.5681
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0807 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4194, fwd=7.6544 cmp_logits=0.0660, sampling=9.4113, total=17.5521
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0573 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4163, fwd=7.6551 cmp_logits=0.0684, sampling=9.4240, total=17.5648
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0972 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4139, fwd=7.6537 cmp_logits=0.0670, sampling=9.4154, total=17.5509
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0614 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4172, fwd=7.6635 cmp_logits=0.0684, sampling=9.3951, total=17.5447
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0573 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4141, fwd=7.6349 cmp_logits=0.0672, sampling=9.4395, total=17.5564
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0826 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4013, fwd=7.7240 cmp_logits=0.0672, sampling=9.0778, total=17.2713
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 500.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7526 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4041, fwd=7.7033 cmp_logits=0.0663, sampling=9.0606, total=17.2350
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7498 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4020, fwd=7.6287 cmp_logits=0.0682, sampling=9.1343, total=17.2338
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 503.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7200 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4015, fwd=7.6435 cmp_logits=0.0665, sampling=9.1472, total=17.2591
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7376 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4036, fwd=7.6356 cmp_logits=0.0663, sampling=9.1300, total=17.2365
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 503.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7114 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4029, fwd=7.6673 cmp_logits=0.0660, sampling=9.1751, total=17.3123
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 500.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7951 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4025, fwd=7.7245 cmp_logits=0.0665, sampling=9.1226, total=17.3168
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 500.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.8187 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3972, fwd=7.6506 cmp_logits=0.0663, sampling=9.1646, total=17.2796
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7531 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:38:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3996, fwd=7.5779 cmp_logits=0.0663, sampling=9.2316, total=17.2758
INFO 10-04 05:38:57 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 05:38:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7450 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:38:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4027, fwd=7.6580 cmp_logits=0.0668, sampling=9.1393, total=17.2677
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 502.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7548 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4058, fwd=7.6504 cmp_logits=0.0660, sampling=9.1751, total=17.2982
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 500.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7979 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([8]), positions.shape=torch.Size([8]) hidden_states.shape=torch.Size([8, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3829, fwd=7.6728 cmp_logits=0.0663, sampling=8.9879, total=17.1111
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 449.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.5865 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3548, fwd=7.7236 cmp_logits=0.0663, sampling=8.4362, total=16.5818
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9654 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3519, fwd=7.7131 cmp_logits=0.0656, sampling=8.4543, total=16.5858
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9721 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3531, fwd=7.6303 cmp_logits=0.0656, sampling=8.5166, total=16.5665
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9587 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3531, fwd=7.6158 cmp_logits=0.0658, sampling=8.5390, total=16.5744
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.2%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9573 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3502, fwd=7.6652 cmp_logits=0.0658, sampling=8.5399, total=16.6218
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0033 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3614, fwd=7.6265 cmp_logits=0.0660, sampling=8.5344, total=16.5894
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9699 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3557, fwd=7.6306 cmp_logits=0.0648, sampling=8.5330, total=16.5854
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9723 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3526, fwd=7.6239 cmp_logits=0.0656, sampling=8.5399, total=16.5825
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9671 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3541, fwd=7.6525 cmp_logits=0.0651, sampling=8.5201, total=16.5925
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9759 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3541, fwd=7.6766 cmp_logits=0.0668, sampling=8.5454, total=16.6438
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0355 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3541, fwd=7.6005 cmp_logits=0.0646, sampling=8.5931, total=16.6132
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0209 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3471, fwd=7.8144 cmp_logits=0.0660, sampling=8.1315, total=16.3600
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 295.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7170 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3393, fwd=7.6129 cmp_logits=0.0668, sampling=8.3268, total=16.3465
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6981 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3362, fwd=7.6714 cmp_logits=0.0653, sampling=8.2622, total=16.3360
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7096 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3383, fwd=7.6573 cmp_logits=0.0651, sampling=8.3137, total=16.3755
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7301 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3381, fwd=7.6959 cmp_logits=0.0660, sampling=8.2459, total=16.3467
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6991 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3378, fwd=7.6051 cmp_logits=0.0648, sampling=8.3206, total=16.3293
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6814 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3371, fwd=7.6008 cmp_logits=0.0653, sampling=8.3323, total=16.3362
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6941 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3393, fwd=7.7457 cmp_logits=0.0653, sampling=8.1720, total=16.3231
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6721 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3381, fwd=7.7095 cmp_logits=0.0658, sampling=8.2440, total=16.3581
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7110 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3495, fwd=7.7479 cmp_logits=0.0656, sampling=8.1911, total=16.3548
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7193 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3381, fwd=7.7348 cmp_logits=0.0668, sampling=8.1937, total=16.3338
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6869 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3462, fwd=7.6442 cmp_logits=0.0651, sampling=8.2660, total=16.3219
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6762 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3388, fwd=7.6880 cmp_logits=0.0653, sampling=8.2505, total=16.3436
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6962 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3407, fwd=7.7105 cmp_logits=0.0653, sampling=8.2591, total=16.3765
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7332 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3405, fwd=7.7305 cmp_logits=0.0656, sampling=8.1482, total=16.2857
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6407 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3395, fwd=7.7662 cmp_logits=0.0656, sampling=8.0819, total=16.2542
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6113 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3471, fwd=7.6780 cmp_logits=0.0737, sampling=8.1606, total=16.2604
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6149 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3378, fwd=7.6265 cmp_logits=0.0656, sampling=8.2197, total=16.2506
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6032 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3381, fwd=7.6828 cmp_logits=0.0660, sampling=8.2033, total=16.2909
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6414 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3381, fwd=7.5986 cmp_logits=0.0646, sampling=8.2374, total=16.2396
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5896 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3376, fwd=7.6408 cmp_logits=0.0656, sampling=8.2195, total=16.2644
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6364 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3347, fwd=7.6914 cmp_logits=0.0656, sampling=7.5719, total=15.6648
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.2 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9786 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3242, fwd=7.6039 cmp_logits=0.0653, sampling=7.6506, total=15.6453
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.2 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.7%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9655 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3204, fwd=7.6404 cmp_logits=0.0653, sampling=7.6663, total=15.6932
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 246.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.0503 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3033, fwd=7.6597 cmp_logits=0.0653, sampling=7.3206, total=15.3494
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6317 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3021, fwd=7.6666 cmp_logits=0.0658, sampling=7.3264, total=15.3618
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6462 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3116, fwd=7.6175 cmp_logits=0.0656, sampling=7.3538, total=15.3491
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6274 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3018, fwd=7.6387 cmp_logits=0.0648, sampling=7.3373, total=15.3432
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6281 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3018, fwd=7.6458 cmp_logits=0.0646, sampling=7.3726, total=15.3856
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6677 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3033, fwd=7.6692 cmp_logits=0.0653, sampling=7.2985, total=15.3370
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.9%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6181 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3030, fwd=7.6029 cmp_logits=0.0648, sampling=7.4096, total=15.3816
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6863 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2856, fwd=7.6840 cmp_logits=0.0648, sampling=7.2913, total=15.3270
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 126.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5718 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3042, fwd=7.6284 cmp_logits=0.0653, sampling=7.3078, total=15.3067
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5518 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2861, fwd=7.6628 cmp_logits=0.0653, sampling=7.3011, total=15.3165
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5635 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2842, fwd=7.6137 cmp_logits=0.0653, sampling=7.3116, total=15.2755
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5208 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2856, fwd=7.7024 cmp_logits=0.0653, sampling=7.2494, total=15.3034
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5458 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2849, fwd=7.6771 cmp_logits=0.0651, sampling=7.2680, total=15.2962
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5387 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2863, fwd=7.6747 cmp_logits=0.0656, sampling=7.2629, total=15.2910
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5363 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2866, fwd=7.6094 cmp_logits=0.0648, sampling=7.3509, total=15.3124
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5616 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2825, fwd=7.6613 cmp_logits=0.0646, sampling=7.2789, total=15.2881
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5306 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2847, fwd=7.6075 cmp_logits=0.0703, sampling=7.3285, total=15.2919
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5432 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2928, fwd=7.5917 cmp_logits=0.0644, sampling=7.3471, total=15.2969
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5399 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2842, fwd=7.5893 cmp_logits=0.0648, sampling=7.3628, total=15.3019
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5458 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2854, fwd=7.6110 cmp_logits=0.0646, sampling=7.3619, total=15.3239
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5635 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2840, fwd=7.5791 cmp_logits=0.0641, sampling=7.3462, total=15.2745
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5170 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2851, fwd=7.6225 cmp_logits=0.0651, sampling=7.3116, total=15.2850
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5303 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2861, fwd=7.6056 cmp_logits=0.0656, sampling=7.3225, total=15.2805
INFO 10-04 05:38:58 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5261 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:38:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2911, fwd=7.5922 cmp_logits=0.0656, sampling=7.3416, total=15.2910
INFO 10-04 05:38:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5578 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:38:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2639, fwd=7.8280 cmp_logits=0.0656, sampling=7.3254, total=15.4836
INFO 10-04 05:38:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 05:38:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6817 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2620, fwd=7.7772 cmp_logits=0.0651, sampling=7.2992, total=15.4047
INFO 10-04 05:38:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6014 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2615, fwd=7.7658 cmp_logits=0.0656, sampling=7.3185, total=15.4123
INFO 10-04 05:38:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6114 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2620, fwd=7.8118 cmp_logits=0.0665, sampling=7.2918, total=15.4328
INFO 10-04 05:38:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:38:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6322 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2677, fwd=7.8433 cmp_logits=0.0660, sampling=7.2677, total=15.4452
INFO 10-04 05:38:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:38:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6438 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2630, fwd=7.8137 cmp_logits=0.0789, sampling=7.3059, total=15.4624
INFO 10-04 05:38:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:38:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6624 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2613, fwd=7.8046 cmp_logits=0.0663, sampling=7.2818, total=15.4150
INFO 10-04 05:38:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:38:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6126 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2706, fwd=7.7820 cmp_logits=0.0739, sampling=7.2877, total=15.4152
INFO 10-04 05:38:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:38:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6126 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:38:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=256
INFO 10-04 05:38:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:38:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2615, fwd=7.8006 cmp_logits=0.0651, sampling=7.3164, total=15.4445
INFO 10-04 05:38:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:38:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6686 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020335.0068157, last_token_time=1728020337.8232632, first_scheduled_time=1728020335.0285354, first_token_time=1728020336.761296, time_in_queue=0.021719694137573242, finished_time=1728020337.8232205, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020335.0082462, last_token_time=1728020338.2269137, first_scheduled_time=1728020336.6800265, first_token_time=1728020336.8162384, time_in_queue=1.6717803478240967, finished_time=1728020338.2268875, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020335.0102808, last_token_time=1728020338.0383115, first_scheduled_time=1728020336.761646, first_token_time=1728020336.9209137, time_in_queue=1.7513651847839355, finished_time=1728020338.0382838, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020335.01158, last_token_time=1728020338.5803647, first_scheduled_time=1728020336.8672154, first_token_time=1728020337.030527, time_in_queue=1.855635404586792, finished_time=1728020338.580343, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020335.0143118, last_token_time=1728020338.6288576, first_scheduled_time=1728020336.9755802, first_token_time=1728020337.164298, time_in_queue=1.961268424987793, finished_time=1728020338.6288328, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020335.0167496, last_token_time=1728020338.0205252, first_scheduled_time=1728020337.118689, first_token_time=1728020337.2069266, time_in_queue=2.1019394397735596, finished_time=1728020338.0205069, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020335.017899, last_token_time=1728020338.0383115, first_scheduled_time=1728020337.1647813, first_token_time=1728020337.249314, time_in_queue=2.1468822956085205, finished_time=1728020338.038298, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020335.018968, last_token_time=1728020338.7394207, first_scheduled_time=1728020337.2074125, first_token_time=1728020337.3888075, time_in_queue=2.1884443759918213, finished_time=1728020338.7394066, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020335.0216033, last_token_time=1728020339.1480892, first_scheduled_time=1728020337.3417678, first_token_time=1728020337.5398455, time_in_queue=2.320164442062378, finished_time=1728020339.1480863, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020335.0252578, last_token_time=1728020339.006095, first_scheduled_time=1728020337.484936, first_token_time=1728020337.7134953, time_in_queue=2.4596781730651855, finished_time=1728020339.0060933, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 11197
End-to-End latency: 4.14 seconds
Throughput: 2.41 requests/s, 2703.63 tokens/s
Per_token_time: 0.370 ms

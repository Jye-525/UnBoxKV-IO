Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=4096, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=16384, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=2048, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=1585, output_len=80, sequence_len=1665...
User Requests: prompt_len=970, output_len=49, sequence_len=1019...
User Requests: prompt_len=2398, output_len=120, sequence_len=2518...
User Requests: prompt_len=2366, output_len=119, sequence_len=2485...
User Requests: prompt_len=652, output_len=33, sequence_len=685...
User Requests: prompt_len=658, output_len=33, sequence_len=691...
User Requests: prompt_len=2495, output_len=125, sequence_len=2620...
User Requests: prompt_len=3566, output_len=179, sequence_len=3745...
User Requests: prompt_len=2969, output_len=149, sequence_len=3118...
Selected required requests 10
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=1585, output_len=80, sequence_len=1665...
User Requests: prompt_len=970, output_len=49, sequence_len=1019...
User Requests: prompt_len=2398, output_len=120, sequence_len=2518...
User Requests: prompt_len=2366, output_len=119, sequence_len=2485...
User Requests: prompt_len=652, output_len=33, sequence_len=685...
User Requests: prompt_len=658, output_len=33, sequence_len=691...
User Requests: prompt_len=2495, output_len=125, sequence_len=2620...
User Requests: prompt_len=3566, output_len=179, sequence_len=3745...
User Requests: prompt_len=2969, output_len=149, sequence_len=3118...
Chunked prefill is enabled. max_num_batched_tokens=2048, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'type', 'original_max_position_embeddings', 'finetuned'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 15:27:08 config.py:654] [SchedulerConfig] max_num_batched_tokens: 2048 chunked_prefill_enabled: True
INFO 10-04 15:27:08 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 15:27:09 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 15:27:14 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 15:27:17 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 15:27:17 model_runner.py:183] Loaded model: 
INFO 10-04 15:27:17 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 15:27:17 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 15:27:17 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 15:27:17 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 15:27:17 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 15:27:17 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 15:27:17 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 15:27:17 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 15:27:17 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 15:27:17 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 15:27:17 model_runner.py:183]         )
INFO 10-04 15:27:17 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 15:27:17 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 15:27:17 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 15:27:17 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 15:27:17 model_runner.py:183]         )
INFO 10-04 15:27:17 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 15:27:17 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 15:27:17 model_runner.py:183]       )
INFO 10-04 15:27:17 model_runner.py:183]     )
INFO 10-04 15:27:17 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 15:27:17 model_runner.py:183]   )
INFO 10-04 15:27:17 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 15:27:17 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 15:27:17 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 15:27:17 model_runner.py:183] )
INFO 10-04 15:27:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 15:27:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 10.9484, fwd=352.0060 cmp_logits=0.2487, sampling=157.1600, total=520.3650
INFO 10-04 15:27:18 worker.py:164] Peak: 13.332 GB, Initial: 38.980 GB, Free: 25.648 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.092 GB
INFO 10-04 15:27:18 gpu_executor.py:117] # GPU blocks: 3083, # CPU blocks: 8192
INFO 10-04 15:27:48 worker.py:189] _init_cache_engine took 24.0859 GB
INFO 10-04 15:27:48 scheduler.py:307] Scheduler initialized with prompt limit: 16384
INFO 10-04 15:27:48 Start warmup...
INFO 10-04 15:27:48 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:48 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:27:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9301, fwd=1977.5784 cmp_logits=71.0847, sampling=0.9656, total=2050.5607
INFO 10-04 15:27:50 metrics.py:335] Avg prompt throughput: 497.7 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 2057.7
INFO 10-04 15:27:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 2051.1084 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:27:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:27:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3109, fwd=40.1502 cmp_logits=0.1206, sampling=6.5031, total=47.0867
INFO 10-04 15:27:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 47.9
INFO 10-04 15:27:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.5376 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:27:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:27:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3111, fwd=8.1632 cmp_logits=0.0713, sampling=6.9866, total=15.5337
INFO 10-04 15:27:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 61.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 15:27:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8551 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:27:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:27:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2828, fwd=8.1127 cmp_logits=0.0713, sampling=7.0193, total=15.4877
INFO 10-04 15:27:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 15:27:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7325 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:27:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:27:50 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2820, fwd=8.1053 cmp_logits=0.0713, sampling=7.0469, total=15.5070
INFO 10-04 15:27:50 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 15:27:50 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7311 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:27:50 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:50 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:27:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2761, fwd=8.1103 cmp_logits=0.0710, sampling=7.0126, total=15.4717
INFO 10-04 15:27:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 15:27:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6920 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:27:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:27:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2787, fwd=8.1787 cmp_logits=0.0751, sampling=6.9485, total=15.4827
INFO 10-04 15:27:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 15:27:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7099 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:27:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:27:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2737, fwd=8.1129 cmp_logits=0.0699, sampling=7.0045, total=15.4617
INFO 10-04 15:27:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 15:27:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7089 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:27:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:27:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2704, fwd=8.0912 cmp_logits=0.0689, sampling=7.0550, total=15.4865
INFO 10-04 15:27:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 15:27:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6896 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:27:51 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:51 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:27:51 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2689, fwd=8.1692 cmp_logits=0.0694, sampling=6.6023, total=15.1107
INFO 10-04 15:27:51 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 10-04 15:27:51 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3308 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:27:56 Start benchmarking...
INFO 10-04 15:27:56 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:56 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 15:27:57 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1315, fwd=1592.6113 cmp_logits=0.1187, sampling=147.3362, total=1741.1985
INFO 10-04 15:27:57 metrics.py:335] Avg prompt throughput: 1145.5 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 4.4%, CPU KV cache usage: 0.0%, Interval(ms): 1787.9
INFO 10-04 15:27:57 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1741.7541 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:27:57 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:57 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 15:27:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1244, fwd=14.8280 cmp_logits=0.1063, sampling=144.6867, total=160.7466
INFO 10-04 15:27:58 metrics.py:335] Avg prompt throughput: 12677.5 tokens/s, Avg generation throughput: 18.6 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 11.3%, CPU KV cache usage: 0.0%, Interval(ms): 161.5
INFO 10-04 15:27:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 161.2122 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 15:27:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 15:27:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.0543, fwd=14.7786 cmp_logits=0.0856, sampling=136.9257, total=152.8454
INFO 10-04 15:27:58 metrics.py:335] Avg prompt throughput: 13328.7 tokens/s, Avg generation throughput: 26.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 16.1%, CPU KV cache usage: 0.0%, Interval(ms): 153.4
INFO 10-04 15:27:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 153.2657 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:27:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 15:27:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.0834, fwd=14.8182 cmp_logits=0.0861, sampling=135.7539, total=151.7422
INFO 10-04 15:27:58 metrics.py:335] Avg prompt throughput: 13417.8 tokens/s, Avg generation throughput: 32.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 152.3
INFO 10-04 15:27:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 152.1735 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:27:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 15:27:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1415, fwd=14.7395 cmp_logits=0.0863, sampling=121.7051, total=137.6736
INFO 10-04 15:27:58 metrics.py:335] Avg prompt throughput: 14762.8 tokens/s, Avg generation throughput: 50.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 23.8%, CPU KV cache usage: 0.0%, Interval(ms): 138.4
INFO 10-04 15:27:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 138.2217 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 15:27:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 15:27:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1604, fwd=14.9176 cmp_logits=0.0861, sampling=140.7166, total=156.8813
INFO 10-04 15:27:58 metrics.py:335] Avg prompt throughput: 12947.3 tokens/s, Avg generation throughput: 50.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 31.1%, CPU KV cache usage: 0.0%, Interval(ms): 157.6
INFO 10-04 15:27:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 157.4619 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:27:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 15:27:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1268, fwd=14.8406 cmp_logits=0.0732, sampling=138.1667, total=154.2082
INFO 10-04 15:27:58 metrics.py:335] Avg prompt throughput: 13166.4 tokens/s, Avg generation throughput: 51.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 31.1%, CPU KV cache usage: 0.0%, Interval(ms): 154.9
INFO 10-04 15:27:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 154.7143 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:27:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2048]), positions.shape=torch.Size([2048]) hidden_states.shape=torch.Size([2048, 4096]) residual=None
INFO 10-04 15:27:58 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1780, fwd=14.8630 cmp_logits=0.0889, sampling=157.8615, total=173.9926
INFO 10-04 15:27:58 metrics.py:335] Avg prompt throughput: 11672.8 tokens/s, Avg generation throughput: 51.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.2%, CPU KV cache usage: 0.0%, Interval(ms): 174.8
INFO 10-04 15:27:58 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 174.5884 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:58 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:58 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1884]), positions.shape=torch.Size([1884]) hidden_states.shape=torch.Size([1884, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 1.1435, fwd=15.0249 cmp_logits=0.0885, sampling=144.9125, total=161.1698
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 11578.7 tokens/s, Avg generation throughput: 61.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.2%, CPU KV cache usage: 0.0%, Interval(ms): 161.9
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 161.7486 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4728, fwd=7.9994 cmp_logits=0.0708, sampling=12.4543, total=20.9978
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5499 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4807, fwd=7.9379 cmp_logits=0.0708, sampling=12.4989, total=20.9889
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5743 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4716, fwd=7.9379 cmp_logits=0.0706, sampling=12.4607, total=20.9415
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 461.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.4851 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4709, fwd=7.9722 cmp_logits=0.0706, sampling=12.4218, total=20.9365
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 461.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.4834 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4706, fwd=7.9820 cmp_logits=0.0699, sampling=12.4700, total=20.9935
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5430 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4663, fwd=7.9818 cmp_logits=0.0706, sampling=12.4538, total=20.9739
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5228 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4728, fwd=7.9608 cmp_logits=0.0710, sampling=12.4674, total=20.9730
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5683 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4687, fwd=7.9412 cmp_logits=0.0701, sampling=12.5175, total=20.9980
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5478 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4752, fwd=7.9100 cmp_logits=0.0784, sampling=12.5062, total=20.9706
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 461.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5147 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4647, fwd=7.9551 cmp_logits=0.0708, sampling=12.4543, total=20.9458
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 461.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.4927 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4668, fwd=7.9753 cmp_logits=0.0701, sampling=12.4559, total=20.9689
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5213 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4718, fwd=7.9556 cmp_logits=0.0696, sampling=12.5051, total=21.0030
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5864 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4656, fwd=7.9944 cmp_logits=0.0699, sampling=12.4497, total=20.9806
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5244 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4697, fwd=7.9694 cmp_logits=0.0727, sampling=12.4826, total=20.9954
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5478 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4632, fwd=7.9331 cmp_logits=0.0706, sampling=12.5468, total=21.0145
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5611 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4764, fwd=7.9293 cmp_logits=0.0732, sampling=12.5434, total=21.0233
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5728 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4683, fwd=7.9849 cmp_logits=0.0710, sampling=12.5370, total=21.0619
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 458.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6174 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4692, fwd=7.8967 cmp_logits=0.0701, sampling=12.6228, total=21.0595
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6067 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4680, fwd=7.9367 cmp_logits=0.0701, sampling=12.5854, total=21.0612
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6041 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4675, fwd=7.9477 cmp_logits=0.0699, sampling=12.5973, total=21.0834
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 458.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6560 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4532, fwd=8.0154 cmp_logits=0.0701, sampling=12.2268, total=20.7665
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 418.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2867 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4549, fwd=8.0237 cmp_logits=0.0713, sampling=12.2182, total=20.7691
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2946 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4494, fwd=8.0030 cmp_logits=0.0799, sampling=12.2499, total=20.7829
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.3025 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4487, fwd=7.9961 cmp_logits=0.0703, sampling=12.2294, total=20.7453
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2648 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4508, fwd=8.0187 cmp_logits=0.0701, sampling=12.2240, total=20.7648
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2798 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4518, fwd=7.9517 cmp_logits=0.0689, sampling=12.2945, total=20.7684
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2843 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4478, fwd=8.0333 cmp_logits=0.0720, sampling=12.2707, total=20.8244
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 418.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.5%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.3420 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4523, fwd=7.9265 cmp_logits=0.0699, sampling=12.3010, total=20.7505
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2970 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4117, fwd=8.0135 cmp_logits=0.0699, sampling=11.6439, total=20.1402
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 335.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5948 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4070, fwd=7.9405 cmp_logits=0.0701, sampling=11.7106, total=20.1290
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5913 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4122, fwd=7.9293 cmp_logits=0.0694, sampling=11.6959, total=20.1075
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.7
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5584 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4051, fwd=8.0235 cmp_logits=0.0701, sampling=11.6713, total=20.1709
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6234 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4063, fwd=7.9145 cmp_logits=0.0691, sampling=11.7345, total=20.1249
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.7
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5808 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4086, fwd=7.9229 cmp_logits=0.0701, sampling=11.7481, total=20.1509
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6091 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4091, fwd=7.9403 cmp_logits=0.0699, sampling=11.7316, total=20.1521
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6149 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4070, fwd=7.8993 cmp_logits=0.0696, sampling=11.7652, total=20.1421
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5948 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4096, fwd=7.9691 cmp_logits=0.0691, sampling=11.7548, total=20.2034
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6599 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4096, fwd=7.9565 cmp_logits=0.0699, sampling=11.7321, total=20.1690
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6196 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4115, fwd=7.9534 cmp_logits=0.0691, sampling=11.7748, total=20.2096
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6687 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4086, fwd=7.9947 cmp_logits=0.0696, sampling=11.6856, total=20.1597
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6146 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:27:59 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4091, fwd=7.9584 cmp_logits=0.0691, sampling=11.7459, total=20.1833
INFO 10-04 15:27:59 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:27:59 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6592 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:27:59 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:27:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3858, fwd=8.0099 cmp_logits=0.0834, sampling=11.4219, total=19.9022
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3252 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3874, fwd=7.9181 cmp_logits=0.0694, sampling=11.4739, total=19.8498
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.4
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2754 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3905, fwd=7.9117 cmp_logits=0.0696, sampling=11.4715, total=19.8443
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.4
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2725 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3972, fwd=7.9243 cmp_logits=0.0706, sampling=11.4923, total=19.8848
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3252 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3877, fwd=7.8986 cmp_logits=0.0684, sampling=11.5170, total=19.8729
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3063 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3874, fwd=7.9691 cmp_logits=0.0696, sampling=11.4758, total=19.9025
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3354 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3889, fwd=7.9253 cmp_logits=0.0699, sampling=11.4689, total=19.8538
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.4
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2813 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3901, fwd=7.8955 cmp_logits=0.0694, sampling=11.5085, total=19.8643
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.4
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2870 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3843, fwd=7.9451 cmp_logits=0.0687, sampling=11.4725, total=19.8717
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2961 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3862, fwd=7.9284 cmp_logits=0.0684, sampling=11.5125, total=19.8965
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3192 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3927, fwd=7.9424 cmp_logits=0.0684, sampling=11.4915, total=19.8960
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3693 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4046, fwd=7.9453 cmp_logits=0.0701, sampling=11.4806, total=19.9015
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3271 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3917, fwd=7.9947 cmp_logits=0.0703, sampling=11.4477, total=19.9051
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3288 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3898, fwd=7.9641 cmp_logits=0.0694, sampling=11.4613, total=19.8855
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3147 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3901, fwd=7.9892 cmp_logits=0.0691, sampling=11.4949, total=19.9442
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3679 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3924, fwd=7.9558 cmp_logits=0.0689, sampling=11.4706, total=19.8889
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3526 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3848, fwd=7.9377 cmp_logits=0.0691, sampling=11.5128, total=19.9051
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3300 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3877, fwd=7.9601 cmp_logits=0.0684, sampling=11.4818, total=19.8989
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3254 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3860, fwd=7.9358 cmp_logits=0.0691, sampling=11.4880, total=19.8798
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3032 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3970, fwd=7.9410 cmp_logits=0.0696, sampling=11.4839, total=19.8929
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3197 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3903, fwd=7.9367 cmp_logits=0.0696, sampling=11.5008, total=19.8982
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3547 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3874, fwd=7.9436 cmp_logits=0.0694, sampling=11.4896, total=19.8910
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3154 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3891, fwd=7.9863 cmp_logits=0.0694, sampling=11.4748, total=19.9211
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3464 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3886, fwd=7.9637 cmp_logits=0.0687, sampling=11.4880, total=19.9096
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3335 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3896, fwd=7.9424 cmp_logits=0.0689, sampling=11.5271, total=19.9287
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3540 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3936, fwd=7.9422 cmp_logits=0.0691, sampling=11.4827, total=19.8886
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3431 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3870, fwd=7.9658 cmp_logits=0.0694, sampling=11.4996, total=19.9227
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3483 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4060, fwd=7.9339 cmp_logits=0.0706, sampling=11.5430, total=19.9544
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3869 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3886, fwd=7.9339 cmp_logits=0.0689, sampling=11.5249, total=19.9172
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3395 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3912, fwd=7.8995 cmp_logits=0.0696, sampling=11.5387, total=19.8998
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3300 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3912, fwd=7.9589 cmp_logits=0.0701, sampling=11.5120, total=19.9327
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 291.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.4151 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3688, fwd=8.0059 cmp_logits=0.0689, sampling=11.1866, total=19.6316
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0193 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3664, fwd=7.9188 cmp_logits=0.0691, sampling=11.2343, total=19.5894
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9742 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3672, fwd=7.9312 cmp_logits=0.0689, sampling=11.2445, total=19.6128
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0043 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3669, fwd=7.9272 cmp_logits=0.0689, sampling=11.2324, total=19.5961
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9840 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3700, fwd=7.9689 cmp_logits=0.0694, sampling=11.2474, total=19.6567
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0791 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3676, fwd=7.9613 cmp_logits=0.0699, sampling=11.2422, total=19.6419
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0348 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3681, fwd=7.9629 cmp_logits=0.0701, sampling=11.2815, total=19.6836
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0703 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3660, fwd=7.9758 cmp_logits=0.0689, sampling=11.2162, total=19.6280
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0171 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3679, fwd=7.9613 cmp_logits=0.0691, sampling=11.2395, total=19.6388
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0293 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3705, fwd=7.9675 cmp_logits=0.0691, sampling=11.1954, total=19.6033
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0007 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3779, fwd=7.9706 cmp_logits=0.0694, sampling=11.1549, total=19.5735
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9630 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3760, fwd=7.9503 cmp_logits=0.0691, sampling=11.1952, total=19.5916
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9847 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3769, fwd=7.9679 cmp_logits=0.0694, sampling=11.1845, total=19.5994
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9921 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3669, fwd=7.9272 cmp_logits=0.0689, sampling=11.2045, total=19.5684
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9602 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3684, fwd=7.9694 cmp_logits=0.0689, sampling=11.1713, total=19.5794
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9857 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3662, fwd=7.9639 cmp_logits=0.0689, sampling=11.1887, total=19.5882
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9840 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3686, fwd=7.9794 cmp_logits=0.0699, sampling=11.1630, total=19.5816
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9711 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3710, fwd=8.0817 cmp_logits=0.0710, sampling=11.0705, total=19.5951
INFO 10-04 15:28:00 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:00 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9866 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:00 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:00 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3698, fwd=8.0168 cmp_logits=0.0684, sampling=11.1275, total=19.5835
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9735 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3712, fwd=7.9930 cmp_logits=0.0706, sampling=11.1701, total=19.6056
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0064 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3715, fwd=7.9932 cmp_logits=0.0694, sampling=11.1275, total=19.5625
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9549 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3674, fwd=8.0159 cmp_logits=0.0699, sampling=11.1015, total=19.5553
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9528 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3703, fwd=9.1367 cmp_logits=0.0682, sampling=11.1706, total=20.7467
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 234.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1401 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3681, fwd=8.0132 cmp_logits=0.0694, sampling=11.1108, total=19.5625
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9583 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3705, fwd=7.9598 cmp_logits=0.0696, sampling=11.1818, total=19.5827
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9816 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3674, fwd=7.9324 cmp_logits=0.0694, sampling=11.2035, total=19.5735
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9645 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3681, fwd=7.9501 cmp_logits=0.0684, sampling=11.1952, total=19.5827
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9666 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3691, fwd=7.9126 cmp_logits=0.0696, sampling=11.2042, total=19.5565
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9447 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3710, fwd=7.9329 cmp_logits=0.0691, sampling=11.1890, total=19.5627
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9530 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3693, fwd=7.9195 cmp_logits=0.0691, sampling=11.2205, total=19.5792
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9783 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3695, fwd=7.9060 cmp_logits=0.0691, sampling=11.2336, total=19.5792
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9726 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3786, fwd=7.9401 cmp_logits=0.0691, sampling=11.1916, total=19.5806
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9757 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3762, fwd=7.9815 cmp_logits=0.0694, sampling=11.1456, total=19.5739
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9633 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3724, fwd=7.9131 cmp_logits=0.0694, sampling=11.1845, total=19.5403
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9304 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3691, fwd=7.9122 cmp_logits=0.0687, sampling=11.2183, total=19.5689
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9721 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3667, fwd=7.8983 cmp_logits=0.0699, sampling=11.2212, total=19.5570
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9566 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3667, fwd=7.9248 cmp_logits=0.0684, sampling=11.2152, total=19.5756
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9714 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3681, fwd=7.9124 cmp_logits=0.0691, sampling=11.2331, total=19.5837
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9747 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3686, fwd=7.9699 cmp_logits=0.0706, sampling=11.1647, total=19.5744
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9637 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3664, fwd=7.9160 cmp_logits=0.0703, sampling=11.2226, total=19.5765
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9704 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3684, fwd=7.9026 cmp_logits=0.0687, sampling=11.2474, total=19.5880
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0450 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3262, fwd=7.9570 cmp_logits=0.0689, sampling=9.7098, total=18.0628
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3706 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3238, fwd=7.9215 cmp_logits=0.0694, sampling=9.7303, total=18.0459
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3489 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3250, fwd=7.9024 cmp_logits=0.0689, sampling=9.7485, total=18.0454
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3496 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3290, fwd=7.9582 cmp_logits=0.0689, sampling=9.6738, total=18.0304
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3406 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3223, fwd=7.9141 cmp_logits=0.0684, sampling=9.7277, total=18.0335
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3403 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3242, fwd=7.9081 cmp_logits=0.0689, sampling=9.7122, total=18.0142
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3203 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3228, fwd=7.8809 cmp_logits=0.0689, sampling=9.7644, total=18.0383
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3456 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3233, fwd=7.8993 cmp_logits=0.0687, sampling=9.7470, total=18.0395
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3809 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3035, fwd=8.0307 cmp_logits=0.0820, sampling=9.3789, total=17.7963
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0624 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3018, fwd=7.9346 cmp_logits=0.0679, sampling=9.4519, total=17.7567
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0180 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3021, fwd=7.9143 cmp_logits=0.0679, sampling=9.4833, total=17.7681
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0299 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3126, fwd=7.9603 cmp_logits=0.0684, sampling=9.4464, total=17.7886
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0523 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3018, fwd=7.8914 cmp_logits=0.0684, sampling=9.5088, total=17.7715
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0337 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3028, fwd=7.9477 cmp_logits=0.0684, sampling=9.4597, total=17.7796
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0473 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3002, fwd=7.9083 cmp_logits=0.0677, sampling=9.5112, total=17.7886
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0490 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3006, fwd=7.9610 cmp_logits=0.0682, sampling=9.4433, total=17.7739
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0371 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3009, fwd=7.9527 cmp_logits=0.0684, sampling=9.4724, total=17.7953
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0576 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3028, fwd=7.9644 cmp_logits=0.0689, sampling=9.4507, total=17.7877
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0478 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3021, fwd=7.9541 cmp_logits=0.0684, sampling=9.4349, total=17.7605
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0256 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3014, fwd=7.9911 cmp_logits=0.0679, sampling=9.4197, total=17.7808
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0411 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3016, fwd=7.9453 cmp_logits=0.0687, sampling=9.4955, total=17.8120
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0709 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3021, fwd=7.9386 cmp_logits=0.0679, sampling=9.5029, total=17.8127
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0750 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3052, fwd=7.9341 cmp_logits=0.0679, sampling=9.4702, total=17.7782
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0390 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2997, fwd=7.9362 cmp_logits=0.0684, sampling=9.4852, total=17.7903
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0609 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2990, fwd=7.9460 cmp_logits=0.0682, sampling=9.4588, total=17.7724
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0328 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3064, fwd=7.9775 cmp_logits=0.0675, sampling=9.4638, total=17.8161
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0769 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3016, fwd=7.9482 cmp_logits=0.0677, sampling=9.4621, total=17.7803
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0421 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2980, fwd=7.9169 cmp_logits=0.0679, sampling=9.4948, total=17.7784
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0395 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3018, fwd=7.9486 cmp_logits=0.0679, sampling=9.4652, total=17.7846
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0485 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:01 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3002, fwd=7.9241 cmp_logits=0.0679, sampling=9.4872, total=17.7805
INFO 10-04 15:28:01 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:01 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0428 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:01 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:01 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3052, fwd=8.0817 cmp_logits=0.0679, sampling=9.3534, total=17.8089
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0721 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3114, fwd=7.9298 cmp_logits=0.0689, sampling=9.4757, total=17.7865
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0504 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3037, fwd=7.9556 cmp_logits=0.0675, sampling=9.5396, total=17.8671
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1360 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3040, fwd=7.9553 cmp_logits=0.0687, sampling=9.5437, total=17.8723
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1410 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2999, fwd=7.9703 cmp_logits=0.0679, sampling=9.5191, total=17.8585
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1620 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2730, fwd=8.0712 cmp_logits=0.0696, sampling=9.5208, total=17.9353
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1453 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2718, fwd=8.0736 cmp_logits=0.0691, sampling=9.5019, total=17.9174
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1270 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2728, fwd=8.0411 cmp_logits=0.0684, sampling=9.5606, total=17.9436
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1522 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2820, fwd=8.0643 cmp_logits=0.0694, sampling=9.5215, total=17.9384
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1510 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2749, fwd=8.0512 cmp_logits=0.0689, sampling=9.5377, total=17.9338
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1427 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2725, fwd=8.0462 cmp_logits=0.0694, sampling=9.5429, total=17.9317
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1389 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2749, fwd=8.0705 cmp_logits=0.0694, sampling=9.5322, total=17.9479
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1577 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2732, fwd=8.0476 cmp_logits=0.0684, sampling=9.5389, total=17.9291
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1372 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2747, fwd=8.0957 cmp_logits=0.0689, sampling=9.4719, total=17.9124
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1234 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2720, fwd=8.0523 cmp_logits=0.0689, sampling=9.5105, total=17.9048
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1119 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2785, fwd=8.0359 cmp_logits=0.0689, sampling=9.5575, total=17.9422
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1501 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2713, fwd=8.0216 cmp_logits=0.0694, sampling=9.5558, total=17.9193
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1336 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2737, fwd=8.0299 cmp_logits=0.0682, sampling=9.5453, total=17.9183
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1284 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2713, fwd=8.0507 cmp_logits=0.0689, sampling=9.5487, total=17.9403
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1525 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2730, fwd=8.0681 cmp_logits=0.0687, sampling=9.5403, total=17.9510
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1599 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2718, fwd=8.0462 cmp_logits=0.0689, sampling=9.5365, total=17.9241
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1344 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2723, fwd=8.0302 cmp_logits=0.0691, sampling=9.5637, total=17.9360
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1470 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2804, fwd=8.0690 cmp_logits=0.0691, sampling=9.5153, total=17.9346
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1456 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2732, fwd=8.0957 cmp_logits=0.0694, sampling=9.5339, total=17.9727
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1849 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2747, fwd=8.0776 cmp_logits=0.0689, sampling=9.5034, total=17.9255
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1336 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2725, fwd=8.0352 cmp_logits=0.0687, sampling=9.5580, total=17.9350
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1453 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2725, fwd=8.0330 cmp_logits=0.0696, sampling=9.5448, total=17.9205
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1293 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2711, fwd=8.0416 cmp_logits=0.0696, sampling=9.5584, total=17.9415
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1513 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2742, fwd=8.0979 cmp_logits=0.0701, sampling=9.4881, total=17.9307
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1417 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2820, fwd=8.0101 cmp_logits=0.0706, sampling=9.5694, total=17.9329
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1417 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2720, fwd=8.0545 cmp_logits=0.0696, sampling=9.5301, total=17.9269
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1363 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2747, fwd=8.0345 cmp_logits=0.0684, sampling=9.5782, total=17.9565
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1656 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2720, fwd=8.0607 cmp_logits=0.0687, sampling=9.5248, total=17.9276
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1365 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:28:02 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=2048, max_num_seqs=256
INFO 10-04 15:28:02 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:28:02 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2730, fwd=8.0538 cmp_logits=0.0699, sampling=9.5246, total=17.9222
INFO 10-04 15:28:02 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 15:28:02 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1799 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055676.0782948, last_token_time=1728055679.545659, first_scheduled_time=1728055676.1145976, first_token_time=1728055677.8559666, time_in_queue=0.036302804946899414, finished_time=1728055679.545619, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055676.080121, last_token_time=1728055680.6229877, first_scheduled_time=1728055676.1145976, first_token_time=1728055678.0174108, time_in_queue=0.034476518630981445, finished_time=1728055680.6229618, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055676.0834413, last_token_time=1728055679.9876873, first_scheduled_time=1728055677.8565316, first_token_time=1728055678.0174108, time_in_queue=1.7730903625488281, finished_time=1728055679.9876635, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055676.0851882, last_token_time=1728055681.4502423, first_scheduled_time=1728055677.8565316, first_token_time=1728055678.1708314, time_in_queue=1.771343469619751, finished_time=1728055681.4502199, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055676.089973, last_token_time=1728055681.4502423, first_scheduled_time=1728055678.0178597, first_token_time=1728055678.323144, time_in_queue=1.927886724472046, finished_time=1728055681.4502258, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055676.0943713, last_token_time=1728055679.7174509, first_scheduled_time=1728055678.1712687, first_token_time=1728055678.4614735, time_in_queue=2.076897382736206, finished_time=1728055679.7174325, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055676.0955865, last_token_time=1728055679.7174509, first_scheduled_time=1728055678.3236518, first_token_time=1728055678.4614735, time_in_queue=2.228065252304077, finished_time=1728055679.7174368, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055676.0967739, last_token_time=1728055681.5983624, first_scheduled_time=1728055678.3236518, first_token_time=1728055678.6190841, time_in_queue=2.2268779277801514, finished_time=1728055681.5983493, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055676.1015458, last_token_time=1728055682.6205676, first_scheduled_time=1728055678.4620564, first_token_time=1728055678.9487736, time_in_queue=2.3605105876922607, finished_time=1728055682.620565, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055676.1086557, last_token_time=1728055682.0901172, first_scheduled_time=1728055678.774625, first_token_time=1728055679.110686, time_in_queue=2.6659693717956543, finished_time=1728055682.0901153, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 19139
End-to-End latency: 6.54 seconds
Throughput: 1.53 requests/s, 2925.33 tokens/s
Per_token_time: 0.342 ms

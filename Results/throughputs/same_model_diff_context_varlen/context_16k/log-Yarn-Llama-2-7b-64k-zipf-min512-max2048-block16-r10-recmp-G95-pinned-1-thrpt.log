Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=16384, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=16384, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Selected required requests 10
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Running vLLM with default batching strategy. max_num_batched_tokens=16384, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'type', 'finetuned', 'original_max_position_embeddings'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 05:31:14 config.py:654] [SchedulerConfig] max_num_batched_tokens: 16384 chunked_prefill_enabled: False
INFO 10-04 05:31:14 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 05:31:15 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 05:31:20 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 05:31:44 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 05:31:44 model_runner.py:183] Loaded model: 
INFO 10-04 05:31:44 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 05:31:44 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 05:31:44 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 05:31:44 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 05:31:44 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 05:31:44 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 05:31:44 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 05:31:44 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 05:31:44 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 05:31:44 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 05:31:44 model_runner.py:183]         )
INFO 10-04 05:31:44 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 05:31:44 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 05:31:44 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 05:31:44 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 05:31:44 model_runner.py:183]         )
INFO 10-04 05:31:44 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 05:31:44 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 05:31:44 model_runner.py:183]       )
INFO 10-04 05:31:44 model_runner.py:183]     )
INFO 10-04 05:31:44 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 05:31:44 model_runner.py:183]   )
INFO 10-04 05:31:44 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 05:31:44 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 05:31:44 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 05:31:44 model_runner.py:183] )
INFO 10-04 05:31:44 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16384]), positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 4096]) residual=None
INFO 10-04 05:31:45 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 12.5892, fwd=418.4017 cmp_logits=0.2420, sampling=850.9977, total=1282.2325
INFO 10-04 05:31:45 worker.py:164] Peak: 14.545 GB, Initial: 38.980 GB, Free: 24.435 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 22.879 GB
INFO 10-04 05:31:45 gpu_executor.py:117] # GPU blocks: 2928, # CPU blocks: 8192
INFO 10-04 05:32:16 worker.py:189] _init_cache_engine took 22.8750 GB
INFO 10-04 05:32:16 scheduler.py:307] Scheduler initialized with prompt limit: 16384
INFO 10-04 05:32:16 Start warmup...
INFO 10-04 05:32:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 05:32:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9642, fwd=11.8136 cmp_logits=72.8881, sampling=0.9410, total=86.6086
INFO 10-04 05:32:16 metrics.py:335] Avg prompt throughput: 9984.0 tokens/s, Avg generation throughput: 9.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 102.6
INFO 10-04 05:32:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.9820 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2947, fwd=61.1999 cmp_logits=0.1204, sampling=6.0921, total=67.7087
INFO 10-04 05:32:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 68.5
INFO 10-04 05:32:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 68.0835 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3037, fwd=8.1489 cmp_logits=0.0772, sampling=6.1989, total=14.7300
INFO 10-04 05:32:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.3
INFO 10-04 05:32:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.0135 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2916, fwd=8.0316 cmp_logits=0.0720, sampling=6.3124, total=14.7088
INFO 10-04 05:32:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.1
INFO 10-04 05:32:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.9486 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2861, fwd=8.1532 cmp_logits=0.0749, sampling=6.2101, total=14.7254
INFO 10-04 05:32:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.1
INFO 10-04 05:32:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.9505 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2830, fwd=8.2822 cmp_logits=0.0741, sampling=6.0530, total=14.6935
INFO 10-04 05:32:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.1
INFO 10-04 05:32:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.9686 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2737, fwd=8.2047 cmp_logits=0.0732, sampling=6.1080, total=14.6611
INFO 10-04 05:32:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.0
INFO 10-04 05:32:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.8630 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2642, fwd=8.2145 cmp_logits=0.0703, sampling=6.0995, total=14.6494
INFO 10-04 05:32:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.1
INFO 10-04 05:32:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.8699 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2728, fwd=8.2340 cmp_logits=0.0665, sampling=6.1235, total=14.6976
INFO 10-04 05:32:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.0
INFO 10-04 05:32:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.8854 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2680, fwd=7.9153 cmp_logits=0.0696, sampling=6.3839, total=14.6377
INFO 10-04 05:32:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.0
INFO 10-04 05:32:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.8406 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:22 Start benchmarking...
INFO 10-04 05:32:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10658]), positions.shape=torch.Size([10658]) hidden_states.shape=torch.Size([10658, 4096]) residual=None
INFO 10-04 05:32:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 4.1938, fwd=10.9177 cmp_logits=0.1123, sampling=665.3624, total=680.5871
INFO 10-04 05:32:22 metrics.py:335] Avg prompt throughput: 14959.1 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.0%, CPU KV cache usage: 0.0%, Interval(ms): 712.5
INFO 10-04 05:32:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 681.3951 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4265, fwd=7.7877 cmp_logits=0.0694, sampling=9.3460, total=17.6306
INFO 10-04 05:32:22 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 545.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 05:32:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0981 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4120, fwd=7.7088 cmp_logits=0.0675, sampling=9.3427, total=17.5316
INFO 10-04 05:32:22 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0383 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4110, fwd=7.7090 cmp_logits=0.0665, sampling=9.3446, total=17.5319
INFO 10-04 05:32:22 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9923 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4151, fwd=7.7226 cmp_logits=0.0660, sampling=9.3513, total=17.5560
INFO 10-04 05:32:22 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0240 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4106, fwd=7.7481 cmp_logits=0.0675, sampling=9.3269, total=17.5538
INFO 10-04 05:32:22 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0352 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:22 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4168, fwd=7.6513 cmp_logits=0.0668, sampling=9.4147, total=17.5507
INFO 10-04 05:32:22 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0202 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:22 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4213, fwd=7.7307 cmp_logits=0.0682, sampling=9.2931, total=17.5140
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9837 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4182, fwd=7.7457 cmp_logits=0.0668, sampling=9.3768, total=17.6086
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0795 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4232, fwd=7.8058 cmp_logits=0.0672, sampling=9.2220, total=17.5188
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0001 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4063, fwd=7.7281 cmp_logits=0.0675, sampling=9.2936, total=17.4963
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9789 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4184, fwd=7.7219 cmp_logits=0.0672, sampling=9.3443, total=17.5526
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0242 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4120, fwd=7.7150 cmp_logits=0.0679, sampling=9.3331, total=17.5290
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0042 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4113, fwd=7.7002 cmp_logits=0.0677, sampling=9.3591, total=17.5390
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0228 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4117, fwd=7.6990 cmp_logits=0.0675, sampling=9.3398, total=17.5192
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9982 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4144, fwd=7.7147 cmp_logits=0.0679, sampling=9.3482, total=17.5462
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0187 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4127, fwd=7.7515 cmp_logits=0.0672, sampling=9.3110, total=17.5428
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0209 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4077, fwd=7.6926 cmp_logits=0.0670, sampling=9.3961, total=17.5643
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0352 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4284, fwd=7.6993 cmp_logits=0.0756, sampling=9.3675, total=17.5717
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0478 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4168, fwd=7.6790 cmp_logits=0.0665, sampling=9.4006, total=17.5636
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0345 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4132, fwd=7.6990 cmp_logits=0.0672, sampling=9.3534, total=17.5338
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0128 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4086, fwd=7.6346 cmp_logits=0.0670, sampling=9.4342, total=17.5452
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0163 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4125, fwd=7.7546 cmp_logits=0.0684, sampling=9.3496, total=17.5862
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0540 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4141, fwd=7.6780 cmp_logits=0.0675, sampling=9.4206, total=17.5812
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0547 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4117, fwd=7.6754 cmp_logits=0.0660, sampling=9.4018, total=17.5560
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0287 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4103, fwd=7.6938 cmp_logits=0.0660, sampling=9.3999, total=17.5707
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 547.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0812 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4098, fwd=7.7167 cmp_logits=0.0670, sampling=9.3589, total=17.5533
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 548.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0418 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4067, fwd=7.7817 cmp_logits=0.0894, sampling=9.0213, total=17.3004
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 501.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.0
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7460 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3996, fwd=7.7131 cmp_logits=0.0665, sampling=9.0661, total=17.2460
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 503.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7197 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3691, fwd=7.7596 cmp_logits=0.0782, sampling=8.6467, total=16.8545
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2434 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3638, fwd=7.7209 cmp_logits=0.0665, sampling=8.6617, total=16.8140
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1983 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3667, fwd=7.7610 cmp_logits=0.0670, sampling=8.7011, total=16.8965
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2820 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3777, fwd=7.6876 cmp_logits=0.0656, sampling=8.7996, total=16.9308
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3185 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3660, fwd=7.7062 cmp_logits=0.0665, sampling=8.7566, total=16.8962
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2794 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3669, fwd=7.7653 cmp_logits=0.0663, sampling=8.6982, total=16.8974
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3054 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3691, fwd=7.6888 cmp_logits=0.0668, sampling=8.7862, total=16.9117
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3154 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3517, fwd=7.7448 cmp_logits=0.0784, sampling=8.5013, total=16.6769
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0324 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3519, fwd=7.7178 cmp_logits=0.0656, sampling=8.5387, total=16.6752
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0302 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3531, fwd=7.7477 cmp_logits=0.0663, sampling=8.4846, total=16.6523
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0088 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3541, fwd=7.7507 cmp_logits=0.0658, sampling=8.5201, total=16.6914
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0512 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3521, fwd=7.6909 cmp_logits=0.0663, sampling=8.5483, total=16.6585
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0250 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3655, fwd=7.6892 cmp_logits=0.0665, sampling=8.5657, total=16.6879
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0434 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3517, fwd=7.7393 cmp_logits=0.0660, sampling=8.5142, total=16.6724
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0305 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3512, fwd=7.7035 cmp_logits=0.0663, sampling=8.5361, total=16.6581
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0147 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3541, fwd=7.6928 cmp_logits=0.0656, sampling=8.5580, total=16.6714
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0290 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3519, fwd=7.7450 cmp_logits=0.0660, sampling=8.4889, total=16.6528
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0066 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3514, fwd=7.7398 cmp_logits=0.0668, sampling=8.5328, total=16.6919
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0493 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3543, fwd=7.7133 cmp_logits=0.0670, sampling=8.5120, total=16.6481
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0064 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3512, fwd=7.7367 cmp_logits=0.0665, sampling=8.5704, total=16.7260
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 346.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1497 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3388, fwd=7.7918 cmp_logits=0.0775, sampling=8.1255, total=16.3341
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6669 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3397, fwd=7.7109 cmp_logits=0.0658, sampling=8.1778, total=16.2952
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6211 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3371, fwd=7.7512 cmp_logits=0.0663, sampling=8.1630, total=16.3186
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6404 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3440, fwd=7.7190 cmp_logits=0.0663, sampling=8.1983, total=16.3283
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6566 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3364, fwd=7.7021 cmp_logits=0.0672, sampling=8.2173, total=16.3240
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6509 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3362, fwd=7.7569 cmp_logits=0.0670, sampling=8.2152, total=16.3763
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7041 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3345, fwd=7.7093 cmp_logits=0.0668, sampling=8.1801, total=16.2914
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6152 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3350, fwd=7.7090 cmp_logits=0.0658, sampling=8.1959, total=16.3064
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6304 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3448, fwd=7.6659 cmp_logits=0.0660, sampling=8.2545, total=16.3321
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6574 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3321, fwd=7.7174 cmp_logits=0.0663, sampling=8.2238, total=16.3403
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6645 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3321, fwd=7.7322 cmp_logits=0.0660, sampling=8.1708, total=16.3019
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6271 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3386, fwd=7.7169 cmp_logits=0.0658, sampling=8.1904, total=16.3124
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6407 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3366, fwd=7.7412 cmp_logits=0.0665, sampling=8.1635, total=16.3088
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6366 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3421, fwd=7.6849 cmp_logits=0.0663, sampling=8.2171, total=16.3114
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6383 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:23 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3343, fwd=7.7915 cmp_logits=0.0660, sampling=8.1289, total=16.3217
INFO 10-04 05:32:23 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:32:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6497 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:23 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3352, fwd=7.7367 cmp_logits=0.0665, sampling=8.2018, total=16.3417
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6709 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3335, fwd=7.6957 cmp_logits=0.0658, sampling=8.2433, total=16.3393
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.6%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7086 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3035, fwd=7.7801 cmp_logits=0.0775, sampling=7.2339, total=15.3956
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 188.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.6%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6536 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3026, fwd=7.6842 cmp_logits=0.0656, sampling=7.3237, total=15.3773
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6546 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2859, fwd=7.7133 cmp_logits=0.0758, sampling=7.2193, total=15.2957
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5160 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2851, fwd=7.6404 cmp_logits=0.0651, sampling=7.2999, total=15.2917
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5132 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2832, fwd=7.6814 cmp_logits=0.0658, sampling=7.2677, total=15.2988
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5170 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2809, fwd=7.6625 cmp_logits=0.0653, sampling=7.2649, total=15.2743
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5272 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2840, fwd=7.7155 cmp_logits=0.0660, sampling=7.2250, total=15.2912
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5125 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2837, fwd=7.6499 cmp_logits=0.0651, sampling=7.3054, total=15.3048
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5246 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2813, fwd=7.7307 cmp_logits=0.0656, sampling=7.2124, total=15.2910
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5127 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2823, fwd=7.6551 cmp_logits=0.0656, sampling=7.2980, total=15.3019
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5213 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2816, fwd=7.7393 cmp_logits=0.0646, sampling=7.2129, total=15.2996
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5220 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2890, fwd=7.6659 cmp_logits=0.0653, sampling=7.3135, total=15.3344
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5764 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2601, fwd=7.8936 cmp_logits=0.0668, sampling=7.2129, total=15.4343
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6107 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2601, fwd=7.8759 cmp_logits=0.0663, sampling=7.2260, total=15.4293
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6066 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2601, fwd=7.8776 cmp_logits=0.0663, sampling=7.2217, total=15.4269
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6024 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2592, fwd=7.8917 cmp_logits=0.0668, sampling=7.2348, total=15.4531
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6298 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2615, fwd=8.0187 cmp_logits=0.0663, sampling=7.0853, total=15.4326
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6088 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2618, fwd=7.8762 cmp_logits=0.0663, sampling=7.2320, total=15.4371
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6140 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2582, fwd=7.8657 cmp_logits=0.0660, sampling=7.2432, total=15.4340
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6100 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2601, fwd=7.8464 cmp_logits=0.0668, sampling=7.2880, total=15.4624
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6381 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2594, fwd=7.8659 cmp_logits=0.0658, sampling=7.2320, total=15.4238
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5978 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2625, fwd=7.8177 cmp_logits=0.0663, sampling=7.2930, total=15.4405
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6171 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2604, fwd=7.9448 cmp_logits=0.0677, sampling=7.1640, total=15.4381
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6121 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2618, fwd=7.8921 cmp_logits=0.0663, sampling=7.2482, total=15.4691
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6457 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:32:24 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:32:24 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2613, fwd=7.8583 cmp_logits=0.0665, sampling=7.2379, total=15.4247
INFO 10-04 05:32:24 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:32:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6245 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728019942.180522, last_token_time=1728019943.3562932, first_scheduled_time=1728019942.2017787, first_token_time=1728019942.882551, time_in_queue=0.021256685256958008, finished_time=1728019943.3562517, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728019942.1823423, last_token_time=1728019943.737952, first_scheduled_time=1728019942.2017787, first_token_time=1728019942.882551, time_in_queue=0.019436359405517578, finished_time=1728019943.7379243, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728019942.1842613, last_token_time=1728019943.5142884, first_scheduled_time=1728019942.2017787, first_token_time=1728019942.882551, time_in_queue=0.0175173282623291, finished_time=1728019943.5142646, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728019942.18555, last_token_time=1728019944.0237288, first_scheduled_time=1728019942.2017787, first_token_time=1728019942.882551, time_in_queue=0.016228675842285156, finished_time=1728019944.0237072, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728019942.188034, last_token_time=1728019944.0237288, first_scheduled_time=1728019942.2017787, first_token_time=1728019942.882551, time_in_queue=0.013744592666625977, finished_time=1728019944.0237136, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728019942.190406, last_token_time=1728019943.3921344, first_scheduled_time=1728019942.2017787, first_token_time=1728019942.882551, time_in_queue=0.011372566223144531, finished_time=1728019943.3921165, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728019942.1915088, last_token_time=1728019943.3921344, first_scheduled_time=1728019942.2017787, first_token_time=1728019942.882551, time_in_queue=0.010269880294799805, finished_time=1728019943.3921208, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728019942.1925526, last_token_time=1728019944.0554335, first_scheduled_time=1728019942.2017787, first_token_time=1728019942.882551, time_in_queue=0.009226083755493164, finished_time=1728019944.0554209, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728019942.1950674, last_token_time=1728019944.4170055, first_scheduled_time=1728019942.2017787, first_token_time=1728019942.882551, time_in_queue=0.006711244583129883, finished_time=1728019944.4170027, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728019942.198556, last_token_time=1728019944.2121305, first_scheduled_time=1728019942.2017787, first_token_time=1728019942.882551, time_in_queue=0.0032227039337158203, finished_time=1728019944.2121289, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 11197
End-to-End latency: 2.24 seconds
Throughput: 4.47 requests/s, 5006.08 tokens/s
Per_token_time: 0.200 ms

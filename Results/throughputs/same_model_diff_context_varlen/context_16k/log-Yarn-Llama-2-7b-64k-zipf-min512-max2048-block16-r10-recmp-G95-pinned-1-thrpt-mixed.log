Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=2048, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=16384, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=16384, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=True)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Selected required requests 10
User Requests: prompt_len=524, output_len=27, sequence_len=551...
User Requests: prompt_len=967, output_len=49, sequence_len=1016...
User Requests: prompt_len=702, output_len=36, sequence_len=738...
User Requests: prompt_len=1314, output_len=66, sequence_len=1380...
User Requests: prompt_len=1300, output_len=66, sequence_len=1366...
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=566, output_len=29, sequence_len=595...
User Requests: prompt_len=1356, output_len=68, sequence_len=1424...
User Requests: prompt_len=1809, output_len=91, sequence_len=1900...
User Requests: prompt_len=1556, output_len=78, sequence_len=1634...
Mixed batch is enabled. max_num_batched_tokens=16384, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings', 'type', 'finetuned'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 05:32:50 config.py:654] [SchedulerConfig] max_num_batched_tokens: 16384 chunked_prefill_enabled: False
INFO 10-04 05:32:50 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=True)
INFO 10-04 05:32:51 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 05:32:55 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 05:32:59 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 05:32:59 model_runner.py:183] Loaded model: 
INFO 10-04 05:32:59 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 05:32:59 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 05:32:59 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 05:32:59 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 05:32:59 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 05:32:59 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 05:32:59 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 05:32:59 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 05:32:59 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 05:32:59 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 05:32:59 model_runner.py:183]         )
INFO 10-04 05:32:59 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 05:32:59 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 05:32:59 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 05:32:59 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 05:32:59 model_runner.py:183]         )
INFO 10-04 05:32:59 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 05:32:59 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 05:32:59 model_runner.py:183]       )
INFO 10-04 05:32:59 model_runner.py:183]     )
INFO 10-04 05:32:59 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 05:32:59 model_runner.py:183]   )
INFO 10-04 05:32:59 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 05:32:59 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 05:32:59 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 05:32:59 model_runner.py:183] )
INFO 10-04 05:32:59 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([16384]), positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 4096]) residual=None
INFO 10-04 05:33:00 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 6.2683, fwd=366.8299 cmp_logits=0.2363, sampling=851.5427, total=1224.8790
INFO 10-04 05:33:00 worker.py:164] Peak: 14.545 GB, Initial: 38.980 GB, Free: 24.435 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 22.879 GB
INFO 10-04 05:33:00 gpu_executor.py:117] # GPU blocks: 2928, # CPU blocks: 8192
INFO 10-04 05:33:32 worker.py:189] _init_cache_engine took 22.8750 GB
INFO 10-04 05:33:32 scheduler.py:307] Scheduler initialized with prompt limit: 16384
INFO 10-04 05:33:32 Start warmup...
INFO 10-04 05:33:32 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 05:33:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9015, fwd=11.8723 cmp_logits=72.7806, sampling=0.9582, total=86.5147
INFO 10-04 05:33:32 metrics.py:335] Avg prompt throughput: 8809.5 tokens/s, Avg generation throughput: 8.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 116.2
INFO 10-04 05:33:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 87.0039 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:32 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3073, fwd=43.6294 cmp_logits=0.1144, sampling=6.4962, total=50.5497
INFO 10-04 05:33:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 51.5
INFO 10-04 05:33:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.9772 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:32 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3154, fwd=8.1213 cmp_logits=0.0784, sampling=6.9506, total=15.4679
INFO 10-04 05:33:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 10-04 05:33:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7847 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:32 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2863, fwd=8.0287 cmp_logits=0.0763, sampling=7.0667, total=15.4595
INFO 10-04 05:33:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 05:33:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7158 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:32 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2997, fwd=8.1959 cmp_logits=0.0734, sampling=6.8946, total=15.4648
INFO 10-04 05:33:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 05:33:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7051 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:32 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2916, fwd=8.2042 cmp_logits=0.0732, sampling=6.8586, total=15.4293
INFO 10-04 05:33:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:33:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6677 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:32 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2754, fwd=8.3899 cmp_logits=0.0756, sampling=5.8982, total=14.6406
INFO 10-04 05:33:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 15.1
INFO 10-04 05:33:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.8692 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:32 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2763, fwd=8.2192 cmp_logits=0.0708, sampling=5.6791, total=14.2465
INFO 10-04 05:33:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 14.7
INFO 10-04 05:33:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4961 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:32 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2687, fwd=7.9930 cmp_logits=0.0708, sampling=5.8999, total=14.2331
INFO 10-04 05:33:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 05:33:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4339 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:32 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:32 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:32 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2694, fwd=7.9675 cmp_logits=0.0682, sampling=5.8997, total=14.2052
INFO 10-04 05:33:32 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 05:33:32 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4153 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:37 Start benchmarking...
INFO 10-04 05:33:37 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:37 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10658]), positions.shape=torch.Size([10658]) hidden_states.shape=torch.Size([10658, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 3.5574, fwd=10.8566 cmp_logits=0.1140, sampling=628.6952, total=643.2242
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 15773.6 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.0%, CPU KV cache usage: 0.0%, Interval(ms): 675.7
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 644.1910 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4351, fwd=7.8838 cmp_logits=0.0691, sampling=9.1946, total=17.5834
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 545.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0581 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4046, fwd=7.7205 cmp_logits=0.0665, sampling=9.2905, total=17.4832
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 552.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9453 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3994, fwd=7.7350 cmp_logits=0.0689, sampling=9.2695, total=17.4735
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 552.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9381 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3972, fwd=7.7434 cmp_logits=0.0672, sampling=9.2750, total=17.4832
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9460 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3972, fwd=7.7055 cmp_logits=0.0665, sampling=9.3000, total=17.4701
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9498 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4017, fwd=7.6852 cmp_logits=0.0668, sampling=9.3076, total=17.4620
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9255 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3996, fwd=7.7839 cmp_logits=0.0691, sampling=9.2027, total=17.4563
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 552.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9250 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4160, fwd=7.7679 cmp_logits=0.0675, sampling=9.2661, total=17.5180
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9980 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4148, fwd=7.7891 cmp_logits=0.0672, sampling=9.2051, total=17.4770
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9529 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4153, fwd=7.7341 cmp_logits=0.0670, sampling=9.2454, total=17.4627
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 552.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9334 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4027, fwd=7.7684 cmp_logits=0.0665, sampling=9.2239, total=17.4625
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9415 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4027, fwd=7.7462 cmp_logits=0.0663, sampling=9.2421, total=17.4582
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 552.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9307 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4025, fwd=7.7703 cmp_logits=0.0672, sampling=9.2576, total=17.4985
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9746 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4025, fwd=7.7658 cmp_logits=0.0672, sampling=9.2351, total=17.4718
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 552.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9405 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4051, fwd=7.7777 cmp_logits=0.0675, sampling=9.2537, total=17.5047
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9720 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4017, fwd=7.6821 cmp_logits=0.0675, sampling=9.3544, total=17.5066
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9842 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4039, fwd=7.6776 cmp_logits=0.0672, sampling=9.3520, total=17.5014
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9670 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4005, fwd=7.6649 cmp_logits=0.0670, sampling=9.3570, total=17.4901
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9646 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4058, fwd=7.6835 cmp_logits=0.0677, sampling=9.3420, total=17.4997
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 551.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9715 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4027, fwd=7.6737 cmp_logits=0.0665, sampling=9.3744, total=17.5183
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0297 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4020, fwd=7.6959 cmp_logits=0.0668, sampling=9.3560, total=17.5216
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9887 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4020, fwd=7.6928 cmp_logits=0.0672, sampling=9.3930, total=17.5557
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0318 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4158, fwd=7.7713 cmp_logits=0.0670, sampling=9.2738, total=17.5290
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0073 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4017, fwd=7.7956 cmp_logits=0.0677, sampling=9.2361, total=17.5016
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9842 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4001, fwd=7.6754 cmp_logits=0.0672, sampling=9.3629, total=17.5066
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0194 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4082, fwd=7.6668 cmp_logits=0.0665, sampling=9.3617, total=17.5042
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 550.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.2%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0018 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3948, fwd=7.7448 cmp_logits=0.0806, sampling=9.0022, total=17.2231
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 503.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6761 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3920, fwd=7.7460 cmp_logits=0.0668, sampling=9.0182, total=17.2238
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 503.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%, Interval(ms): 17.9
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.7155 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3619, fwd=7.7643 cmp_logits=0.0796, sampling=8.5838, total=16.7904
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1878 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3622, fwd=7.7176 cmp_logits=0.0660, sampling=8.6615, total=16.8078
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2381 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3598, fwd=7.7634 cmp_logits=0.0670, sampling=8.5788, total=16.7699
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 403.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.3
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.1778 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3622, fwd=7.7183 cmp_logits=0.0663, sampling=8.7075, total=16.8552
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2498 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3595, fwd=7.7262 cmp_logits=0.0660, sampling=8.6987, total=16.8509
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 402.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2439 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3583, fwd=7.7608 cmp_logits=0.0663, sampling=8.6977, total=16.8841
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 401.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2811 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3581, fwd=7.7581 cmp_logits=0.0668, sampling=8.6787, total=16.8624
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3059 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3493, fwd=7.7987 cmp_logits=0.0784, sampling=8.4105, total=16.6380
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0100 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3471, fwd=7.7651 cmp_logits=0.0658, sampling=8.4186, total=16.5973
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9661 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3507, fwd=7.7078 cmp_logits=0.0660, sampling=8.4810, total=16.6066
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9680 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3448, fwd=7.7355 cmp_logits=0.0665, sampling=8.4624, total=16.6101
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9740 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3457, fwd=7.7548 cmp_logits=0.0663, sampling=8.4493, total=16.6171
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0197 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3536, fwd=7.7646 cmp_logits=0.0670, sampling=8.4026, total=16.5887
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9544 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3433, fwd=7.7639 cmp_logits=0.0658, sampling=8.4491, total=16.6230
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9845 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:33:38 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3469, fwd=7.7791 cmp_logits=0.0663, sampling=8.4276, total=16.6209
INFO 10-04 05:33:38 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:33:38 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9804 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:33:38 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:38 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3471, fwd=7.7393 cmp_logits=0.0658, sampling=8.4600, total=16.6132
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9816 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3445, fwd=7.7169 cmp_logits=0.0660, sampling=8.4753, total=16.6037
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 349.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9959 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3467, fwd=7.7333 cmp_logits=0.0663, sampling=8.4565, total=16.6037
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9647 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3433, fwd=7.7353 cmp_logits=0.0663, sampling=8.4641, total=16.6099
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 350.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9749 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3517, fwd=7.6830 cmp_logits=0.0668, sampling=8.5802, total=16.6821
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 17.2
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.0615 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3314, fwd=7.8304 cmp_logits=0.0792, sampling=8.0202, total=16.2616
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6006 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3324, fwd=7.7443 cmp_logits=0.0658, sampling=8.0886, total=16.2320
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6090 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3321, fwd=7.7562 cmp_logits=0.0660, sampling=8.0965, total=16.2518
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5904 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3331, fwd=7.7386 cmp_logits=0.0658, sampling=8.1172, total=16.2556
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5939 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3304, fwd=7.7591 cmp_logits=0.0675, sampling=8.0914, total=16.2492
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5842 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3307, fwd=7.7474 cmp_logits=0.0663, sampling=8.1096, total=16.2547
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5894 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3307, fwd=7.7457 cmp_logits=0.0660, sampling=8.1279, total=16.2714
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6373 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3321, fwd=7.7322 cmp_logits=0.0665, sampling=8.1301, total=16.2616
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5966 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3307, fwd=7.7863 cmp_logits=0.0653, sampling=8.0767, total=16.2599
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5932 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3402, fwd=7.7040 cmp_logits=0.0732, sampling=8.1491, total=16.2675
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5977 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3295, fwd=7.7331 cmp_logits=0.0658, sampling=8.1258, total=16.2551
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5894 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3321, fwd=7.7965 cmp_logits=0.0663, sampling=8.1165, total=16.3124
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6495 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3307, fwd=7.7279 cmp_logits=0.0656, sampling=8.1720, total=16.2971
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 297.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6306 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3293, fwd=7.7903 cmp_logits=0.0660, sampling=8.0898, total=16.2764
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6094 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3450, fwd=7.7288 cmp_logits=0.0663, sampling=8.1348, total=16.2756
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6066 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3316, fwd=7.7696 cmp_logits=0.0668, sampling=8.1079, total=16.2768
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 298.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.5%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6125 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3307, fwd=7.8075 cmp_logits=0.0660, sampling=8.1213, total=16.3262
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 296.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.6%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7015 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2961, fwd=7.7698 cmp_logits=0.0772, sampling=7.2005, total=15.3446
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.6%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6138 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2992, fwd=7.6475 cmp_logits=0.0651, sampling=7.3066, total=15.3193
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 190.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6085 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2789, fwd=7.7310 cmp_logits=0.0758, sampling=7.1759, total=15.2624
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4965 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2763, fwd=7.6785 cmp_logits=0.0646, sampling=7.2193, total=15.2402
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4738 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2775, fwd=7.6799 cmp_logits=0.0651, sampling=7.2556, total=15.2788
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5115 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2768, fwd=7.6373 cmp_logits=0.0653, sampling=7.2427, total=15.2225
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4552 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2768, fwd=7.6463 cmp_logits=0.0653, sampling=7.2258, total=15.2154
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4450 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2861, fwd=7.6177 cmp_logits=0.0651, sampling=7.2815, total=15.2514
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4810 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2768, fwd=7.6084 cmp_logits=0.0663, sampling=7.2739, total=15.2261
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4569 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2768, fwd=7.6776 cmp_logits=0.0648, sampling=7.2584, total=15.2786
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5096 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2754, fwd=7.6509 cmp_logits=0.0651, sampling=7.2339, total=15.2259
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4588 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2754, fwd=7.6520 cmp_logits=0.0641, sampling=7.2331, total=15.2259
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4791 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2563, fwd=7.8614 cmp_logits=0.0670, sampling=7.1726, total=15.3582
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5497 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2558, fwd=7.8158 cmp_logits=0.0668, sampling=7.2193, total=15.3584
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5456 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2546, fwd=7.8545 cmp_logits=0.0665, sampling=7.2367, total=15.4130
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6016 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2561, fwd=7.8187 cmp_logits=0.0668, sampling=7.2594, total=15.4016
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5890 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2546, fwd=7.7937 cmp_logits=0.0663, sampling=7.2603, total=15.3756
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5632 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2556, fwd=7.8347 cmp_logits=0.0665, sampling=7.2012, total=15.3587
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5466 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2553, fwd=7.8294 cmp_logits=0.0660, sampling=7.2107, total=15.3627
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5506 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2637, fwd=7.8278 cmp_logits=0.0658, sampling=7.2784, total=15.4364
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6233 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2563, fwd=7.8163 cmp_logits=0.0663, sampling=7.2455, total=15.3854
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5725 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2565, fwd=7.8177 cmp_logits=0.0668, sampling=7.2260, total=15.3677
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5544 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2558, fwd=7.8063 cmp_logits=0.0660, sampling=7.2381, total=15.3670
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5559 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2568, fwd=7.8170 cmp_logits=0.0663, sampling=7.2498, total=15.3909
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5773 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 05:33:39 scheduler.py:990] Scheduling mixed batching: max_num_batched_tokens=16384, max_num_seqs=256
INFO 10-04 05:33:39 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 05:33:39 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2568, fwd=7.8652 cmp_logits=0.0660, sampling=7.2155, total=15.4045
INFO 10-04 05:33:39 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 10-04 05:33:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6167 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020017.5595007, last_token_time=1728020018.697098, first_scheduled_time=1728020017.5813148, first_token_time=1728020018.224726, time_in_queue=0.02181410789489746, finished_time=1728020018.697058, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020017.5611818, last_token_time=1728020019.077659, first_scheduled_time=1728020017.5813148, first_token_time=1728020018.224726, time_in_queue=0.020133018493652344, finished_time=1728020019.0776336, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020017.5630958, last_token_time=1728020018.8547351, first_scheduled_time=1728020017.5813148, first_token_time=1728020018.224726, time_in_queue=0.018218994140625, finished_time=1728020018.8547115, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020017.564365, last_token_time=1728020019.362716, first_scheduled_time=1728020017.5813148, first_token_time=1728020018.224726, time_in_queue=0.016949892044067383, finished_time=1728020019.362695, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020017.5670383, last_token_time=1728020019.362716, first_scheduled_time=1728020017.5813148, first_token_time=1728020018.224726, time_in_queue=0.014276504516601562, finished_time=1728020019.3627007, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020017.5694547, last_token_time=1728020018.7328634, first_scheduled_time=1728020017.5813148, first_token_time=1728020018.224726, time_in_queue=0.011860132217407227, finished_time=1728020018.7328455, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020017.5705912, last_token_time=1728020018.7328634, first_scheduled_time=1728020017.5813148, first_token_time=1728020018.224726, time_in_queue=0.010723590850830078, finished_time=1728020018.7328498, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020017.5716507, last_token_time=1728020019.3942964, first_scheduled_time=1728020017.5813148, first_token_time=1728020018.224726, time_in_queue=0.009664058685302734, finished_time=1728020019.3942835, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020017.5742714, last_token_time=1728020019.7547355, first_scheduled_time=1728020017.5813148, first_token_time=1728020018.224726, time_in_queue=0.007043361663818359, finished_time=1728020019.7547328, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728020017.5778687, last_token_time=1728020019.55051, first_scheduled_time=1728020017.5813148, first_token_time=1728020018.224726, time_in_queue=0.0034461021423339844, finished_time=1728020019.550508, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 11197
End-to-End latency: 2.20 seconds
Throughput: 4.55 requests/s, 5100.13 tokens/s
Per_token_time: 0.196 ms

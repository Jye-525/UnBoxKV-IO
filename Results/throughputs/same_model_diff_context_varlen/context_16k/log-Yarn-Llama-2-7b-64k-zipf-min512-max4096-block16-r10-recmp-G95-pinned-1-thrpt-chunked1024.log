Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=10, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='zipf', input_len=1024, output_len=10, min_len=512, max_len=4096, prefill_to_decode_ratio=20.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=16384, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=1024, max_num_seqs=256, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 10 requests, requiring 10 requests.
Original order of the requests
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=1585, output_len=80, sequence_len=1665...
User Requests: prompt_len=970, output_len=49, sequence_len=1019...
User Requests: prompt_len=2398, output_len=120, sequence_len=2518...
User Requests: prompt_len=2366, output_len=119, sequence_len=2485...
User Requests: prompt_len=652, output_len=33, sequence_len=685...
User Requests: prompt_len=658, output_len=33, sequence_len=691...
User Requests: prompt_len=2495, output_len=125, sequence_len=2620...
User Requests: prompt_len=3566, output_len=179, sequence_len=3745...
User Requests: prompt_len=2969, output_len=149, sequence_len=3118...
Selected required requests 10
User Requests: prompt_len=564, output_len=29, sequence_len=593...
User Requests: prompt_len=1585, output_len=80, sequence_len=1665...
User Requests: prompt_len=970, output_len=49, sequence_len=1019...
User Requests: prompt_len=2398, output_len=120, sequence_len=2518...
User Requests: prompt_len=2366, output_len=119, sequence_len=2485...
User Requests: prompt_len=652, output_len=33, sequence_len=685...
User Requests: prompt_len=658, output_len=33, sequence_len=691...
User Requests: prompt_len=2495, output_len=125, sequence_len=2620...
User Requests: prompt_len=3566, output_len=179, sequence_len=3745...
User Requests: prompt_len=2969, output_len=149, sequence_len=3118...
Chunked prefill is enabled. max_num_batched_tokens=1024, max_num_batched_seqs=256
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings', 'finetuned', 'type'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 10-04 15:28:28 config.py:654] [SchedulerConfig] max_num_batched_tokens: 1024 chunked_prefill_enabled: True
INFO 10-04 15:28:28 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
INFO 10-04 15:28:29 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 10-04 15:28:33 selector.py:27] Using FlashAttention-2 backend.
INFO 10-04 15:28:37 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 10-04 15:28:37 model_runner.py:183] Loaded model: 
INFO 10-04 15:28:37 model_runner.py:183]  LlamaForCausalLM(
INFO 10-04 15:28:37 model_runner.py:183]   (model): LlamaModel(
INFO 10-04 15:28:37 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 15:28:37 model_runner.py:183]     (layers): ModuleList(
INFO 10-04 15:28:37 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 10-04 15:28:37 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 10-04 15:28:37 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 10-04 15:28:37 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 15:28:37 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 10-04 15:28:37 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 10-04 15:28:37 model_runner.py:183]         )
INFO 10-04 15:28:37 model_runner.py:183]         (mlp): LlamaMLP(
INFO 10-04 15:28:37 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 10-04 15:28:37 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 10-04 15:28:37 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 10-04 15:28:37 model_runner.py:183]         )
INFO 10-04 15:28:37 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 15:28:37 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 15:28:37 model_runner.py:183]       )
INFO 10-04 15:28:37 model_runner.py:183]     )
INFO 10-04 15:28:37 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 10-04 15:28:37 model_runner.py:183]   )
INFO 10-04 15:28:37 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 10-04 15:28:37 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 10-04 15:28:37 model_runner.py:183]   (sampler): Sampler()
INFO 10-04 15:28:37 model_runner.py:183] )
INFO 10-04 15:28:37 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:28:37 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 8.4896, fwd=345.4351 cmp_logits=0.2325, sampling=109.7705, total=463.9297
INFO 10-04 15:28:37 worker.py:164] Peak: 13.307 GB, Initial: 38.980 GB, Free: 25.673 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.118 GB
INFO 10-04 15:28:37 gpu_executor.py:117] # GPU blocks: 3087, # CPU blocks: 8192
INFO 10-04 15:29:08 worker.py:189] _init_cache_engine took 24.1250 GB
INFO 10-04 15:29:08 scheduler.py:307] Scheduler initialized with prompt limit: 16384
INFO 10-04 15:29:08 Start warmup...
INFO 10-04 15:29:08 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:08 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:08 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9038, fwd=332.7279 cmp_logits=71.1656, sampling=1.0114, total=405.8111
INFO 10-04 15:29:08 metrics.py:335] Avg prompt throughput: 2479.5 tokens/s, Avg generation throughput: 2.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 413.0
INFO 10-04 15:29:08 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 406.3585 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:08 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:08 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:08 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3200, fwd=42.4159 cmp_logits=0.1173, sampling=6.6006, total=49.4561
INFO 10-04 15:29:08 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 50.4
INFO 10-04 15:29:08 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.9122 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:08 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:08 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:09 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3164, fwd=8.1372 cmp_logits=0.0787, sampling=7.0553, total=15.5892
INFO 10-04 15:29:09 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 61.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 10-04 15:29:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9109 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:09 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:09 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:09 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2816, fwd=8.0011 cmp_logits=0.0734, sampling=7.1545, total=15.5118
INFO 10-04 15:29:09 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 15:29:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7707 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:09 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:09 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:09 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2708, fwd=7.8905 cmp_logits=0.0713, sampling=7.2823, total=15.5168
INFO 10-04 15:29:09 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 15:29:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7535 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:09 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:09 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:09 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2742, fwd=8.0867 cmp_logits=0.0708, sampling=7.0817, total=15.5151
INFO 10-04 15:29:09 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 15:29:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7599 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:09 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:09 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:09 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2770, fwd=8.0218 cmp_logits=0.0744, sampling=7.1237, total=15.4989
INFO 10-04 15:29:09 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.0
INFO 10-04 15:29:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7297 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:09 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:09 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:09 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2649, fwd=8.0237 cmp_logits=0.0689, sampling=7.1154, total=15.4743
INFO 10-04 15:29:09 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 10-04 15:29:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7208 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:09 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:09 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:09 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2689, fwd=7.9739 cmp_logits=0.0679, sampling=6.2511, total=14.5628
INFO 10-04 15:29:09 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.9
INFO 10-04 15:29:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.7631 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:09 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:09 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:09 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2606, fwd=7.9820 cmp_logits=0.0672, sampling=5.9597, total=14.2703
INFO 10-04 15:29:09 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 10-04 15:29:09 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4882 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:14 Start benchmarking...
INFO 10-04 15:29:14 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:14 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:14 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8371, fwd=47.1928 cmp_logits=0.0939, sampling=70.6360, total=118.7613
INFO 10-04 15:29:14 metrics.py:335] Avg prompt throughput: 6174.7 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 4.4%, CPU KV cache usage: 0.0%, Interval(ms): 165.8
INFO 10-04 15:29:14 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 119.2372 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:14 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:14 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:14 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7398, fwd=14.4343 cmp_logits=0.0725, sampling=72.5226, total=87.7700
INFO 10-04 15:29:14 metrics.py:335] Avg prompt throughput: 11594.5 tokens/s, Avg generation throughput: 11.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 4.4%, CPU KV cache usage: 0.0%, Interval(ms): 88.2
INFO 10-04 15:29:14 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 88.0196 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:14 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:14 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:14 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7498, fwd=14.3890 cmp_logits=0.1190, sampling=75.9463, total=91.2049
INFO 10-04 15:29:14 metrics.py:335] Avg prompt throughput: 11162.2 tokens/s, Avg generation throughput: 21.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%, Interval(ms): 91.6
INFO 10-04 15:29:14 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 91.5074 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:29:14 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:14 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:14 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7808, fwd=14.4839 cmp_logits=0.0849, sampling=70.1079, total=85.4585
INFO 10-04 15:29:14 metrics.py:335] Avg prompt throughput: 11889.6 tokens/s, Avg generation throughput: 34.9 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 11.2%, CPU KV cache usage: 0.0%, Interval(ms): 86.0
INFO 10-04 15:29:14 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 85.8133 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 15:29:14 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:14 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:14 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7577, fwd=14.5717 cmp_logits=0.0691, sampling=72.1991, total=87.5983
INFO 10-04 15:29:14 metrics.py:335] Avg prompt throughput: 11591.4 tokens/s, Avg generation throughput: 34.1 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 11.2%, CPU KV cache usage: 0.0%, Interval(ms): 88.1
INFO 10-04 15:29:14 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 87.9278 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 15:29:14 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:14 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:14 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7856, fwd=14.4472 cmp_logits=0.0842, sampling=74.4042, total=89.7219
INFO 10-04 15:29:14 metrics.py:335] Avg prompt throughput: 11309.8 tokens/s, Avg generation throughput: 44.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 16.0%, CPU KV cache usage: 0.0%, Interval(ms): 90.3
INFO 10-04 15:29:14 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 90.1251 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:14 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:14 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:14 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.7927, fwd=14.4260 cmp_logits=0.0708, sampling=68.1055, total=83.3957
INFO 10-04 15:29:14 metrics.py:335] Avg prompt throughput: 12150.5 tokens/s, Avg generation throughput: 47.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 16.0%, CPU KV cache usage: 0.0%, Interval(ms): 83.9
INFO 10-04 15:29:14 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 83.7486 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:14 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:14 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:14 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8454, fwd=14.3640 cmp_logits=0.0861, sampling=73.2648, total=88.5615
INFO 10-04 15:29:14 metrics.py:335] Avg prompt throughput: 11443.3 tokens/s, Avg generation throughput: 56.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%, Interval(ms): 89.1
INFO 10-04 15:29:14 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 88.9802 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:14 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:14 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:14 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8206, fwd=14.4680 cmp_logits=0.0861, sampling=61.1644, total=76.5400
INFO 10-04 15:29:14 metrics.py:335] Avg prompt throughput: 13205.1 tokens/s, Avg generation throughput: 90.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%, Interval(ms): 77.2
INFO 10-04 15:29:14 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 77.0051 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:29:14 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:14 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8240, fwd=14.6518 cmp_logits=0.0710, sampling=59.4685, total=75.0160
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 13435.5 tokens/s, Avg generation throughput: 92.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 23.8%, CPU KV cache usage: 0.0%, Interval(ms): 75.7
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 75.5267 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8371, fwd=14.5273 cmp_logits=0.0710, sampling=75.0728, total=90.5089
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 11151.6 tokens/s, Avg generation throughput: 76.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 23.9%, CPU KV cache usage: 0.0%, Interval(ms): 91.2
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 91.0280 resumed_reqs=0, running_reqs=8 raw_running=8
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9162, fwd=14.4200 cmp_logits=0.0844, sampling=77.2679, total=92.6893
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 10888.4 tokens/s, Avg generation throughput: 85.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 31.1%, CPU KV cache usage: 0.0%, Interval(ms): 93.4
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 93.2326 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8681, fwd=14.4801 cmp_logits=0.0741, sampling=69.3765, total=84.8000
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 11888.2 tokens/s, Avg generation throughput: 93.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 31.1%, CPU KV cache usage: 0.0%, Interval(ms): 85.5
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 85.2869 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8619, fwd=14.4567 cmp_logits=0.0713, sampling=84.5637, total=99.9548
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 10097.4 tokens/s, Avg generation throughput: 79.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 31.2%, CPU KV cache usage: 0.0%, Interval(ms): 100.6
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 100.4484 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9069, fwd=14.4632 cmp_logits=0.0858, sampling=99.9289, total=115.3858
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 8748.4 tokens/s, Avg generation throughput: 77.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.2%, CPU KV cache usage: 0.0%, Interval(ms): 116.1
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 115.9632 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9255, fwd=14.4632 cmp_logits=0.0794, sampling=63.6275, total=79.0966
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 12712.3 tokens/s, Avg generation throughput: 112.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.2%, CPU KV cache usage: 0.0%, Interval(ms): 79.8
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 79.6642 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1024]), positions.shape=torch.Size([1024]) hidden_states.shape=torch.Size([1024, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.9136, fwd=14.3666 cmp_logits=0.0720, sampling=79.1187, total=94.4719
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 10665.1 tokens/s, Avg generation throughput: 94.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.2%, CPU KV cache usage: 0.0%, Interval(ms): 95.2
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 94.9910 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([910]), positions.shape=torch.Size([910]) hidden_states.shape=torch.Size([910, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.8910, fwd=14.5438 cmp_logits=0.0827, sampling=92.9189, total=108.4375
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 8253.3 tokens/s, Avg generation throughput: 91.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 109.2
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 108.9914 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4525, fwd=7.8452 cmp_logits=0.0691, sampling=12.6436, total=21.0114
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5607 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4568, fwd=7.8838 cmp_logits=0.0677, sampling=12.5706, total=20.9796
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5209 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4575, fwd=7.9262 cmp_logits=0.0689, sampling=12.5833, total=21.0369
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5716 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4570, fwd=7.8416 cmp_logits=0.0679, sampling=12.6078, total=20.9754
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 461.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5197 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4675, fwd=7.9062 cmp_logits=0.0823, sampling=12.5186, total=20.9758
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 461.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5108 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4530, fwd=7.9918 cmp_logits=0.0684, sampling=12.4638, total=20.9780
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5244 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4673, fwd=7.8261 cmp_logits=0.0682, sampling=12.6548, total=21.0176
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5580 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:29:15 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4599, fwd=7.9503 cmp_logits=0.0691, sampling=12.5744, total=21.0547
INFO 10-04 15:29:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 15:29:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5871 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:29:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:15 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4554, fwd=7.9718 cmp_logits=0.0687, sampling=12.5167, total=21.0135
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5409 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4568, fwd=7.8743 cmp_logits=0.0675, sampling=12.6350, total=21.0345
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5683 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([10]), positions.shape=torch.Size([10]) hidden_states.shape=torch.Size([10, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4854, fwd=7.8235 cmp_logits=0.0682, sampling=12.6865, total=21.0645
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.2%, CPU KV cache usage: 0.0%, Interval(ms): 21.8
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.6131 resumed_reqs=0, running_reqs=10 raw_running=10
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4449, fwd=7.8685 cmp_logits=0.0818, sampling=12.3672, total=20.7639
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2693 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4327, fwd=8.0054 cmp_logits=0.0679, sampling=12.2142, total=20.7210
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 420.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2431 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4373, fwd=7.8390 cmp_logits=0.0682, sampling=12.3708, total=20.7164
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 420.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2193 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4423, fwd=7.8607 cmp_logits=0.0682, sampling=12.3458, total=20.7179
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 420.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2317 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4430, fwd=7.9334 cmp_logits=0.0679, sampling=12.2879, total=20.7331
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 420.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.3%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2407 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4408, fwd=7.8807 cmp_logits=0.0684, sampling=12.3639, total=20.7546
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 420.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2555 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4737, fwd=7.8368 cmp_logits=0.0682, sampling=12.4788, total=20.8588
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 417.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.6
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.3814 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4396, fwd=7.8542 cmp_logits=0.0684, sampling=12.3920, total=20.7553
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2584 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4399, fwd=7.8852 cmp_logits=0.0684, sampling=12.3606, total=20.7553
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 420.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2605 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4420, fwd=7.9279 cmp_logits=0.0682, sampling=12.3022, total=20.7412
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 420.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2476 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4396, fwd=7.8700 cmp_logits=0.0677, sampling=12.3899, total=20.7684
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2774 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([9]), positions.shape=torch.Size([9]) hidden_states.shape=torch.Size([9, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4380, fwd=7.9308 cmp_logits=0.0679, sampling=12.3758, total=20.8135
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 418.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.6%, CPU KV cache usage: 0.0%, Interval(ms): 21.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.3511 resumed_reqs=0, running_reqs=9 raw_running=9
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3934, fwd=7.9057 cmp_logits=0.0675, sampling=11.7855, total=20.1533
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5929 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3960, fwd=7.9129 cmp_logits=0.0679, sampling=11.7815, total=20.1592
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5960 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3986, fwd=7.9670 cmp_logits=0.0679, sampling=11.6990, total=20.1335
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 337.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.7
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5708 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4113, fwd=7.9293 cmp_logits=0.0815, sampling=11.7753, total=20.1986
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6666 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3967, fwd=7.8261 cmp_logits=0.0679, sampling=11.8897, total=20.1814
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6206 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3967, fwd=7.7779 cmp_logits=0.0677, sampling=11.8999, total=20.1433
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5970 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4268, fwd=7.8418 cmp_logits=0.0668, sampling=11.8570, total=20.1933
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6428 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3965, fwd=7.8385 cmp_logits=0.0687, sampling=11.8570, total=20.1614
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6087 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3963, fwd=7.9083 cmp_logits=0.0679, sampling=11.8003, total=20.1738
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6170 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4029, fwd=8.1370 cmp_logits=0.0677, sampling=11.5981, total=20.2067
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 335.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6749 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([7]), positions.shape=torch.Size([7]) hidden_states.shape=torch.Size([7, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3979, fwd=7.9441 cmp_logits=0.0670, sampling=11.7686, total=20.1783
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.5 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6435 resumed_reqs=0, running_reqs=7 raw_running=7
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4036, fwd=7.9391 cmp_logits=0.0794, sampling=11.4703, total=19.8934
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3025 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3777, fwd=7.9100 cmp_logits=0.0677, sampling=11.5123, total=19.8684
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2949 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3822, fwd=7.9360 cmp_logits=0.0677, sampling=11.4996, total=19.8865
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3078 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3827, fwd=7.8120 cmp_logits=0.0670, sampling=11.6031, total=19.8658
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3154 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3831, fwd=7.8485 cmp_logits=0.0663, sampling=11.5910, total=19.8905
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3054 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3762, fwd=7.8020 cmp_logits=0.0672, sampling=11.6279, total=19.8741
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3075 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3812, fwd=7.9329 cmp_logits=0.0675, sampling=11.5108, total=19.8934
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3037 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4029, fwd=7.9010 cmp_logits=0.0799, sampling=11.5035, total=19.8884
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2961 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3803, fwd=7.9052 cmp_logits=0.0675, sampling=11.5311, total=19.8851
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3483 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3786, fwd=7.8771 cmp_logits=0.0689, sampling=11.5302, total=19.8557
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.4
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2656 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3762, fwd=7.8614 cmp_logits=0.0670, sampling=11.5764, total=19.8820
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2940 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3786, fwd=7.8773 cmp_logits=0.0670, sampling=11.5430, total=19.8667
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.4
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2761 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3748, fwd=7.9148 cmp_logits=0.0668, sampling=11.5228, total=19.8801
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.2920 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3884, fwd=7.8921 cmp_logits=0.0677, sampling=11.5194, total=19.8686
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3149 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3984, fwd=7.8118 cmp_logits=0.0670, sampling=11.6460, total=19.9244
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3381 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3781, fwd=7.8487 cmp_logits=0.0668, sampling=11.6317, total=19.9268
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3395 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3843, fwd=7.8301 cmp_logits=0.0663, sampling=11.6324, total=19.9142
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3328 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3936, fwd=7.8502 cmp_logits=0.0660, sampling=11.6041, total=19.9146
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3755 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3839, fwd=7.8917 cmp_logits=0.0668, sampling=11.5790, total=19.9223
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3707 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3881, fwd=7.8290 cmp_logits=0.0670, sampling=11.6341, total=19.9192
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3772 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3784, fwd=7.7784 cmp_logits=0.0668, sampling=11.7085, total=19.9330
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3407 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:16 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4046, fwd=7.9184 cmp_logits=0.0675, sampling=11.5387, total=19.9301
INFO 10-04 15:29:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3872 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:16 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3788, fwd=7.7972 cmp_logits=0.0670, sampling=11.7154, total=19.9594
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 290.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.6
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.4768 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.4451, fwd=8.0006 cmp_logits=0.0687, sampling=11.4634, total=19.9785
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 288.4 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5605 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3843, fwd=7.9768 cmp_logits=0.0665, sampling=11.5070, total=19.9356
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3507 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3848, fwd=8.0166 cmp_logits=0.0668, sampling=11.4660, total=19.9351
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3464 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3850, fwd=7.9069 cmp_logits=0.0782, sampling=11.5399, total=19.9113
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3216 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3822, fwd=7.9091 cmp_logits=0.0679, sampling=11.5275, total=19.8879
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3011 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3912, fwd=7.8964 cmp_logits=0.0677, sampling=11.5697, total=19.9263
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3731 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([6]), positions.shape=torch.Size([6]) hidden_states.shape=torch.Size([6, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3800, fwd=7.8328 cmp_logits=0.0670, sampling=11.6096, total=19.8905
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3273 resumed_reqs=0, running_reqs=6 raw_running=6
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3705, fwd=7.9548 cmp_logits=0.0672, sampling=11.2691, total=19.6626
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0465 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3574, fwd=7.8256 cmp_logits=0.0794, sampling=11.3285, total=19.5918
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9740 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3924, fwd=7.9045 cmp_logits=0.0672, sampling=11.2770, total=19.6419
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0219 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3691, fwd=7.9427 cmp_logits=0.0675, sampling=11.2209, total=19.6011
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0124 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3579, fwd=7.8375 cmp_logits=0.0670, sampling=11.3761, total=19.6395
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0195 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3614, fwd=7.7758 cmp_logits=0.0663, sampling=11.3809, total=19.5854
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.7%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9637 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3591, fwd=7.8278 cmp_logits=0.0677, sampling=11.3895, total=19.6450
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0243 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3603, fwd=7.9341 cmp_logits=0.0665, sampling=11.3070, total=19.6688
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0503 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3614, fwd=7.8847 cmp_logits=0.0670, sampling=11.3435, total=19.6574
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0441 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3610, fwd=7.8809 cmp_logits=0.0663, sampling=11.3401, total=19.6493
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0257 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3552, fwd=7.8113 cmp_logits=0.0675, sampling=11.3890, total=19.6240
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0026 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3586, fwd=7.9284 cmp_logits=0.0677, sampling=11.2910, total=19.6466
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0388 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3581, fwd=7.8540 cmp_logits=0.0672, sampling=11.3764, total=19.6569
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0293 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3572, fwd=7.8630 cmp_logits=0.0682, sampling=11.3432, total=19.6326
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0157 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3572, fwd=7.8819 cmp_logits=0.0672, sampling=11.3478, total=19.6548
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0319 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3552, fwd=7.9064 cmp_logits=0.0691, sampling=11.2214, total=19.5529
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 249.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9254 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3629, fwd=7.9308 cmp_logits=0.0739, sampling=11.2309, total=19.5994
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9797 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3955, fwd=7.8907 cmp_logits=0.0682, sampling=11.2433, total=19.5987
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9795 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3648, fwd=7.8845 cmp_logits=0.0679, sampling=11.2469, total=19.5651
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9530 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3629, fwd=7.8979 cmp_logits=0.0708, sampling=11.2524, total=19.5849
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9647 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3572, fwd=7.8611 cmp_logits=0.0672, sampling=11.2836, total=19.5696
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9649 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3588, fwd=7.8630 cmp_logits=0.0675, sampling=11.2801, total=19.5706
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9809 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3593, fwd=7.8878 cmp_logits=0.0670, sampling=11.2417, total=19.5570
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9344 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3607, fwd=7.9455 cmp_logits=0.0687, sampling=11.2422, total=19.6178
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0131 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3581, fwd=7.9453 cmp_logits=0.0689, sampling=11.1988, total=19.5720
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9537 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3598, fwd=7.8461 cmp_logits=0.0668, sampling=11.3111, total=19.5847
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9640 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3872, fwd=7.8075 cmp_logits=0.0675, sampling=11.3475, total=19.6106
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9912 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3624, fwd=7.8599 cmp_logits=0.0675, sampling=11.2629, total=19.5539
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9339 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3633, fwd=7.8723 cmp_logits=0.0677, sampling=11.2877, total=19.5916
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 247.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9854 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3624, fwd=7.9310 cmp_logits=0.0670, sampling=11.1902, total=19.5515
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9323 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3574, fwd=7.8528 cmp_logits=0.0668, sampling=11.2805, total=19.5584
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9385 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3624, fwd=7.8993 cmp_logits=0.0672, sampling=11.2412, total=19.5711
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9552 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3591, fwd=7.8392 cmp_logits=0.0670, sampling=11.3077, total=19.5742
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9573 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3569, fwd=7.8368 cmp_logits=0.0684, sampling=11.3122, total=19.5756
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9614 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3638, fwd=7.8406 cmp_logits=0.0687, sampling=11.3063, total=19.5801
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9594 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3607, fwd=7.8764 cmp_logits=0.0670, sampling=11.2576, total=19.5625
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9449 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3598, fwd=7.9122 cmp_logits=0.0670, sampling=11.2045, total=19.5446
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 249.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9161 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3581, fwd=7.8595 cmp_logits=0.0672, sampling=11.3020, total=19.5875
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.5 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9668 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3595, fwd=7.8185 cmp_logits=0.0677, sampling=11.3297, total=19.5765
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9614 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3581, fwd=7.9494 cmp_logits=0.0668, sampling=11.1852, total=19.5603
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9392 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3560, fwd=7.8299 cmp_logits=0.0665, sampling=11.3084, total=19.5618
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9404 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:17 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3583, fwd=7.8571 cmp_logits=0.0677, sampling=11.2550, total=19.5389
INFO 10-04 15:29:17 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 249.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 29.2%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 10-04 15:29:17 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9158 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:17 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:17 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([5]), positions.shape=torch.Size([5]) hidden_states.shape=torch.Size([5, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3569, fwd=7.8354 cmp_logits=0.0682, sampling=11.3208, total=19.5820
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 248.1 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.0%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9955 resumed_reqs=0, running_reqs=5 raw_running=5
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([4]), positions.shape=torch.Size([4]) hidden_states.shape=torch.Size([4, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3402, fwd=7.9954 cmp_logits=0.0670, sampling=10.1478, total=18.5513
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%, Interval(ms): 19.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.9350 resumed_reqs=0, running_reqs=4 raw_running=4
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3185, fwd=7.8692 cmp_logits=0.0660, sampling=9.7392, total=17.9937
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.2965 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3192, fwd=7.7920 cmp_logits=0.0668, sampling=9.7871, total=17.9660
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 163.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.2633 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3276, fwd=7.8490 cmp_logits=0.0668, sampling=9.7451, total=17.9894
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3139 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3181, fwd=7.8189 cmp_logits=0.0670, sampling=9.8469, total=18.0519
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3518 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3247, fwd=7.8027 cmp_logits=0.0668, sampling=9.8875, total=18.0826
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4002 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3159, fwd=7.7813 cmp_logits=0.0665, sampling=9.9027, total=18.0674
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3656 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3202, fwd=7.7419 cmp_logits=0.0653, sampling=9.9387, total=18.0669
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3852 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3152, fwd=7.7350 cmp_logits=0.0651, sampling=9.9463, total=18.0626
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3585 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3159, fwd=7.8008 cmp_logits=0.0784, sampling=9.8474, total=18.0435
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3413 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([3]), positions.shape=torch.Size([3]) hidden_states.shape=torch.Size([3, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3173, fwd=7.7803 cmp_logits=0.0670, sampling=9.9020, total=18.0674
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.7%, CPU KV cache usage: 0.0%, Interval(ms): 18.6
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.4071 resumed_reqs=0, running_reqs=3 raw_running=3
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2952, fwd=7.9746 cmp_logits=0.0675, sampling=9.4883, total=17.8263
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.7%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0867 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2928, fwd=7.9377 cmp_logits=0.0670, sampling=9.5165, total=17.8149
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0719 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2923, fwd=7.8325 cmp_logits=0.0670, sampling=9.6354, total=17.8282
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0833 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2921, fwd=7.9260 cmp_logits=0.0665, sampling=9.5682, total=17.8537
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1086 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2966, fwd=7.9293 cmp_logits=0.0672, sampling=9.5310, total=17.8251
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0848 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2925, fwd=7.9589 cmp_logits=0.0656, sampling=9.5131, total=17.8308
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0850 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2944, fwd=7.8917 cmp_logits=0.0670, sampling=9.5577, total=17.8118
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0755 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2961, fwd=7.9889 cmp_logits=0.0670, sampling=9.4602, total=17.8132
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0674 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2966, fwd=7.8263 cmp_logits=0.0665, sampling=9.6285, total=17.8187
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0724 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3159, fwd=7.8244 cmp_logits=0.0727, sampling=9.6309, total=17.8452
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1038 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2937, fwd=7.8323 cmp_logits=0.0663, sampling=9.6166, total=17.8099
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0621 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3014, fwd=7.8030 cmp_logits=0.0658, sampling=9.6676, total=17.8387
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1160 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2942, fwd=7.9029 cmp_logits=0.0670, sampling=9.5484, total=17.8134
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0666 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2940, fwd=7.8237 cmp_logits=0.0656, sampling=9.6250, total=17.8092
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0643 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3002, fwd=8.0099 cmp_logits=0.0660, sampling=9.4635, total=17.8404
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0988 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2954, fwd=7.8993 cmp_logits=0.0679, sampling=9.5556, total=17.8192
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0750 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2935, fwd=7.8752 cmp_logits=0.0663, sampling=9.5868, total=17.8227
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0788 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2933, fwd=7.8924 cmp_logits=0.0658, sampling=9.5742, total=17.8268
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0800 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2952, fwd=7.8225 cmp_logits=0.0665, sampling=9.6688, total=17.8537
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1067 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3159, fwd=7.7651 cmp_logits=0.0672, sampling=9.6986, total=17.8478
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1069 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3045, fwd=7.8766 cmp_logits=0.0675, sampling=9.5732, total=17.8230
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0764 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2952, fwd=7.8888 cmp_logits=0.0665, sampling=9.5823, total=17.8337
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0981 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2935, fwd=7.7925 cmp_logits=0.0658, sampling=9.6841, total=17.8366
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0926 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2944, fwd=7.7920 cmp_logits=0.0658, sampling=9.6865, total=17.8397
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0945 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2947, fwd=7.7071 cmp_logits=0.0656, sampling=9.7494, total=17.8177
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.8%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0790 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2937, fwd=7.8163 cmp_logits=0.0658, sampling=9.7077, total=17.8845
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1410 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2956, fwd=7.8857 cmp_logits=0.0672, sampling=9.6412, total=17.8907
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1437 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2925, fwd=7.8483 cmp_logits=0.0658, sampling=9.7077, total=17.9152
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1682 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.3104, fwd=7.8609 cmp_logits=0.0660, sampling=9.6812, total=17.9198
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.9%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1782 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([2]), positions.shape=torch.Size([2]) hidden_states.shape=torch.Size([2, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2968, fwd=7.8142 cmp_logits=0.0668, sampling=9.6967, total=17.8754
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.2025 resumed_reqs=0, running_reqs=2 raw_running=2
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2735, fwd=8.0879 cmp_logits=0.0672, sampling=9.5339, total=17.9636
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1682 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2654, fwd=8.0903 cmp_logits=0.0772, sampling=9.5155, total=17.9493
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1508 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2685, fwd=8.0113 cmp_logits=0.0670, sampling=9.6302, total=17.9780
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1777 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2658, fwd=8.0712 cmp_logits=0.0677, sampling=9.5539, total=17.9596
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1630 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2751, fwd=8.0464 cmp_logits=0.0677, sampling=9.5832, total=17.9732
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1789 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2890, fwd=8.0171 cmp_logits=0.0677, sampling=9.6095, total=17.9844
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1890 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2754, fwd=7.9944 cmp_logits=0.0682, sampling=9.6540, total=17.9930
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.2128 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2666, fwd=7.9865 cmp_logits=0.0677, sampling=9.6302, total=17.9522
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1553 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2685, fwd=8.0283 cmp_logits=0.0679, sampling=9.5916, total=17.9572
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1618 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2699, fwd=8.1081 cmp_logits=0.0679, sampling=9.5119, total=17.9591
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1830 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2673, fwd=7.9637 cmp_logits=0.0679, sampling=9.6817, total=17.9813
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1832 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:18 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2680, fwd=8.0855 cmp_logits=0.0741, sampling=9.5501, total=17.9782
INFO 10-04 15:29:18 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:18 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1804 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:18 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:18 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2680, fwd=8.0435 cmp_logits=0.0672, sampling=9.5723, total=17.9520
INFO 10-04 15:29:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1541 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2708, fwd=8.0156 cmp_logits=0.0677, sampling=9.5944, total=17.9496
INFO 10-04 15:29:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1520 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2697, fwd=8.0109 cmp_logits=0.0675, sampling=9.6283, total=17.9772
INFO 10-04 15:29:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 15:29:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1868 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2677, fwd=8.1406 cmp_logits=0.0741, sampling=9.5029, total=17.9865
INFO 10-04 15:29:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1894 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2775, fwd=8.0521 cmp_logits=0.0679, sampling=9.5806, total=17.9791
INFO 10-04 15:29:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1985 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2687, fwd=8.1737 cmp_logits=0.0677, sampling=9.4562, total=17.9672
INFO 10-04 15:29:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1701 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2677, fwd=8.0647 cmp_logits=0.0679, sampling=9.5725, total=17.9739
INFO 10-04 15:29:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1751 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2689, fwd=8.1296 cmp_logits=0.0675, sampling=9.4912, total=17.9582
INFO 10-04 15:29:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1637 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2661, fwd=8.0450 cmp_logits=0.0672, sampling=9.6028, total=17.9818
INFO 10-04 15:29:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1928 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2666, fwd=7.9463 cmp_logits=0.0668, sampling=9.6676, total=17.9484
INFO 10-04 15:29:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1513 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2689, fwd=8.1732 cmp_logits=0.0701, sampling=9.5251, total=18.0383
INFO 10-04 15:29:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 15:29:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.2419 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2680, fwd=8.1191 cmp_logits=0.0672, sampling=9.5179, total=17.9732
INFO 10-04 15:29:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.2025 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2697, fwd=8.0514 cmp_logits=0.0672, sampling=9.5959, total=17.9851
INFO 10-04 15:29:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 15:29:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1923 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2673, fwd=8.0009 cmp_logits=0.0725, sampling=9.6219, total=17.9634
INFO 10-04 15:29:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 10-04 15:29:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.1682 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 10-04 15:29:19 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=1024, max_num_seqs=256
INFO 10-04 15:29:19 llama.py:322] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 10-04 15:29:19 model_runner.py:861] ModelRunner execute_model (ms): prepare_input = 0.2682, fwd=8.0156 cmp_logits=0.0668, sampling=9.6304, total=17.9818
INFO 10-04 15:29:19 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.4
INFO 10-04 15:29:19 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.2505 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=1, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055754.1129506, last_token_time=1728055756.0488453, first_scheduled_time=1728055754.1497176, first_token_time=1728055754.2686074, time_in_queue=0.036767005920410156, finished_time=1728055756.0488048, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055754.1150846, last_token_time=1728055757.1499496, first_scheduled_time=1728055754.1497176, first_token_time=1728055754.4484763, time_in_queue=0.03463292121887207, finished_time=1728055757.1499236, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=3, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055754.1183643, last_token_time=1728055756.5349126, first_scheduled_time=1728055754.3571699, first_token_time=1728055754.534411, time_in_queue=0.23880553245544434, finished_time=1728055756.5348866, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=4, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055754.1201994, last_token_time=1728055758.0160975, first_scheduled_time=1728055754.4488425, first_token_time=1728055754.7127297, time_in_queue=0.3286430835723877, finished_time=1728055758.0160763, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=5, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055754.1248682, last_token_time=1728055758.035286, first_scheduled_time=1728055754.6228921, first_token_time=1728055754.8857925, time_in_queue=0.49802398681640625, finished_time=1728055758.0352683, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=6, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055754.129254, last_token_time=1728055756.3061948, first_scheduled_time=1728055754.7971082, first_token_time=1728055754.9629242, time_in_queue=0.6678540706634521, finished_time=1728055756.3061764, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=7, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055754.1304827, last_token_time=1728055756.3061948, first_scheduled_time=1728055754.8862584, first_token_time=1728055754.9629242, time_in_queue=0.7557756900787354, finished_time=1728055756.3061812, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=8, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055754.1317112, last_token_time=1728055758.2203512, first_scheduled_time=1728055754.963452, first_token_time=1728055755.223193, time_in_queue=0.8317408561706543, finished_time=1728055758.2203383, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=9, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055754.1365051, last_token_time=1728055759.2626874, first_scheduled_time=1728055755.1303637, first_token_time=1728055755.5253768, time_in_queue=0.9938585758209229, finished_time=1728055759.2626846, rescheduled_time=None, reget_first_token_time=None))
RequestOutput(request_id=10, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1728055754.1437898, last_token_time=1728055758.7678678, first_scheduled_time=1728055755.4098456, first_token_time=1728055755.8095455, time_in_queue=1.2660558223724365, finished_time=1728055758.7678661, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 19139
End-to-End latency: 5.15 seconds
Throughput: 1.94 requests/s, 3716.33 tokens/s
Per_token_time: 0.269 ms

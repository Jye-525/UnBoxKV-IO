Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=1, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='fixed', input_len=8191, output_len=1, min_len=16, max_len=4096, prefill_to_decode_ratio=10000.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=8192, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=32, max_num_seqs=32, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 1 requests, requiring 1 requests.
Original order of the requests
User Requests: prompt_len=8191, output_len=1, sequence_len=8192...
Selected required requests 1
User Requests: prompt_len=8191, output_len=1, sequence_len=8192...
Chunked prefill is enabled. max_num_batched_tokens=32, max_num_batched_seqs=32
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'type', 'original_max_position_embeddings', 'finetuned'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 09-24 01:53:33 config.py:654] [SchedulerConfig] max_num_batched_tokens: 32 chunked_prefill_enabled: True
INFO 09-24 01:53:33 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
/home/jieye/viper2/vllm_v0_4_2/vllm/executor/gpu_executor.py:36: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 09-24 01:53:33 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 09-24 01:53:38 selector.py:27] Using FlashAttention-2 backend.
INFO 09-24 01:53:42 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 09-24 01:53:42 model_runner.py:183] Loaded model: 
INFO 09-24 01:53:42 model_runner.py:183]  LlamaForCausalLM(
INFO 09-24 01:53:42 model_runner.py:183]   (model): LlamaModel(
INFO 09-24 01:53:42 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 09-24 01:53:42 model_runner.py:183]     (layers): ModuleList(
INFO 09-24 01:53:42 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 09-24 01:53:42 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 09-24 01:53:42 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 09-24 01:53:42 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 09-24 01:53:42 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 09-24 01:53:42 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 09-24 01:53:42 model_runner.py:183]         )
INFO 09-24 01:53:42 model_runner.py:183]         (mlp): LlamaMLP(
INFO 09-24 01:53:42 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 09-24 01:53:42 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 09-24 01:53:42 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 09-24 01:53:42 model_runner.py:183]         )
INFO 09-24 01:53:42 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 09-24 01:53:42 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 09-24 01:53:42 model_runner.py:183]       )
INFO 09-24 01:53:42 model_runner.py:183]     )
INFO 09-24 01:53:42 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 09-24 01:53:42 model_runner.py:183]   )
INFO 09-24 01:53:42 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 09-24 01:53:42 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 09-24 01:53:42 model_runner.py:183]   (sampler): Sampler()
INFO 09-24 01:53:42 model_runner.py:183] )
INFO 09-24 01:53:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:53:42 worker.py:164] Peak: 13.031 GB, Initial: 38.980 GB, Free: 25.948 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.393 GB
INFO 09-24 01:53:42 gpu_executor.py:117] # GPU blocks: 3122, # CPU blocks: 8192
INFO 09-24 01:54:13 worker.py:189] _init_cache_engine took 24.3906 GB
INFO 09-24 01:54:13 scheduler.py:307] Scheduler initialized with prompt limit: 8192
INFO 09-24 01:54:13 Start warmup...
INFO 09-24 01:54:13 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:13 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 16.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 1888.1
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1881.9997 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 2123.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.1
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.6940 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 2189.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4510 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 2190.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4124 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 2143.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.9
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.7660 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 2146.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.9
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.7533 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 2057.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4002 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 2061.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3644 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 2039.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5313 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 2009.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.7456 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1988.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9461 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1961.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1681 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1937.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.5
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.3713 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1935.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.5
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.3875 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 2005.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.0
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8100 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 2047.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4829 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 2041.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5289 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 2028.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6307 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1979.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.0205 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1964.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1388 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1953.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.4
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.2332 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1938.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.5
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.3660 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1911.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.7
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.5911 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1886.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8231 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1892.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7630 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1881.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8684 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1830.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3352 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1822.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.6
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.4117 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1815.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.6
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.4437 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1805.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.7
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.5750 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1770.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9217 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 1379.2 tokens/s, Avg generation throughput: 43.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0548 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 22.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 45.3
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.1305 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.9
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5228 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4086 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3948 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3919 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4176 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4041 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3726 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:15 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:54:15 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4060 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:15 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:15 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2094.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.3
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3166 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2249.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.2
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.0886 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2257.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.2
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.0369 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2240.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.3
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.1401 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2241.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.3
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.1349 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2241.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.3
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.1339 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2236.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.3
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.1647 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2214.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3077 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2216.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.4
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.2925 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2198.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4103 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2134.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.0
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.8466 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2128.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.0
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.8885 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2119.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.1
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.9567 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2115.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.1
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.9798 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2048.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4796 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2049.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4746 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2035.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.7
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.5790 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 2026.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6522 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 1976.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.0518 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 1959.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1881 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 1958.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1982 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 1956.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.4
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.2170 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 1902.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6779 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 1890.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7789 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 1887.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7711 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 1878.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8879 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 1830.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3399 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 1822.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.6
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.4117 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 1816.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.6
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.4737 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 1808.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 17.7
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.5469 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 1768.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 18.1
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.9141 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 1386.5 tokens/s, Avg generation throughput: 43.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%, Interval(ms): 23.1
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.9301 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3893 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4215 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3867 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3838 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3921 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4036 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3802 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3836 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:16 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:16 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:54:16 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:54:16 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4045 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 Start benchmarking...
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 732.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 43.7
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.1093 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 2239.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 14.3
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.0915 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 2146.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 14.9
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.7676 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 2165.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 14.8
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.6339 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 2147.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 14.9
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.7572 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 2123.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 15.1
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.9279 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 1868.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9830 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 2052.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 15.6
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.4347 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 2058.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3913 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 2062.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3613 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 1994.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.0
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8925 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 1998.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.0
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8615 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 1988.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9385 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 1959.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.1779 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 1909.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6073 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 1904.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.8
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.6457 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 1895.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7332 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 1882.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8500 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 1837.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.4
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.2651 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 1881.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8521 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 1964.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.3
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.0921 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:21 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:21 metrics.py:335] Avg prompt throughput: 1938.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.5
INFO 09-24 01:54:21 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.3572 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:21 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1896.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.9
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.7189 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1882.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8443 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1881.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8557 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1869.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.1
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.9604 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1820.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.6
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3883 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1828.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3523 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1799.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6344 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1811.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.7
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.5161 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1759.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0392 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1756.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0616 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1752.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.3
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0719 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1732.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3244 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1709.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.7
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.5742 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1703.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.8
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.6379 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1692.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.9
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.7545 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1681.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.0
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.8835 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1651.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.4
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.2230 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1636.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3970 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1645.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.4
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.2909 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1629.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.6
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.4895 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1597.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.0
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8729 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1591.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9580 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1583.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.2
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.0579 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1577.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.3
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.1283 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1544.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.7
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5698 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1641.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.5
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.3384 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1542.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.7
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5970 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1536.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6897 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1500.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1897 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1500.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1806 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1495.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2519 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1493.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.4
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.2789 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1464.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.9
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.7083 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1454.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.0
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.8573 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1453.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.0
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.8649 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1446.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.1
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.9383 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1420.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.5
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.3820 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1415.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4562 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1413.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.5024 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1402.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.8
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.6657 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1381.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0265 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1376.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.2
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.0651 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1375.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1202 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1364.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.5
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.3164 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1345.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.8
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.6402 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1341.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.8
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.7048 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1334.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 24.0
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.8376 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:22 metrics.py:335] Avg prompt throughput: 1333.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 24.0
INFO 09-24 01:54:22 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.8163 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:22 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:22 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1308.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 24.5
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 24.3127 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1303.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 24.6
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 24.4124 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1303.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 24.5
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 24.3979 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1295.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 24.7
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 24.5540 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1268.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 25.2
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 25.0852 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1273.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 25.1
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 24.9538 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1270.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 25.2
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 25.0382 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1260.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 25.4
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 25.2483 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1243.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 25.7
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 25.5826 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1239.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 25.8
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 25.6803 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1238.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 25.8
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 25.7022 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1230.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 26.0
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 25.8524 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1213.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 26.4
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 26.2208 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1207.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 26.5
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 26.3591 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1207.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 26.5
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 26.3522 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1196.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 26.7
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 26.5884 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1187.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 26.9
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 26.8013 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1176.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 27.2
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.0576 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1179.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 27.1
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 26.9892 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1175.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 27.2
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.0782 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1157.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 27.7
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.5099 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1150.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6778 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1150.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6818 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1147.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 27.9
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.7376 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1128.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.1723 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1127.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.2452 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1122.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.5
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.3725 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1122.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.5
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.3780 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1105.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.9
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.7921 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1098.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 29.1
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.9903 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1099.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 29.1
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.9159 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1093.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 29.3
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 29.1147 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1081.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 29.6
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 29.4449 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1076.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 29.7
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 29.5846 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1071.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 29.9
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 29.7167 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1076.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 29.7
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 29.5813 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:23 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:23 metrics.py:335] Avg prompt throughput: 1054.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 30.3
INFO 09-24 01:54:23 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 30.1578 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:23 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 1052.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 30.4
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 30.2639 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 1053.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 30.4
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 30.2310 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 1049.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 30.5
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 30.3409 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 1033.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 31.0
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 30.8239 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 1033.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 31.0
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 30.8206 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 1031.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 31.0
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 30.8483 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 1025.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 31.2
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 31.0590 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 1012.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 31.6
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 31.4627 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 1012.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 31.6
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 31.4555 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 1008.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 31.7
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 31.5762 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 1008.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 31.7
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 31.5895 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 990.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 32.3
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 32.1610 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 991.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 32.3
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 32.1338 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 986.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 32.4
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 32.2948 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 988.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 32.4
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 32.2242 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 974.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 32.8
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 32.7013 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 971.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 32.9
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 32.7959 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 968.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 33.0
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 32.8979 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 967.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 33.1
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 32.9275 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 953.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 33.6
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 33.4308 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 952.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 33.6
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 33.4616 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 952.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 33.6
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 33.4463 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 948.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 33.7
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 33.6058 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 935.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 34.2
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.0552 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 934.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 34.2
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.0948 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 933.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 34.3
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.1532 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 929.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 34.4
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.2703 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 918.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 34.8
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.6990 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 917.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 34.9
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.7259 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:24 metrics.py:335] Avg prompt throughput: 915.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.0
INFO 09-24 01:54:24 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8279 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:24 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:24 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 912.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.1
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.8718 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 901.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.3682 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 899.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.6
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.4242 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 901.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.5
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.3560 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 896.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.7
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.5620 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 886.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 36.1
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.9592 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 884.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 36.2
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 36.0098 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 883.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 36.2
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 36.0751 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 879.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 36.4
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 36.2446 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 869.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 36.8
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 36.6402 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 869.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 36.8
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 36.6704 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 867.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 36.9
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 36.7634 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 865.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 37.0
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 36.7870 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 852.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 37.5
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 37.3945 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 854.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 37.4
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 37.2860 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 852.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 37.6
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 37.4136 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 849.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 37.7
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 37.5278 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 841.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 38.0
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 37.9028 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 839.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 38.1
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 37.9951 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 837.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 38.2
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 38.0685 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 835.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 38.3
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 38.1675 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 826.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 38.7
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 38.5859 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 825.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 38.8
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 38.6307 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 823.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 38.9
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 38.7390 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 820.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 39.0
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 38.8637 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 812.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 39.4
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 39.2401 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:25 metrics.py:335] Avg prompt throughput: 811.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 39.5
INFO 09-24 01:54:25 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 39.3159 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:25 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:25 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 811.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 39.4
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 39.3035 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 808.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 39.6
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 39.4464 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 799.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 40.1
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 39.9072 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 799.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 40.0
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 39.8796 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 797.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 40.1
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 39.9697 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 795.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 40.2
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 40.0970 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 786.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 40.7
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 40.5383 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 785.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 40.7
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 40.5982 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 785.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 40.7
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 40.5858 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 780.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 41.0
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 40.8540 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 776.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 41.2
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 41.0435 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 775.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 41.3
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 41.1396 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 772.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 41.4
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 41.3027 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 771.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 41.5
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 41.3606 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 764.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 41.9
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 41.7325 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 760.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 42.1
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 41.9259 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 760.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 42.1
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 41.8704 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 759.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 42.1
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 41.9738 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 751.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 42.6
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 42.4151 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 749.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 42.7
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 42.5482 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 753.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 42.5
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 42.3350 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 747.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 42.8
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 42.6905 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 740.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 43.2
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.0434 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:26 metrics.py:335] Avg prompt throughput: 739.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 43.3
INFO 09-24 01:54:26 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.1299 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:26 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:26 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 737.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 43.4
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.2396 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 735.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 43.5
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.3519 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 729.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 43.9
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.7348 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 727.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 44.0
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.8271 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 727.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 44.0
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.8612 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 724.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 44.1
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.0006 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 720.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 44.4
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.2994 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 717.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 44.6
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.4717 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 717.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 44.6
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.4877 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 715.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 44.7
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.5964 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 707.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 45.2
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.1014 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 708.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 45.2
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.0394 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 705.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 45.4
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.2302 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 705.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 45.4
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.2256 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 698.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 45.8
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.6870 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 696.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 45.9
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.7780 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 696.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 45.9
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.7911 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 694.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 46.1
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.8934 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 688.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 46.5
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.3641 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 687.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 46.5
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.3812 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 686.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 46.6
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.4861 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:27 metrics.py:335] Avg prompt throughput: 686.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 46.6
INFO 09-24 01:54:27 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.4516 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:27 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:27 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 680.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 47.1
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.9127 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 679.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 47.1
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.9096 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 676.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 47.3
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.1900 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 676.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 47.3
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.1892 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 671.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 47.7
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.5385 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 669.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 47.8
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.6677 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 668.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 47.9
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.7114 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 665.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 48.1
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.9147 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 662.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 48.3
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.1756 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 659.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 48.5
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.3911 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 659.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 48.5
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.3797 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 658.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 48.6
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.4529 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 652.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 49.1
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.9154 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 651.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 49.1
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.9073 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 651.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 49.2
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.0088 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 649.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 49.3
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.1447 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 643.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 49.7
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.5679 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 643.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 49.7
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.5763 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 641.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 49.9
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.7224 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 640.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 49.9
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.7890 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:28 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:28 metrics.py:335] Avg prompt throughput: 635.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 50.3
INFO 09-24 01:54:28 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.1850 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:28 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 634.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 50.5
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.3118 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 635.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 50.4
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.2410 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 632.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 50.6
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.4644 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 628.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 50.9
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.7870 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 626.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 51.1
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.9057 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 626.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 51.1
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.9460 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 625.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 51.2
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 51.0385 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 620.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 51.6
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 51.4493 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 618.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 51.7
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 51.5847 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 618.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 51.7
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 51.5649 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 618.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 51.7
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 51.5993 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 612.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 52.2
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 52.0587 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 611.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 52.3
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 52.1600 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 610.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 52.4
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 52.2726 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 609.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 52.5
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 52.3286 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 605.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 52.8
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 52.6841 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 605.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 52.9
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 52.7425 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 603.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 53.0
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 52.8135 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:29 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:29 metrics.py:335] Avg prompt throughput: 601.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 53.2
INFO 09-24 01:54:29 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.0293 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:29 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:30 metrics.py:335] Avg prompt throughput: 597.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 53.6
INFO 09-24 01:54:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.4189 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:30 metrics.py:335] Avg prompt throughput: 596.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 53.7
INFO 09-24 01:54:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.5488 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:30 metrics.py:335] Avg prompt throughput: 596.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 53.6
INFO 09-24 01:54:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.4883 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:30 metrics.py:335] Avg prompt throughput: 591.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 54.1
INFO 09-24 01:54:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.9615 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([32]), positions.shape=torch.Size([32]) hidden_states.shape=torch.Size([32, 4096]) residual=None
INFO 09-24 01:54:30 metrics.py:335] Avg prompt throughput: 588.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 54.4
INFO 09-24 01:54:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 54.1556 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:54:30 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=32, max_num_seqs=32
INFO 09-24 01:54:30 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([31]), positions.shape=torch.Size([31]) hidden_states.shape=torch.Size([31, 4096]) residual=None
INFO 09-24 01:54:30 metrics.py:335] Avg prompt throughput: 326.0 tokens/s, Avg generation throughput: 10.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 95.1
INFO 09-24 01:54:30 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 94.9545 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727142861.629003, last_token_time=1727142870.3286836, first_scheduled_time=1727142861.6477654, first_token_time=1727142870.3284602, time_in_queue=0.01876235008239746, finished_time=1727142870.328681, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 8192
End-to-End latency: 8.70 seconds
Throughput: 0.11 requests/s, 941.62 tokens/s
Per_token_time: 1.062 ms

Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=1, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='fixed', input_len=8191, output_len=1, min_len=16, max_len=4096, prefill_to_decode_ratio=10000.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=8192, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=512, max_num_seqs=512, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 1 requests, requiring 1 requests.
Original order of the requests
User Requests: prompt_len=8191, output_len=1, sequence_len=8192...
Selected required requests 1
User Requests: prompt_len=8191, output_len=1, sequence_len=8192...
Chunked prefill is enabled. max_num_batched_tokens=512, max_num_batched_seqs=512
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'type', 'finetuned', 'original_max_position_embeddings'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 09-24 01:42:53 config.py:654] [SchedulerConfig] max_num_batched_tokens: 512 chunked_prefill_enabled: True
INFO 09-24 01:42:53 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
/home/jieye/viper2/vllm_v0_4_2/vllm/executor/gpu_executor.py:36: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 09-24 01:42:54 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 09-24 01:42:58 selector.py:27] Using FlashAttention-2 backend.
INFO 09-24 01:43:02 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 09-24 01:43:02 model_runner.py:183] Loaded model: 
INFO 09-24 01:43:02 model_runner.py:183]  LlamaForCausalLM(
INFO 09-24 01:43:02 model_runner.py:183]   (model): LlamaModel(
INFO 09-24 01:43:02 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 09-24 01:43:02 model_runner.py:183]     (layers): ModuleList(
INFO 09-24 01:43:02 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 09-24 01:43:02 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 09-24 01:43:02 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 09-24 01:43:02 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 09-24 01:43:02 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 09-24 01:43:02 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 09-24 01:43:02 model_runner.py:183]         )
INFO 09-24 01:43:02 model_runner.py:183]         (mlp): LlamaMLP(
INFO 09-24 01:43:02 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 09-24 01:43:02 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 09-24 01:43:02 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 09-24 01:43:02 model_runner.py:183]         )
INFO 09-24 01:43:02 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 09-24 01:43:02 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 09-24 01:43:02 model_runner.py:183]       )
INFO 09-24 01:43:02 model_runner.py:183]     )
INFO 09-24 01:43:02 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 09-24 01:43:02 model_runner.py:183]   )
INFO 09-24 01:43:02 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 09-24 01:43:02 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 09-24 01:43:02 model_runner.py:183]   (sampler): Sampler()
INFO 09-24 01:43:02 model_runner.py:183] )
INFO 09-24 01:43:02 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:02 worker.py:164] Peak: 13.523 GB, Initial: 38.980 GB, Free: 25.456 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 23.901 GB
INFO 09-24 01:43:02 gpu_executor.py:117] # GPU blocks: 3059, # CPU blocks: 8192
INFO 09-24 01:43:33 worker.py:189] _init_cache_engine took 23.8984 GB
INFO 09-24 01:43:33 scheduler.py:307] Scheduler initialized with prompt limit: 8192
INFO 09-24 01:43:33 Start warmup...
INFO 09-24 01:43:33 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:33 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 271.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 1883.6
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1877.0986 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 6121.8 tokens/s, Avg generation throughput: 12.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 83.6
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 83.2336 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 22.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 45.2
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.9679 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 61.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.2
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.8095 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6889 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6276 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6367 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6243 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4396 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4129 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4153 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 32161.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.9
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.9612 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 8064.2 tokens/s, Avg generation throughput: 15.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 63.5
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 63.3466 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4365 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4038 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4415 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4000 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4193 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4362 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4072 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4069 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:35 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:35 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:43:35 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:43:35 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4763 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:40 Start benchmarking...
INFO 09-24 01:43:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:40 metrics.py:335] Avg prompt throughput: 11699.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 43.8
INFO 09-24 01:43:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.2752 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:40 metrics.py:335] Avg prompt throughput: 11252.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 45.5
INFO 09-24 01:43:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.3231 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:41 metrics.py:335] Avg prompt throughput: 9768.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 52.4
INFO 09-24 01:43:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 52.2742 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:41 metrics.py:335] Avg prompt throughput: 8737.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 58.6
INFO 09-24 01:43:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 58.4545 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:41 metrics.py:335] Avg prompt throughput: 9930.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 51.6
INFO 09-24 01:43:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 51.4140 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:41 metrics.py:335] Avg prompt throughput: 9040.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 56.6
INFO 09-24 01:43:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 56.4930 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:41 metrics.py:335] Avg prompt throughput: 8282.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 61.8
INFO 09-24 01:43:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 61.6753 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:41 metrics.py:335] Avg prompt throughput: 7673.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 66.7
INFO 09-24 01:43:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 66.5808 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:41 metrics.py:335] Avg prompt throughput: 7133.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 71.8
INFO 09-24 01:43:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 71.6333 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:41 metrics.py:335] Avg prompt throughput: 6644.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 77.1
INFO 09-24 01:43:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 76.9091 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:41 metrics.py:335] Avg prompt throughput: 6234.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 82.1
INFO 09-24 01:43:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 81.9840 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:41 metrics.py:335] Avg prompt throughput: 5876.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 87.1
INFO 09-24 01:43:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 86.9899 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:41 metrics.py:335] Avg prompt throughput: 5548.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 92.3
INFO 09-24 01:43:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 92.0897 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:41 metrics.py:335] Avg prompt throughput: 5263.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 97.3
INFO 09-24 01:43:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 97.1229 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:41 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([512]), positions.shape=torch.Size([512]) hidden_states.shape=torch.Size([512, 4096]) residual=None
INFO 09-24 01:43:41 metrics.py:335] Avg prompt throughput: 5012.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.7%, CPU KV cache usage: 0.0%, Interval(ms): 102.1
INFO 09-24 01:43:41 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 102.0050 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:43:41 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=512, max_num_seqs=512
INFO 09-24 01:43:42 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([511]), positions.shape=torch.Size([511]) hidden_states.shape=torch.Size([511, 4096]) residual=None
INFO 09-24 01:43:42 metrics.py:335] Avg prompt throughput: 2478.2 tokens/s, Avg generation throughput: 4.8 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 206.2
INFO 09-24 01:43:42 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 206.0509 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727142220.877338, last_token_time=1727142222.12024, first_scheduled_time=1727142220.8960204, first_token_time=1727142222.1200266, time_in_queue=0.018682479858398438, finished_time=1727142222.1202374, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 8192
End-to-End latency: 1.24 seconds
Throughput: 0.80 requests/s, 6589.73 tokens/s
Per_token_time: 0.152 ms

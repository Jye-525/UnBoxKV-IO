Namespace(backend='vllm', model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', tensor_parallel_size=1, trust_remote_code=False, seed=1234, n=1, use_beam_search=False, num_prompts=1, generator='synthetic', workload='ShareGPT', dataset='/lus/grand/projects/VeloC/jye/viper2/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', synthetic_type='fixed', input_len=8191, output_len=1, min_len=16, max_len=4096, prefill_to_decode_ratio=10000.0, dtype='auto', gpu_memory_utilization=0.95, swap_space=64, enforce_eager=True, max_model_len=8192, kv_cache_dtype='auto', device='cuda', enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=128, max_num_seqs=32, download_dir=None, block_size=16, preemption_mode='recompute', enable_mixed_batch=False)
Loaded 1 requests, requiring 1 requests.
Original order of the requests
User Requests: prompt_len=8191, output_len=1, sequence_len=8192...
Selected required requests 1
User Requests: prompt_len=8191, output_len=1, sequence_len=8192...
Chunked prefill is enabled. max_num_batched_tokens=128, max_num_batched_seqs=32
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'finetuned', 'type', 'original_max_position_embeddings'}
rope_scaling = {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned': True, 'rope_type': 'yarn'}
INFO 09-24 01:50:57 config.py:654] [SchedulerConfig] max_num_batched_tokens: 128 chunked_prefill_enabled: True
INFO 09-24 01:50:57 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', speculative_config=None, tokenizer='/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, kv_block_size=16, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1234, served_model_name=/grand/projects/VeloC/jye/viper2/huggingface-hub/Yarn-Llama-2-7b-64k, log_stats=True, preemption_mode=recompute, enable_mixed_batch=False)
/home/jieye/viper2/vllm_v0_4_2/vllm/executor/gpu_executor.py:36: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 09-24 01:50:58 utils.py:660] Found nccl from library /home/jieye/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 09-24 01:51:03 selector.py:27] Using FlashAttention-2 backend.
INFO 09-24 01:51:06 model_runner.py:180] Loading model weights took 12.5669 GB
INFO 09-24 01:51:06 model_runner.py:183] Loaded model: 
INFO 09-24 01:51:06 model_runner.py:183]  LlamaForCausalLM(
INFO 09-24 01:51:06 model_runner.py:183]   (model): LlamaModel(
INFO 09-24 01:51:06 model_runner.py:183]     (embed_tokens): VocabParallelEmbedding(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 09-24 01:51:06 model_runner.py:183]     (layers): ModuleList(
INFO 09-24 01:51:06 model_runner.py:183]       (0-31): 32 x LlamaDecoderLayer(
INFO 09-24 01:51:06 model_runner.py:183]         (self_attn): LlamaAttention(
INFO 09-24 01:51:06 model_runner.py:183]           (qkv_proj): QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)
INFO 09-24 01:51:06 model_runner.py:183]           (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 09-24 01:51:06 model_runner.py:183]           (rotary_emb): YaRNScalingRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=4096, base=10000.0, is_neox_style=True)
INFO 09-24 01:51:06 model_runner.py:183]           (attn): Attention(head_size=128, num_heads=32, num_kv_heads=32, scale=0.08838834764831845)
INFO 09-24 01:51:06 model_runner.py:183]         )
INFO 09-24 01:51:06 model_runner.py:183]         (mlp): LlamaMLP(
INFO 09-24 01:51:06 model_runner.py:183]           (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=22016, bias=False, tp_size=1, gather_output=False)
INFO 09-24 01:51:06 model_runner.py:183]           (down_proj): RowParallelLinear(input_features=11008, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 09-24 01:51:06 model_runner.py:183]           (act_fn): SiluAndMul()
INFO 09-24 01:51:06 model_runner.py:183]         )
INFO 09-24 01:51:06 model_runner.py:183]         (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 09-24 01:51:06 model_runner.py:183]         (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 09-24 01:51:06 model_runner.py:183]       )
INFO 09-24 01:51:06 model_runner.py:183]     )
INFO 09-24 01:51:06 model_runner.py:183]     (norm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 09-24 01:51:06 model_runner.py:183]   )
INFO 09-24 01:51:06 model_runner.py:183]   (lm_head): ParallelLMHead(num_embeddings=32000, embedding_dim=4096, org_vocab_size=32000, num_embeddings_padded=32000, tp_size=1)
INFO 09-24 01:51:06 model_runner.py:183]   (logits_processor): LogitsProcessor(vocab_size=32000, forg_vocab_size=32000, scale=1.0, logits_as_input=False)
INFO 09-24 01:51:06 model_runner.py:183]   (sampler): Sampler()
INFO 09-24 01:51:06 model_runner.py:183] )
INFO 09-24 01:51:07 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:07 worker.py:164] Peak: 13.041 GB, Initial: 38.980 GB, Free: 25.939 GB, Total: 39.394 GB,               cache_block_size: 8388608 Bytes, available GPU for KV cache: 24.383 GB
INFO 09-24 01:51:07 gpu_executor.py:117] # GPU blocks: 3121, # CPU blocks: 8192
INFO 09-24 01:51:38 worker.py:189] _init_cache_engine took 24.3828 GB
INFO 09-24 01:51:38 scheduler.py:307] Scheduler initialized with prompt limit: 8192
INFO 09-24 01:51:38 Start warmup...
INFO 09-24 01:51:38 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:38 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:39 metrics.py:335] Avg prompt throughput: 66.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 1922.2
INFO 09-24 01:51:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 1914.7429 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:39 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:39 metrics.py:335] Avg prompt throughput: 8005.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.0
INFO 09-24 01:51:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6045 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:39 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:39 metrics.py:335] Avg prompt throughput: 7278.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.6
INFO 09-24 01:51:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.4239 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:39 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:39 metrics.py:335] Avg prompt throughput: 6932.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 09-24 01:51:39 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.2693 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:39 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 6640.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 19.3
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.1123 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 6382.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.8936 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 6109.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 21.0
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.7973 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 4741.8 tokens/s, Avg generation throughput: 37.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 27.0
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 26.8376 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 22.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 44.5
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.3027 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.9
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5223 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4062 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3683 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3681 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3938 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5087 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.6
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.4076 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.8 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3971 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 8259.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.5
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.5521 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 8088.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 15.8
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.6815 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 7944.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 16.1
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.9695 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 7523.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.0
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.8705 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 7310.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 17.5
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.3666 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 7047.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.2
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.0171 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 6821.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 18.8
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.6236 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 4930.1 tokens/s, Avg generation throughput: 38.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 26.0
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 25.8181 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3831 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3750 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3991 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3616 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 69.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3580 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3828 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 69.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3588 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3728 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:40 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:40 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([1]), positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 4096]) residual=None
INFO 09-24 01:51:40 metrics.py:335] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.9 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 14.5
INFO 09-24 01:51:40 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 14.3743 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 Start benchmarking...
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 2940.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 43.5
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 15.3732 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 7806.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 16.4
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 16.2067 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 7193.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 17.8
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 17.6528 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 6912.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 18.5
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 18.3737 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 6617.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 19.3
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.1979 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 6375.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.1
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 19.9318 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 6145.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.8
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.6487 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 5897.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.7
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.5633 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 6237.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.5
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.3784 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 6188.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 20.7
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 20.5388 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 6018.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 21.3
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.1236 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 5826.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.0
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 21.8234 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 5662.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 22.6
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 22.4319 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 5495.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.3
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.1473 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 5381.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 23.8
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 23.6406 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 5208.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 24.6
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 24.4331 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 5086.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 25.2
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 25.0187 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 4957.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 25.8
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 25.6753 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 4838.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 26.5
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 26.2778 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 4715.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 27.1
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 26.9978 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:45 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:45 metrics.py:335] Avg prompt throughput: 4607.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 27.8
INFO 09-24 01:51:45 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 27.6353 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:45 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 4501.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 28.4
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.2958 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 4398.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 29.1
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 28.9609 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 4314.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 29.7
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 29.5253 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 4204.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 30.4
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 30.3006 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 4123.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 31.0
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 30.9029 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 4046.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 31.6
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 31.4865 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 3956.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 32.4
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 32.2061 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 3876.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 33.0
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 32.8748 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 3809.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 33.6
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 33.4535 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 3725.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 34.4
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.2121 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 3665.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 34.9
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 34.7812 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 3596.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 35.6
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 35.4450 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 3526.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 36.3
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 36.1574 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 3476.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 36.8
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 36.6771 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 3395.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 37.7
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 37.5533 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 3347.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 38.2
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 38.0914 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 3299.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 38.8
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 38.6479 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 3232.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 39.6
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 39.4542 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 3190.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 40.1
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 39.9678 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 3136.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 40.8
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 40.6656 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 3091.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 41.4
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 41.2602 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 3036.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 42.2
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 42.0070 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 2995.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 42.7
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 42.5818 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 2951.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 43.4
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.2153 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 2909.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 44.0
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 43.8457 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 2865.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 44.7
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 44.5218 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:46 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:46 metrics.py:335] Avg prompt throughput: 2826.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 45.3
INFO 09-24 01:51:46 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.1455 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:46 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:47 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:47 metrics.py:335] Avg prompt throughput: 2781.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 46.0
INFO 09-24 01:51:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 45.8810 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:47 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:47 metrics.py:335] Avg prompt throughput: 2745.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 46.6
INFO 09-24 01:51:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 46.4363 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:47 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:47 metrics.py:335] Avg prompt throughput: 2708.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 47.3
INFO 09-24 01:51:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.1108 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:47 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:47 metrics.py:335] Avg prompt throughput: 2671.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 47.9
INFO 09-24 01:51:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 47.7607 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:47 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:47 metrics.py:335] Avg prompt throughput: 2636.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 48.5
INFO 09-24 01:51:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 48.3999 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:47 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:47 metrics.py:335] Avg prompt throughput: 2601.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 49.2
INFO 09-24 01:51:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.0563 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:47 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:47 metrics.py:335] Avg prompt throughput: 2565.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 49.9
INFO 09-24 01:51:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 49.7470 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:47 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:47 metrics.py:335] Avg prompt throughput: 2537.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 50.5
INFO 09-24 01:51:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 50.2622 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:47 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:47 metrics.py:335] Avg prompt throughput: 2502.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 51.2
INFO 09-24 01:51:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 51.0082 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:47 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:47 metrics.py:335] Avg prompt throughput: 2472.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 51.8
INFO 09-24 01:51:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 51.6386 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:47 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:47 metrics.py:335] Avg prompt throughput: 2441.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 52.4
INFO 09-24 01:51:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 52.2931 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:47 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:47 metrics.py:335] Avg prompt throughput: 2408.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 53.2
INFO 09-24 01:51:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.0140 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:47 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:47 metrics.py:335] Avg prompt throughput: 2381.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 53.8
INFO 09-24 01:51:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 53.6120 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:47 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:47 metrics.py:335] Avg prompt throughput: 2356.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 54.3
INFO 09-24 01:51:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 54.1415 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:47 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([128]), positions.shape=torch.Size([128]) hidden_states.shape=torch.Size([128, 4096]) residual=None
INFO 09-24 01:51:47 metrics.py:335] Avg prompt throughput: 2324.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.4%, CPU KV cache usage: 0.0%, Interval(ms): 55.1
INFO 09-24 01:51:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 54.9207 resumed_reqs=0, running_reqs=1 raw_running=1
INFO 09-24 01:51:47 scheduler.py:871] Scheduling chunked prefill: max_num_batched_tokens=128, max_num_seqs=32
INFO 09-24 01:51:47 llama.py:316] before LlamaModel forward() input_ids.shape=torch.Size([127]), positions.shape=torch.Size([127]) hidden_states.shape=torch.Size([127, 4096]) residual=None
INFO 09-24 01:51:47 metrics.py:335] Avg prompt throughput: 1288.1 tokens/s, Avg generation throughput: 10.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%, Interval(ms): 98.6
INFO 09-24 01:51:47 llm_engine.py:629] LLMEngine step() (unit:ms): time_cost 98.4459 resumed_reqs=0, running_reqs=1 raw_running=1
RequestOutput(request_id=2, request_type=SequenceType.RAW, finished=True, metrics=RequestMetrics(arrival_time=1727142705.5199497, last_token_time=1727142707.8514102, first_scheduled_time=1727142705.5383563, first_token_time=1727142707.8511987, time_in_queue=0.01840662956237793, finished_time=1727142707.8514075, rescheduled_time=None, reget_first_token_time=None))
Total_num_tokens: 8192
End-to-End latency: 2.33 seconds
Throughput: 0.43 requests/s, 3513.31 tokens/s
Per_token_time: 0.285 ms
